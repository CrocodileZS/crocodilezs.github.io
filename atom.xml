<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>鳄鱼Crc2H0U</title>
  
  <subtitle>野生鳄鱼的自留地</subtitle>
  <link href="https://blog.crocodilezs.top/atom.xml" rel="self"/>
  
  <link href="https://blog.crocodilezs.top/"/>
  <updated>2022-08-21T04:15:59.940Z</updated>
  <id>https://blog.crocodilezs.top/</id>
  
  <author>
    <name>CrocodileZS</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>雅思备考经验</title>
    <link href="https://blog.crocodilezs.top/202208/ielts/"/>
    <id>https://blog.crocodilezs.top/202208/ielts/</id>
    <published>2022-08-17T16:16:21.000Z</published>
    <updated>2022-08-21T04:15:59.940Z</updated>
    
    <content type="html"><![CDATA[<p>备考时间大约40天 (最后10天全职备考), 最后出分 6.5 (6.5), 记录一下自己的备考流程和经验.</p><span id="more"></span><p>bg: 六级分数530</p><div class="table-container"><table><thead><tr><th>Listening</th><th>Reading</th><th>Writing</th><th>Speaking</th></tr></thead><tbody><tr><td>6.5</td><td>7.0</td><td>6.5</td><td>6.5</td></tr></tbody></table></div><h2 id="Overall"><a href="#Overall" class="headerlink" title="Overall"></a>Overall</h2><p>备考用到的材料包括:</p><ul><li>听力: 何琼网课, 王陆语料库</li><li>阅读: 刘洪波阅读真经</li><li>写作: Simon网课, Vince网课, 某宝的批改</li><li>口语: B站雅思口语9分安, 杨帅口语网课</li><li>真题: 剑 6, 13, 14, 16, 17</li></ul><p>机考听阅抽到的题目难度比剑17稍微简单一点点, 但是出现了很多少见的题型.</p><p>听力出现了 <code>no more than 2/3 words</code> 的填空题, Section3和Section4的题目形式换了一下.</p><p>阅读出现了17道匹配题, 还有一个 <code>no more than 3 words</code> 的简答题. (这个题型在我刷到的剑雅里都没出现过)</p><p>写作大作文抽到了比例相对较小的 problem / solution 类似的题目. 问题是现有的商家用来促销的手段有哪些, 其中哪个最effective.</p><p>口语 Part1 抽到了 <code>Advertisement</code>, Part2 抽到了没准备的题目 <code>Traffic Jam</code>, Part3 问了七八个题目, 超时了好久.</p><h2 id="Listening"><a href="#Listening" class="headerlink" title="Listening"></a>Listening</h2><ul><li>何琼听力网课</li></ul><p>看了何琼听力网课的内容, 非常有用, 但是有些细节的做题方法有点旧了.</p><p>比如Section1填空题的答案不仅出现在对话的回答里, 而且也会出现在对话的问题中. 在剑17里填空题想要得到答案已经不能仅仅靠定位词了, 需要对对话情景比较了解. 需要准备一下何琼的场景词. </p><p>关于Section2和Section3的选择题, 我感觉何琼的方法已经不太好用了, 最好的方法还是做题然后精听或者听写. 剑17听力选择题的干扰项太变态了. </p><ul><li>王陆语料库</li></ul><p>因为自己的备考时间不太够, 所以最后没有用上, 但是备考时间充足的话我一定会老实用她的语料库听写.</p><p><code>experience</code>, <code>february</code>, <code>journalism</code> 都拼写错了, 0.5分就这样没了(哭).</p><h2 id="Reading"><a href="#Reading" class="headerlink" title="Reading"></a>Reading</h2><p>没花太多时间在阅读上, 了解了刘洪波的做题方法后主要就是至少每两天一套题保持手感. </p><p>剑6 用来训练做题方法.</p><p>剑13 - 16的分数在7.5到8.5之间浮动, 剑17的分数差不多都是7.0</p><p>Tips: 剑17已经不再能通过简单的同义替换做题了, 需要理解文章内容.</p><h2 id="Writing"><a href="#Writing" class="headerlink" title="Writing"></a>Writing</h2><p>Simon的小作文网课, 看完了之后把每节课的handout都完成, 写了六七篇的样子.</p><p>Simon的大作文网课看完了, 但是最后没有采用, 还是用了B站<code>Vince9120</code>的写作逻辑. 写了两篇找某宝批改, 一篇6一篇5.</p><p>Tips: </p><ul><li>大作文的逻辑框架可以采用Simon的, 但是观点的展开一定要用Vince的三种方法. Simon的 <code>Listing</code> 的展开方式也被某宝批改的两位老师声讨了, 所以练习的两篇文章分数都比较低. </li><li>考场上时间不够的话不要再展开新观点了. 我最后半分钟的时候想到了一个很好的exemplify <code>It is the good taste rather than low price makes CocaCola one of the best beverages.</code>但是没有展开, 那一段的逻辑最后都没顺明白. </li></ul><h2 id="Speaking"><a href="#Speaking" class="headerlink" title="Speaking"></a>Speaking</h2><p>考前把 Part1 和 Part2 的题目都串了一遍, 我用的杨帅整理的题库. </p><p>但是考试还是抽到新题了(悲). 没有准备过的新题, 我都没想到我能说够两分钟还被考官打断. (考官是一位白叔, 口语很舒服. 为了让我听清楚, 问问题的时候 key words 都是0.1倍速在讲)</p><p>推荐B站<code>雅思口语9分安</code>的公开课, 他讲的扩充的方法非常有用, 让 Part2 比较容易能讲到 2min .</p><p>避雷杨帅整理的题库, 还是要在雅思哥上看本季的题目. 考试的时候不要怯场讲话就会流利很多.</p><p>杨帅的 <code>filler words</code> 和每日练三句都是好的备考素材.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;备考时间大约40天 (最后10天全职备考), 最后出分 6.5 (6.5), 记录一下自己的备考流程和经验.&lt;/p&gt;</summary>
    
    
    
    <category term="英语" scheme="https://blog.crocodilezs.top/categories/%E8%8B%B1%E8%AF%AD/"/>
    
    
    <category term="雅思" scheme="https://blog.crocodilezs.top/tags/%E9%9B%85%E6%80%9D/"/>
    
  </entry>
  
  <entry>
    <title>2022年2月产品实习面经</title>
    <link href="https://blog.crocodilezs.top/202202/pm%E9%9D%A2%E7%BB%8F/"/>
    <id>https://blog.crocodilezs.top/202202/pm%E9%9D%A2%E7%BB%8F/</id>
    <published>2022-04-06T18:16:21.000Z</published>
    <updated>2022-08-21T04:39:19.850Z</updated>
    
    <content type="html"><![CDATA[<p>一共面了6个部门的产品实习 (字节Pico, 字节飞书, 滴滴地图事业部, Momenta, 小米互部, 小冰算法), 拿到两个offer最后选择了小米.</p><span id="more"></span><div class="table-container"><table><thead><tr><th>Job Positions</th><th>Enterprise</th><th>Final Result</th></tr></thead><tbody><tr><td>VR和AR产品经理实习生</td><td>字节Pico</td><td>一面挂</td></tr><tr><td>地图点产品经理实习生</td><td>滴滴地图事业部</td><td>三面oc</td></tr><tr><td>产品架构实习生</td><td>Momenta</td><td>一面挂</td></tr><tr><td>商业平台产品经理实习生</td><td>小米互联网业务部</td><td>二面oc</td></tr><tr><td>算法产品经理实习生</td><td>红棉小冰</td><td>一面挂</td></tr><tr><td>测试产品经理实习生</td><td>字节飞书</td><td>一面挂</td></tr></tbody></table></div><p>面试中简历挖掘和业务问题基本是五五开，简历挖掘部分不再赘述.</p><h2 id="字节Pico"><a href="#字节Pico" class="headerlink" title="字节Pico"></a>字节Pico</h2><p>方向：vr and ar<br>2月9日投递简历，安排在2月11日一面。一共1h40min，给我面麻了</p><ul><li>一面<br>1、自我介绍<br>2、来聊一聊你曾经的项目，你想先聊哪一个？<br>（选择了小猴启蒙的用户增长运营，因为和产品岗位更加相关。）<br>3、向我介绍一下这段实习你做了什么？（共分成了三点进行描述，微信平台数据跟踪、裂变活动策划、其他渠道向微信引流）<br>4、详细介绍你在其中做了什么事情？（在裂变活动策划中，ab test 制定新的微信推送sop，制定了稳定时间频率的；在微信平台数据跟踪中，通过漏斗分析数据，中间还提问了各种细节）<br>5、介绍一下区块链项目你在其中做的内容？（介绍）你觉得如果不用区块链能够解决这个问题吗？（可，把农民工的工资让开发方质押在银行）你觉得你的这个想法为什么没有人实施呢？（我这个问题说不出个所以然，减分点）<br>6、你的职业规划是什么？（想来体验一下产品，再决定自己未来的职业道路规划）<br>7、你对ar/mr有了解吗？（整个由两部分组成：硬件和内容。介绍了我体验过的两个产品，但是忘记爱奇艺那个产品的名字了，是一个挺大的减分点吧）<br>8、你对未来ar、mr的畅想？（ar野生动物园导览）<br>9、现场做一个产品调研，调研一下arkit，向我介绍一下它？（这个问题说的太笼统了，在我看网页的过程中面试官给我细化了一些sdk，我看了一下哪些sdk然后向面试官进行介绍，面试官也给我讲了很多相关的技术介绍）<br>10、你平时玩游戏吗？（玩，巫师三 凯娜精神之桥）你觉得游戏中哪些最吸引你？（我最后选了“双人成行”，我觉得他比较牛逼的地方是两个人的游戏交互形式非常的多）如果你自己要做一个游戏，你该怎么借鉴双人成行呢？（我说的非常的不清楚）<br>11、关于实习的时间<br>12、反问环节，我问了pico的具体业务（</li></ul><p>总结：其实整个过程还是聊的很愉快的，但是后续我觉得自己其实是非常缺少“产品逻辑”，整个面试过程中聊的都是自己的直观体验，而没有从产品的角度去看待问题。这就是自己的不足。应该更多的了解产品岗位的视角是怎样的，从更专业的角度去剖析各种产品。</p><p>业务反馈被挂原因：对ar和vr的了解不够。</p><h2 id="滴滴地图事业部"><a href="#滴滴地图事业部" class="headerlink" title="滴滴地图事业部"></a>滴滴地图事业部</h2><p>主要方向：地图点<br>共三面, oc，每一面都有30min，2.16一面；2.17二面；2.18三面</p><ul><li>一面<br>1、简历项目挖掘，关于学而思小猴的那一段用户增长运营经历<br>2、如果你的mentor让你去给她买一杯咖啡，你会怎么做？（用pm项目管理的思路去答）<br>3、在学校里最让你有成就感的一件事（3rd人工智能论坛）项目如何推进（项目管理），如果遇到不同意见如何说服？（pm沟通）<br>4、如何快速估计出北京的公交车站点数（开放问题，采样思路随便说说）</li></ul><p>面试官是一个非常和蔼的姐姐，面完当场告诉我通过了，准备二面</p><ul><li>二面<br>1、简历挖掘，关于区块链的项目，如何从pm的角度思考产品的可行性<br>2、竞品分析：滴滴和高德打车流程的异同。<br>（1）都是自动推荐周围上车点，没有给用户提示<br>（2）下车点的选择大同小异（滴滴 添加途经点， 高德可以对调出发和到达）<br>（3）打车界面（高德有价格筛选、应答率、多平台支持） 应答速度还是滴滴更快<br>我还讲了一下关于高德的助老打车，无论是app还是小程序，用户路径都比滴滴的要好。</li></ul><p>二面面试官觉得问题不大，说三面不出差错就没大问题。</p><ul><li>三面<br>1、对上车点和下车点的异同分析<br>2、为什么打车软件从高德换到了滴滴<br>3、高德和滴滴的分析（问的好笼统）<br>最后向我介绍了地图事业部的业务</li></ul><h2 id="momenta-产品架构"><a href="#momenta-产品架构" class="headerlink" title="momenta 产品架构"></a>momenta 产品架构</h2><p>方向：产品系统架构 一面挂<br>明显地感觉出来对面不想说话 kpi面 互相介绍了一下就没了</p><h2 id="小米-商业平台"><a href="#小米-商业平台" class="headerlink" title="小米 商业平台"></a>小米 商业平台</h2><p>方向：adpm<br>一共两面，oc</p><ul><li><p>一面<br>简历挖掘；关于广告的兴趣；职业规划</p></li><li><p>二面<br>广告计费方式的考察；简历挖掘；然后就是leader向我介绍工作内容了<br>最后问了广告的一些算法（我懵了，还好没瞎编，后来知道leader也是cs出身的）</p></li></ul><h2 id="小冰"><a href="#小冰" class="headerlink" title="小冰"></a>小冰</h2><p>方向：算法pm（听说小冰的pm数量比rd多很多，，）<br>一面挂. 因为当时已经拿了滴滴offer，小米的offer也觉得停稳，小冰和字节飞书就面着玩儿了</p><ul><li>一面</li></ul><p>1、简历挖掘<br>2、nlp的算法问我了解多少（<br>3、在一个充满机器人的群聊（只有用户一个真人），发现这个用户非常喜欢说晚安，该如何构建用户场景？<br>（用户画像建立-群聊场景-功能性建立）</p><h2 id="字节飞书"><a href="#字节飞书" class="headerlink" title="字节飞书"></a>字节飞书</h2><p>我之前真的超想去飞书，在字节做校园大使的时候就觉得飞书真的好用，大三的时候投简历被挂了。今年飞书toB业务开展比较缺人，，然而我面试的时候懈怠了，又是一面挂。</p><p>方向：测试软件的pm<br>简历挖掘 + 问了些软工课程里的东西，关于测试的内容。<br>如何测试微信朋友圈发送图片的功能</p><p>其实这个方向离我的预想差的太远了，我以为是toC体验的方向</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;一共面了6个部门的产品实习 (字节Pico, 字节飞书, 滴滴地图事业部, Momenta, 小米互部, 小冰算法), 拿到两个offer最后选择了小米.&lt;/p&gt;</summary>
    
    
    
    <category term="产品" scheme="https://blog.crocodilezs.top/categories/%E4%BA%A7%E5%93%81/"/>
    
    
    <category term="产品" scheme="https://blog.crocodilezs.top/tags/%E4%BA%A7%E5%93%81/"/>
    
    <category term="面经" scheme="https://blog.crocodilezs.top/tags/%E9%9D%A2%E7%BB%8F/"/>
    
  </entry>
  
  <entry>
    <title>博客内容归档和英文博客</title>
    <link href="https://blog.crocodilezs.top/202111/%E5%8D%9A%E5%AE%A2%E5%86%85%E5%AE%B9%E5%BD%92%E6%A1%A3%E5%92%8C%E8%8B%B1%E6%96%87%E5%8D%9A%E5%AE%A2/"/>
    <id>https://blog.crocodilezs.top/202111/%E5%8D%9A%E5%AE%A2%E5%86%85%E5%AE%B9%E5%BD%92%E6%A1%A3%E5%92%8C%E8%8B%B1%E6%96%87%E5%8D%9A%E5%AE%A2/</id>
    <published>2021-11-20T10:08:21.000Z</published>
    <updated>2022-05-23T17:36:55.902Z</updated>
    
    <content type="html"><![CDATA[<div class="tag-plugin note" ><div class="body"><p>从2019年开始尝试搭建自己的博客，到现在已经本科毕业。这三年的时间虽然没有坚持更新博客，但是一直在坚持记录。无论是理论知识的学习、Debug的记录，还是读书健身笔记和感慨万千时的随笔，他们都散落在各种在线笔记软件和手机的备忘录中。</p><p>随着时间的推移，我对博客的认识也发生了很多变化。“记录”对我来说仍然具有非常大的意义，但是“博客”最重要的价值不应该是“记录”，而是“分享”——传递有价值的信息。我把平时的日常生活和学习经历记录在各种笔记中，过一段时间再去回顾和整理，选取其中有价值的东西去分享。</p><p>过去的一年再次参加了美赛，加入了几个和区块链有关的项目，也完成了不少专利和论文。更多的时间忙于运用知识，以需求为导向迫使自己进行广泛而不深刻的学习。对我来说真正有价值的输入变少了，输出和表达的欲望也降低了很多。</p><p>   我在原先的博客域名之下又建立了一个英文博客<a href="https://blog.crocodilezs.top/eysblog_en/">BLOG-EN</a>，旨在整理自己过去一年经历的各种项目。同时也将原博客中的内容进行整理归档，便于自己和访客的查阅。</p></div></div><span id="more"></span><h2 id="课程设计与实验"><a href="#课程设计与实验" class="headerlink" title="课程设计与实验"></a>课程设计与实验</h2><p><a href="/201911/Linux开发环境及应用作业%2020191031/" itemprop="url"> Linux文本处理作业 </a> &emsp;&emsp;<br><a href="/categories/操作系统/" itemprop="url"> 《30天自制操作系统》实验合辑 </a> &emsp;&emsp;<br><a href="/201909/周宇洋_「学生宿舍管理系统」实验报告/" itemprop="url"> 学生宿舍管理系统Python开发 </a> &emsp;&emsp;<br><a href="/201911/KNN与Naive_Bayes代码实现/" itemprop="url"> KNN和朴素贝叶斯的代码实现 </a> &emsp;&emsp;<br><a href="/201911/Price_Suggestion_Chanllenge/" itemprop="url"> 商品价格预测挑战 </a> &emsp;&emsp;<br><a href="/201911/Fisher算法&SVM&K-Means及其优化/" itemprop="url"> Fisher算法 &amp; SVM &amp; K-Means的实现和优化 </a> &emsp;&emsp;<br><a href="/201910/FINDS算法和ID3算法/" itemprop="url"> FINDS算法和ID3算法 </a> &emsp;&emsp;<br><a href="/201904/插入排序归并排序和快速排序/" itemprop="url"> 算法设计之排序 </a> &emsp;&emsp;<br><a href="/201904/循环赛赛程安排/" itemprop="url"> 算法设计之循环赛赛程安排 </a> &emsp;&emsp;</p><h2 id="展示和汇报"><a href="#展示和汇报" class="headerlink" title="展示和汇报"></a>展示和汇报</h2><p><a href="/201904/基于链接内容的社区发现（一）/" itemprop="url"> 基于链接内容的社区发现（一） </a> &emsp;&emsp;<br><a href="/201904/基于链接内容的社区发现（二）/" itemprop="url"> 基于链接内容的社区发现（二） </a> &emsp;&emsp;</p><h2 id="学习笔记"><a href="#学习笔记" class="headerlink" title="学习笔记"></a>学习笔记</h2><p><a href="/201908/「迁移学习简明手册」学习笔记（1）/" itemprop="url"> 《迁移学习简明手册》学习笔记 </a> &emsp;&emsp;<br><a href="/201909/实验室苦逼搬砖暑假生活纪实/" itemprop="url"> 用户对齐（实验室搬砖纪实） </a> &emsp;&emsp;</p><h2 id="读书笔记"><a href="#读书笔记" class="headerlink" title="读书笔记"></a>读书笔记</h2><p><a href="/202005/《苏东坡传》摘录/" itemprop="url"> 《苏东坡传》 </a> &emsp;&emsp;<br><a href="/202002/祭亡妻程氏文/" itemprop="url"> 《祭亡妻程氏文》 </a> &emsp;&emsp; </p>]]></content>
    
    
    <summary type="html">&lt;div class=&quot;tag-plugin note&quot; &gt;&lt;div class=&quot;body&quot;&gt;&lt;p&gt;从2019年开始尝试搭建自己的博客，到现在已经本科毕业。这三年的时间虽然没有坚持更新博客，但是一直在坚持记录。无论是理论知识的学习、Debug的记录，还是读书健身笔记和感慨万千时的随笔，他们都散落在各种在线笔记软件和手机的备忘录中。&lt;/p&gt;&lt;p&gt;随着时间的推移，我对博客的认识也发生了很多变化。“记录”对我来说仍然具有非常大的意义，但是“博客”最重要的价值不应该是“记录”，而是“分享”——传递有价值的信息。我把平时的日常生活和学习经历记录在各种笔记中，过一段时间再去回顾和整理，选取其中有价值的东西去分享。&lt;/p&gt;&lt;p&gt;过去的一年再次参加了美赛，加入了几个和区块链有关的项目，也完成了不少专利和论文。更多的时间忙于运用知识，以需求为导向迫使自己进行广泛而不深刻的学习。对我来说真正有价值的输入变少了，输出和表达的欲望也降低了很多。&lt;/p&gt;&lt;p&gt;   我在原先的博客域名之下又建立了一个英文博客&lt;a href=&quot;https://blog.crocodilezs.top/eysblog_en/&quot;&gt;BLOG-EN&lt;/a&gt;，旨在整理自己过去一年经历的各种项目。同时也将原博客中的内容进行整理归档，便于自己和访客的查阅。&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>《苏东坡传》摘录</title>
    <link href="https://blog.crocodilezs.top/202005/%E3%80%8A%E8%8B%8F%E4%B8%9C%E5%9D%A1%E4%BC%A0%E3%80%8B%E6%91%98%E5%BD%95/"/>
    <id>https://blog.crocodilezs.top/202005/%E3%80%8A%E8%8B%8F%E4%B8%9C%E5%9D%A1%E4%BC%A0%E3%80%8B%E6%91%98%E5%BD%95/</id>
    <published>2020-05-23T01:21:11.000Z</published>
    <updated>2022-05-06T13:24:35.173Z</updated>
    
    <content type="html"><![CDATA[<p>今年五月份读完的这本书，暑假开始的时候决定把读书笔记和读后感整理放在博客上。关于《苏东坡传》的文章一共有两篇，这是第一篇，内容为读书时的摘录。第二篇如下：  </p><p>人生到处知何似， 应似飞鸿踏雪泥， 泥上偶然留指爪， 鸿飞那复计东西。</p><p>拿西洋作家为例，李白可以媲美雪莱或拜伦，是一个燃烧自己展现出瞬间壮景的文学彗星。杜甫就象米尔顿，是一个热心的哲学家和老好人，以贴切、渊博的古典比喻写出了丰富的作品。苏东坡永远年轻。他性格比较象萨克莱，政治和诗词的盛名则象雨果，同时又具有约翰生博士那份动人的本质。不知怎么约翰生博士的痛风病直到今天还叫我们感动，米尔顿的瞎眼却不尽然。如果约翰生同时又兼有甘斯伯劳的特色，而且象波普用诗词批评政治，又象史维夫特吃过那么多苦而没有史维夫特的尖酸味儿，我们就能找出一个英国的类比了。苏东坡的道精神由于遭受许多困难而更醇美，却没有变酸。今天我们爱他，只因为他吃苦吃得太多了。</p><p>苏东坡在面对痛苦时展现出了超常的淡然，他像是一个已经预知了自己一生的人。我最爱他的地方是他面对所有事情的坦诚。  </p><p>今年五月份读完的这本书，暑假开始的时候决定把读书笔记和读后感整理放在博客上。关于《苏东坡传》的文章一共有两篇，这是第一篇，内容为读书时的摘录。第二篇如下：</p><span id="more"></span><hr><p>我们可以检视一幕幕已经终了的情节，眼见许多事情因外在事变和内在性格的必然性而自然发展。  </p><p>王安石热衷社会改革，自然觉得任何手段都没有错，甚至不惜清除异己。神圣的目标向来是最危险的。<strong>一旦目标神圣化，实行的手段必然日渐卑鄙。</strong>这种发展趋势逃不过苏东坡敏锐的心灵，甚至有点违背他的幽默感。他的行径和王安石不合；彼此的冲突决定了他一生的经历，也决定了宋朝的命运。  </p><p>“文章如精金美玉”，苏东坡写信给谢民师说，“市有定价，非人所能以口舌贵贱也”。</p><p><strong>构成永恒特质的“真诚性”</strong>究竟是什么呢？苏东坡清清楚楚表达了他对写作和文体的意见。“大略如行云流水。初无定质。但常行于所当行，常止于不可不止。文理自然，姿态横生。孔子曰，言之不文，行之不远。又曰，词达而已矣，夫言止于达意，则疑若不文，是大不然。求物之妙，如系风捕影，能使是物了然于心者，盖千万人而不一遇也。而况能使了然于口与手乎。是之谓词达。词至于能达，则文不可胜用矣。杨雄好为艰深之词。以文浅易之说，若正言之，则人人知之矣。此正所谓雕虫篆刻者。”苏东坡为文体下定义，正好贴切地描述了他自己写文章的过程，动笔收笔都象“行云流水”，揭露出文章和修辞的一切奥秘。什么时候进行，什么时候结束都没有一定的规矩。<strong>如果作家的思绪很美，只要他能忠实、诚恳、妥当表达，魅力和美感自然存在。</strong>这些不是硬放人作品的东西，词达而造成的单纯、自然和某一种自由感正是好文章的秘诀。这些特质具备了，文体不虚浮空洞，就可以写出真诚的文学作品。  </p><p>“生平最快乐的时刻”，有一天苏东坡对他的朋友说，“就是写作时笔端能表达一切思想的脉络。我自忖道‘人间自乐莫过于此’”。  </p><p>他曾一度坚称，给人快慰的力量便是文学本身的报酬。  </p><p>当时有一位作家说，文士不怕刑罚，不爱晋升，也不贪生怕死，只怕欧阳修的意见。欧阳修对一位同事说，“读苏东坡的信，我全身喜极流汗。我应当退隐，使这个青年出人头地。”想想这句话对苏东坡有多大的影响!  </p><p>日月何促促， 尘世苦局束。  </p><p>我简直想说，苏东坡的精神代表“火”，他一生和水灾、旱灾奋斗，每到一地就忙着修建供水系统、水运系统和水井。火的象征很恰当，因为他活力充沛；换句话说，他的脾气和一生都象烈焰，到处给人生机和温暖，也一路烧毁了某些东西。</p><p><em>我没有懂作者这里说的烧毁的是什么东西</em>  </p><p>人生到处知何似， 应似飞鸿踏雪泥， 泥上偶然留指爪， 鸿飞那复计东西。 这是东坡的佳作之一，飞鸿象征人类的精神。事实上这本书所写的苏东坡生来事略只是一个伟大心灵偶尔留下的足迹，真正的苏东坡是一个幻鸟般的灵魂，说不定今天还在星宿间梦游呢。  </p><p>她要他当心那些表现太露骨的泛泛之交，以及他根据“世间无恶人”理论而交上的朋友。他的麻烦就出在这儿；他无法看出别人的错处。他太太对他说：“当心那些朋友。太快建立的友情不会长久的。”东坡承认，她的话总是应验，我想她这方面的智慧是来自中国<strong>“君子之交淡如水”的古训——没有令人兴奋的味道，却永远不会生厌。</strong>诚挚的友情从来不表现太多。真正的好友不常写信，因为全心信任彼此的友谊，根本不必写。分别几年又重逢，友情依然如故。   </p><p>坑上架着一个小木板桥，百英尺下有激流飞过，四周是直立的峡谷。章惇自己很勇敢，对苏轼一鞠躬，他走过木板桥，在对面峭壁上留几个字。苏东坡拒绝了，章惇独自过桥，泰然自若。他拢拢长袍，抓住一根吊索，沿峭壁到溪流对岸写了六个字：“苏轼章惇来游”。然后若无其事走回来。苏东坡拍拍朋友的背说，“有一天你会杀人。” “为什么？”章惇问道。 苏东坡答道，“能将自己性命玩弄于股掌之上，也就能杀人。”苏东坡的预言到底对不对，我们以后就知道了。  </p><p>这不是中国第一次试行国家资本主义，却是最后的一次。在中国四千年的历史中，曾四度试行极权主义、国家资本主义、社会主义，以及剧烈的社会革命，每一次都惨败而终。最成功的是法家商鞍的右派极权主义，他的理论由兴建长城的秦始皇（公元前三世纪）有效推行。早期法家理论最重要的两大原则就是教战与重农。两者其实是同一回事，因为商鞍相信农夫是最好的军人，所有中产阶级的商人和贸易家都该尽量受到压制。大家都知道，根据这一教条而建立发展的强大军事系统使秦国统一了全中国；但是此一政治理论刚遍行全国，不到几年就完全崩溃。  </p><p>王安石的怪习惯是不是伪装，我们无法断定；<strong>不过一个人的行为如果太过份，大家难免怀疑他有自我宣传的意味。</strong>  </p><p>王安石说，他宁愿谈谈尧舜的贤臣。“在上等人才眼中，诸葛亮根本不值得一提。” 诸葛亮的政治天才在于一步步走向既定的目标，这位急功自信的财政鬼才觉得很不对胃口。  </p><p>好官知道这些贷款对人民不利，确定他们付不出本利会下狱坐牢。他们遵守政府的明文规定，宣布贷款完全出于“自愿”，心中打算有一天会因“阻碍新政”而丢官。  </p><p>王安石认为，控制文人的思想更属必要。他和古代的王莽，近代的希特勒一样，具有“一个国家、一个信仰、一个领袖”的信念〔他像希特勒，遭到反对就大发雷霆；现代精神病学家可以把他列为妄想狂。  </p><p>无论古今中外，人民爱不爱某一政权唯有等这个专制政府失势才能判断。  </p><p>郑侠终日站在宫门边，看到成群难民由东北逃来，挤满京师的街道。郑侠知道图画比言辞更有力，就把这些可怜的农民画下来，献给皇上。有一张画描写难民饥寒交迫，在大风雨中流浪。另一张描写半裸的男女正在吃草根树皮，还有人拴着铁链搬砖负柴来缴税。皇帝看到这些图画，不禁掉下泪来，精采的献图——我们以后会谈到——加上一颗彗星出现。圣山发生土崩，皇帝终于废除了许多“新法”。  </p><p>司马光学问和品德都冠绝当代，从头到尾为原则而争。他和王安石代表相反的政策立场。  </p><p>过了两个月老相富弼辞职，临行警告说，<strong>治斗争总是好人输，奸小一定会爬到高位。因为好人争原则，坏人争权利，最后双方都各得其所，好人去职，坏人留下来。</strong>他预言这样下去国家不久就会陷入纷乱。</p><p>苏东坡的九千字“上皇帝书”非常重要，可以代表他的政治哲学，也显出他个人的脾气和文风，充满机智、学问和大无畏的勇气。义愤的争论夹着冷静、简明的推理。有时候沮丧、严苛、挑剔、直爽无比；有时候却徐徐辩论，引经（孔孟）据典，引史例来支持他的理论。内容巧妙、诚挚、有力，对世事满怀激动和悲哀。  </p><p>苏东坡认为，好政府要靠异议的健全作用来维持。民主就根据各党异议的原则而存在。我相信苏东坡若生在现代，一定反对联合国安理会的否决权，认为不民主。他知道盘古开天以来，没有两个人看法完全一样，除了民主就是专制。我从来没有发现一个反对民主的人在家、在国、在世界政局上不是暴君。  </p><p>若使言无不同，意无不合，更唱迭和，何者非贤。  </p><p>孔子曾说，人应该“驱郑声，远佞人”。有一天王安石和惠卿谈话，他弟弟安国在外面吹笛子。宰相对弟弟大叫说，“你能不能驱郑声？”他弟弟回答说，“你能不能远佞人？”  </p><p>两兄弟政治观点始终相同，立场也一致，但是性格却完全不一样。子由性安稳，实事求是，保守，不爱多说话；东坡性豪放，开朗，多嘴多舌，天真而不计一切后果。朋友同伴都觉得子由很可靠，东坡开朗的天才，他的嘲弄和恶作剧却常常使人害怕。  </p><p>苏东坡最大的缺点就是喜欢在宾客面前或者作品中坦白说出他的想法，十分不利。子由十分了解他的哥哥。子由把手放在他的嘴上，叫他从此沉默些，后来东坡出狱，子由也曾做过同样的暗示。  </p><p>风中飞蓬正是苏东坡一生最好的象征，从此他就成为政治风暴中的海燕，直到老死从未在一个地方住过三年以上。  </p><p>这里是他的第二故乡，不只因为此地有美丽的山丘、森林、湖泊、大海、热闹的市街和壮观的寺庙，也因为当地人民都很喜欢他，他度过了这一生中最幸福的日子。居民有南方人快乐的天性，有诗歌有美人，他们敬爱这位年轻的名诗人，欣赏他冲动、热情和无忧无虑的个性。美景启发了他的灵感，此外柔婉的魅力更抚慰了他的心灵。杭州赢得他的青睐，他也赢得杭州人民的爱戴。他担任杭州通判（助理官员），没有机会为人民多尽力，但是诗人的身份已经足够了；他被捕的时候，杭州人纷纷在街上设龛拜祭，替他解灾。他走了以后，南国的美景和温情一直令他魂牵梦系。他知道他会回来，十八年后他再度回来当太守，对本城建树极多，在杭州人心目中留下了不朽的回忆，大家都说他是杭州人。在他死后千年的今天。你走上西湖，登上孤山岛或凤山，或者在湖滨的一家饭店喝茶，你会听到杭州本籍的店主一再提到“苏东坡——苏东坡。”你若点明东坡是四川人，他可不高兴。咦，他认为苏东坡生在那儿，除了京师从来没到过别的地方哩!  </p><p>苏东坡几乎相信他前生曾住在这儿。他自己的诗词和同代人的杂记都有记载。有一天他拜访寿星院，一进大门就觉得景物很熟悉。他告诉同伴，他知道有九十二级石阶通向忏堂。结果完全正确。他还向同伴描述后殿的建筑、庭院和木石。我们不必相信这些转生的故事，不过社会若相信神鬼和轮回，总有很多这一类的说法，就象鬼故事，没有人能证明是真是假。  </p><p>游这些山往往要一整天，他常在傍晚回来，街灯都亮了。穿过灯火通明、人潮汹涌的小河塘夜市，他往往半醉才回家，想起一些诗句然后又忘掉一些： 睡眼忽惊矍， 繁灯闹河塘。 <strong>市人拍手笑， 状如失林莺。</strong> 始悟山野姿， 异趣难自强。 人生安为笑， 吾策殊未良。  </p><p>这些家船都精雕细琢，船头有笕嘴。湖上还有其它船只专卖食品给游客。有人卖栗子、瓜子、莲藕、甜食、炸鸡和海鲜。有人专供茶水。有些船上载着艺人，照例贴近游客的小船，为大家表演歌唱、杂耍，并供应吊索和其他射猎的游戏。他们身边就是澄蓝的湖水，周长十英里左右。远处白云栖在山顶上，山峰若隐若现。云霞使山峰千变万化，多采多姿，山峰给云霞一个栖息的所在。有时候天冷欲雪，雾气盖满山脚。隔着雾气，游人可以看见零零落落的的亭台楼阁，瞥见远山模糊的棱线。睛天湖水清爽极了，水中鱼儿历历可数，苏东坡曾以两行愉快的诗句描写船夫的黄头巾与青山的背景相映照，画面十分动人： 映山黄帽螭头舫， 夹道青烟鹊尾炉。  </p><p>他常常借一张和尚的躺椅，搬到附近竹林中；完全卸下官吏的尊严，脱下衣衫，赤身露体睡午觉。小和尚用敬畏的眼光偷看这位大学者，看到了别人无权一窥的场向。他看见——也许是自以为看见——苏东坡背上有七粒黑痣，排列的方位很象北斗七星。老和尚说，可见他是天廷派下来的神灵，暂时在人间作客而已  </p><p><strong>苏东坡眼中感官的生活和灵性的生活是同一回事，以诗意哲学化的人生观看来并没有什么冲突。有了诗，他热爱今生，不可能变成禁欲的和尚。有了哲学，他十分明智，也不会沉沦在“魔鬼”手中。他不会弃绝青山绿水，也不会弃绝美人、诗歌和酒肉。但是他有深度，不可能披上纨绔子弟肤浅、愤世嫉俗的外衣。</strong>  </p><p>苏东坡身为通判，有一次曾裁决一件与和尚有关的案子。灵隐寺有一位和尚名叫了然，常到红灯区走动，爱上一个名叫秀奴的少女。后来他床头金尽，衣衫槛楼，秀奴就不肯见他了。有一天晚上他喝醉酒又去找那个女孩，吃了闭门羹，就强闯进去，将她打死。于是他被控杀人。官吏审问他，发现他臂上刺了两行诗：“但愿生同极乐国，免教今世苦相思。”调查完毕，证物送到苏东坡手中。苏东坡忍不住写下这一首词： 这个秃奴，修行忒煞，云山顶上空持戒。只因迷恋玉楼人，鹑衣百结浑无奈。 毒手伤心，花容粉碎，色空空色今安在，臂间刺道苦相思，这回还了相思债。 和尚被送到刑场处决。  </p><p>苏太太聪明贤慧，不想用错方法，把丈夫逼到妓女怀中。此外她知道她丈夫是一个妻子或皇帝都无法阻挡的人，她采取明智的作风——充分信任他。  </p><p>苏东坡个性复杂多变，很难了解。他是大哲学家，不可能变成清教徒，但他又是儒家子弟，不可能变成酒鬼。<strong>他了解生命，珍惜生命，不会把时光完全浪费在醇酒美人身上。他是自然诗人，怀有特殊健全的神秘人生观，往往和自然的了解密切融合。我相信任何一个人和自然、四季、雨、雪、山、谷那么接近，接受它的治疗，一定不会心思闭塞，具有封闭的人生观。</strong>  </p><p>这是苏东坡最沮丧的时期，说也奇怪，诗人最悲哀的时候却写出了最好的作品。照中国的标准，他在这段期间达到诗词的成熟期。愤怒与尖酸都过去了，只留下满心安详与去意。  </p><p>韩琦和欧阳修已逝。富弼和范镇辞官归隐。司马光潜心著作。张方平沉迷酒杯，东坡的弟弟明哲保身，一句话也不说。东坡不够圆滑。<strong>一个人亲眼看到百性受苦，这只是该不该忘掉一切后果表达心中感慨的问题。也许他从来没有考虑过。</strong>  </p><p><strong>现在苏东坡很受欢迎，不仅因为他对抗洪水成功，也因为他亲身关切狱囚的健康和利益，当时很少太守这么做。他亲自去看犯人，第一次派狱医照顾病患。苏东坡指出，法律虽禁止地方官鞭死囚犯，对囚犯病死或失于照顾而死，却没有明文规定。囚犯也是百姓。他遂赢得囚犯亲友的感激。</strong>  </p><p><em>每一个人都是平等的。</em>  </p><p>套一句苏东坡自己的话，他始终如蝇在食，吐之乃已，到目前为止还平安无事。但是他“吐”一百次，终于被捉了。  </p><p>苏东坡笑着对他们说了一个故事： 真宗时代，皇帝四处探访隐居的大学者，有人推荐杨朴。杨朴不愿入京，却被押到朝中见皇帝。 “听说你会写诗”皇帝说。 “不，我不会。”杨朴想掩饰自己的才华，不愿从政。 “朋友们送你出来，有没有人写诗给你？”皇帝又问。 “没有，”杨朴说，“只有臣妻写了一首。” “请问诗中写什么？”陛下问他。 于是杨朴把妻子送行的诗念给皇帝听。全诗如下： 更休落魄贪酒杯， 且莫猖狂爱咏诗。 今日捉将官里去， 这回断送老头皮。 苏太太听到这个故事，热泪盈眶，却忍不住笑出来。这个故事出现在东坡的笔记中，不知道是不是他临时杜撰的。  </p><p><em>幽默</em>  </p><p>外在的工作与责任隐藏了一个人的本性。去掉这些时势和传统的陷阱，真我就出现了。<strong>苏东坡回到百姓群中，有如水里的海豹；在陆地上摇鳍摆尾的海豹只是半只海豹而已。</strong>  </p><p>解放的生活使他的心灵产生蜕变，又反映到作品中。刻薄的讽刺、尖锐的笔锋、一切激情与愤怒都过去了，代之而起的是光辉、温暖、亲切、宽容的幽默感，绝对醇美，完全成熟。<strong>哲学的价值就是教人笑自己。就我所知，动物只有猩猩会笑，但是我相信只有人才会笑自己。</strong>不知道这能不能称为神祗的笑容。希腊诸神充满人性的错误和缺点，他们一定常常有机会自嘲一番；但是基督教的上帝或天使太完美了，不可能这样做。把这种自嘲的特色称为堕落人类独一的美德，该算是一大恭维吧。  </p><p>任何情况下，幸福都是一种秘密。但是研究苏东坡的作品，就不难探出他幸福的奥秘了。   这位慷慨的天才对世人的贡献远超过他从世上收取的一切，他到处捕捉诗意的片刻，化为永恒，使我们大家都充实不少。  </p><p>归去来兮，吾归何处……人生底事，来往如梭，待闲看秋风，洛水清波，好在堂前细柳，应念我莫剪柔柯。仍传语江南父老，时与晒鱼蓑。  </p><p><strong>到了南京，苏东坡去看王安石，后者现在已是疲惫的病老头了。他们一起谈诗论佛。双方都是大诗人，佛家弟子，有不少话可说。传说有一次两人比诗，同韵同题，苏东坡赢了。王安石中途放弃。谈话中苏东坡不免责备王安石招来战祸，迫害学者。</strong>  </p><p>事实上，奢华的日子和简朴的日子在幸福方面倒没有什么差别。只有不配作高官的人才羡慕高官的荣宠。通常不想作官的人为当局一心争取，想作官的人却又不够资格。一旦“官愿”满足了，做大官的乐趣不见得胜过成功的铁匠。  </p><p>乐事可慕，苦事可畏，皆是未至时心尔。及苦乐既至，以身履之。求畏慕者初不可得况，既过之后复有何物。  </p><p>当时的知识分子只有两条路可走，不是做官就是自甘淡泊——淡泊通常代表贫穷。当然人可以以潜心学术，得到永远的声名；但是对许多人来说，不朽的声名就算有把握，也只是空腹的自我安慰罢了。  </p><p>这些年来苏东坡不断在策论中说，“独立思考”和“公正无私”是好大臣的重要条件。但是独立思考和意见公正却是党人最讨厌的。  </p><p><strong>诗、书、画最主要的材料就是两种液体：酒和墨；他们有上好美酒，上好名墨，还有最好的毛笔和最珍贵的纸张。</strong>  </p><p>把中国书法当做一种抽象画，也许最能解释其中的特性。中国书法和抽象画的问题其实非常相似。判断中国书法的好坏，批评家完全不管文字的意思，只把它视为抽象的构图。它是抽象画，因为它并不描绘任何可辨的物体，与一般绘画不同。中国字是由线条所构成，线条组合千变万化，书法就是把这些字完美凑出来，而且要和同一行、同一页的其它字体相配合。中国字是由最复杂的成份所构成，不免呈现一切构图的问题，包括轴线、轮廓、组织、对比、平衡、比例等等，尤其注重整体的统一概念。 一切艺术的问题都是节奏的问题，无论绘画、雕刻或音乐都是一样。既然美感就是动感，每一种形式都有隐含的韵律。就连建筑方面亦然，哥德式的教堂仿佛在沉思。美学上甚至可以用“冲”、“扫”、“粗鲁”等人格的形容词，这些都是韵律的观念。  </p><p>这种动作的韵律美观念改变了一切技术家对线条、质量、表面、构图和材料的看法。若属于力学而非静态的美，一切全是平衡的直线画，像工程师的蓝图一般，那就不值得考虑了。相反的艺术家必须寻找扭曲不平的树枝线条，只因为弯曲扭转才能显出生命和运动。这种不平均的线条我们很容易看出生命和动作，其中敏感的压力、休止和扫动以及树枝偶然的哗啦声都仔细保存下来。国画和书法可以说有一项基本原则，除非必要——譬如画书桌和茶几——千万别用均衡的线条。构图的概念也变了。中国艺术家绝不以静态的安排、线面的对比为满足，因为这些线面都是死的。画家因此强调活线条，这是国画技巧和其它绘画的一大差别。  </p><p>为了培养活线条的基础，书法家便回头观察大自然。自然的线条总让人想起动作，变化永无止尽。善跑的灵堤猎犬结实光滑，自有一种美姿；而爱尔兰小诜多毛矮胖，又另是一番风味。我们可以欣赏小鹿的灵巧，同时又爱慕狮掌强大的肌力。小鹿身材优美，不仅因为轮廓匀称，也因为它让人想起跳跃的动作，狮掌优美则因为让人想起飞扑，就是这种飞扑和跳跃的功能使线条具有活生生的协调感。若追求这种韵律美，我们可以欣赏大象庞然的身躯，小蛇扭曲的张力，甚至长颈鹿枯瘦笨拙的动作。所以自然的韵律永远充满机能，只因为线条和轮廓都是生长过程的结果，具有一定的作用。借自然丰富的韵律，才能极度锻炼我们的鉴赏眼光。中国书法家挥笔时想模仿的就是这种自然的律动，也只有最敏感的画笔才能模摹出来。有些笔触稳定而圆熟，令人想起狮掌的威力；有些令人想起马足的肌力，节骨分明。有些想表现明快的清爽感，字体有肩、有腰、有支架，正如完美的女性，或者像中国批评家所说的“如美人鬓带鲜花”。有些想效法枯藤难摹的雅姿，末端形成温文安定的小卷，用几片细叶加以平衡。别忘枯藤的平衡最完美，因为末端弯曲的角度和形状要看藤蔓整个的重量——茎株的支持点和残叶在哪一边而定。  </p><p>在八大山人的鱼鸟或石涛的兰花中也许更能看出印象派艺术的极端例证。无论画鱼画鸡画鸟，八大山人的画可以说是用最少线条、最少墨汁来表现最多的韵味。大艺术家只花几分钟，迅速泼墨完成一张鱼、马或人像图；不是成功就是失败，万一失败他就把纸条揉成一团，丢入字纸篓中，从头来起。  </p><p>我前面已说过，在位党和反对党也没有明确的权责。多数党统治的机能并不存在。于是政治游戏便成为个人之间的斗争，比西方更剧烈。<strong>但是东西方的政治规则完全一样：爬到顶端的一定是庸才。</strong>  </p><p>第一，好政客要会说一大堆话，却不透露任何消息。好官决不肯定什么，只用否定。只要学会“无可奉告”、“你说得对”等至理名言，好官就可以无往而不利。第二、他应该施惠于朋友。第三、他应该小心不得罪人。一个人如果不随便说话，爱用文雅、细柔、愉快的低语，又很喜欢施小惠给人家，他就算不位极人臣，也不会失势。他到死都有官做。  </p><p>苏东坡逃避政治，政治却在追逐他。他和司马光政见不合——<strong>独立的心灵永远不会完全一致</strong>——但是他到京师半年，司马光就去世了。苏东坡陷入显赫遭忌的地位。  </p><p>他维护意见不一的原则。信中指出，<strong>“若上之所可，不问其是非，下亦可之。上之所否，不问其曲直，下亦否之”</strong>，对国家并没有好处。君主和大臣应该互相提供意见，如果百官唯唯诺诺，就变成孔子所谓“足以丧邦”的跟屁虫了。  </p><p>当时政府最大的间题——中国每一朝代都是如此——就是冗官充斥。文人太多，官位太少，在“学而优则仕”的中国社会成为经常存在的弊端。除非现在能改变此一观念，否则教育普及就能把国家拖垮。我们要如何提供四亿五千万知识分子的官位呢？如果公职制度严格遵行，用人惟才，则考中的人数自然有限，素质也会提高。但是苏东坡时代已盛行亲族主义。  </p><p>事实上他已决心离开朝廷。他说君子如麟凤难求，小人<strong>“易进如蛆蝇，腥膻所聚，瞬息千万”</strong>。  </p><p>“聚蚊成雷，积羽成舟，寡不胜众也”。  </p><p>苏东坡对于这种零碎、没有组织的救病工作并不满意，他由政府基金拨出两千缗，自己又捐了五十两金子，在城中心众安桥建了一座公立医院。就我所知，这个“安乐坊”是中国最早的公立医院。三年内曾医过一千个病人，主管医院的道士由政府赠以紫袍和金钱。后来医院搬到湖边，改名“安济坊”，苏东坡离开后还继续看病。  </p><p>太后死前十天，范纯仁和苏子由等六位大臣进去看她。 “我大概无法复原了，”太后说，“不能长期看着你们。你们要尽力侍候小皇帝。” 大臣即将告退，太后指名要范纯仁留下来。于是哲宗叫别人退开，只剩范纯仁和吕大防。 朝中传闻太后谋反，要立自己的儿子为帝，太后问道，“皇上年幼，神宗托老身治国。九年里你们可曾看我特别照顾高家？” “没有，”吕大防说，“太后未曾厚待娘家，一切以邦国为重。” “正是如此，”太后含泪说，“所以老身临死才见不到自己的儿子和女儿。”她并没有派儿子在京师做官。 “太后必能康复，”吕大防说，“请听医生的劝告。您现在不该说这些事情。” “不，”太后说，“今天当你们的面，我要对皇上说几句话。我知道我死后很多大臣会愚弄他。孙子，你该当心。”她转向吕大防和范纯仁说：“我觉得老身死后，你们还是辞官归隐吧，小皇帝会用新人。” 她问侍从宫中有没有请大臣吃饭，她对吕大防和范纯仁说：“现在去用餐吧。明年此日，请记得老身。”  </p><p>一个王朝的悲剧在于皇后们有必要接连生出善良、聪明、能干的儿子、孙子和曾孙，皇室才能长保权位——这是生物学上人类从来没有听过的不保险假设。天才不生天才，迟早贤明的君主会生出邪恶、昏庸的后代。  </p><p>中国历史上若有一个时期可以称得上残暴与混乱的时代，那就是蔡京手下的政府了。他替皇帝建设精美的乐园，在中国历史中写下最可怕的一页，因为皇家乐园也用不着国人付出那么多悲惨的代价呀。园中每一块异石，每一朵奇花都曾牺牲几条人命。读到徽宗和大臣们赞美这个花园、假山、清溪、异石的诗句，我们不禁脊骨发冷，感受到中国文学史上从未有过的悲剧。悲剧在于作者并不知道这些。  </p><p>章惇劝皇帝挖司马光的坟墓，打烂棺材，鞭尸示众，以警告所有不忠的臣民。在小皇帝心目中，司马光变成元祐时代奸诈、不忠、邪恶的象征。上朝的时候大家都表示赞成，只有许将不说话。小皇帝打量他，退朝后叫他留下来。  </p><p>他转念一想：<strong>“此间有什么歇不得处。由是心若挂钩之鱼忽得解脱。若人悟此，当恁么时也不妨熟歇。”</strong>  </p><p>他又恢复了自然的本性。他在广州曾买了一些檀香，现在他常关门静坐，享受奇特的异香，反省自己以往的错误。有时午后小睡一回，凉爽的江风吹进窗口，房顶的鸟鸦打断他的幽梦，他突然觉得自己卸下一切责任。他看到大江的光影射入他房中。真美，他暗自说，美得像清空的明月。他不懂为什么有人喜欢云中的翳月。他觉得晴空是光明磊落的象征。  </p><p>苏东坡写过一篇酒颂。就是不解杯中乐趣的人读到他描写半痴半醉的幸福状态也会为之入迷： “浊醪有妙理赋 酒勿嫌浊，人当取醇。失忧心于昨梦，信妙理之疑神……仔人之生，以酒为命。常因既醉之适，人识此心之正。稻米无知，岂解穷理。麴栗有毒，安能发性，乃知神物之自然，盖与天工而相并。得时行道，我则师齐相之饮醇。远害全身，我则学徐公之中圣。湛若秋露，穆如春风。疑宿云之解驳，漏朝日之暾红。初体栗之失去，旋眼花之扫空……兀尔坐忘，浩然天纵。如如不动而体无碍，了了常知而心不用。座中客满。惟忧百種之空。身后名轻，但觉一杯之重。今夫明月之珠，不可以襦，夜光之璧，不可以哺。刍肉饱我而不我觉，布帛袄我而不我娱。惟此君独游万物之表，盖天下不可一日而无。在醉常醒，孰是狂人之乐。得意忘味，始知至道之腴。”  </p><p>她是虔诚的佛教徒，临死还念着《金刚经》的一道偈/jié/语： <strong>一切有为法， 如梦幻泡影。 如露亦如电， 应作如是观。</strong> 根据她的遗嘱，苏东坡将她安葬在城西丰湖边的山脚上，靠近一座亭台和几间佛寺。墓后有山溪瀑布流入湖中。  </p><p>玉骨那愁瘴雾，冰肌自有仙风。海仙时遣探花丛，倒挂绿毛么凤。 素面常嫌粉污，洗妆不退唇红。高情已逐晓云空，不与梨花同梦。  </p><p>苏东坡自以为晚年可以定居惠州，没想到突然被贬到海外。新居落成两个月，移居海南岛的命令就来了。有人记载说，他写了两行诗描述他在春风中小睡，聆听屋后庙院钟声的情景。章惇读到这段诗，就说：“原来苏东坡那么惬意。”于是颁布了移居的命令。  </p><p>但是他不屈的灵魂和人生观不容许他失去生活的乐趣：“尚有此身付与造物者，听其运转流行坎止无不可者。故人知之，免忧热。”他写信给一位朋友说。  </p><p>“吾始至南海，环视天水无际，凄然伤之曰‘何时得出此岛也’。已而思之：天地在积水中，九洲在大浪海中，中国在少海中。有生孰不在岛者。譬如注水于地，小草浮其上，一蚁抱草叶求活，已而水干，遇他蚁而泣日‘不意尚能相见尔’。小蚁岂知瞬间竟得全哉？思及此事甚妙。与诸友人小饮后记之。” 苏东坡也许是倔强，也许是真的掌握了自己。至少他从未失去幽默感。  </p><p>苏东坡曾经对他弟弟说：“我上可以陪玉皇大帝，下可陪卑田院乞儿。在我眼中天下没有一个不是好人。”现在他和默默无闻的穷学者、农夫农妇交往。他和这些纯朴小民谈话不必有戒心，自由自在，最能表现自己。家里一天没有客人他就不自在，别人不来他就出去拜访邻居。和黄州时期一样，他与高官、平民、学者、农夫杂处。聊天总是他发言；他天生爱说话。但是他也希望别人开口。他带着海南种的大犬“乌嘴”到处闲逛。他和村民坐在槟榔树下，想畅谈一番。无知的穷农夫能对他说什么呢？农夫对这位大学者敬畏万分，“我们不知道要谈什么。”苏东坡说：“那就谈鬼吧，说几个鬼故事来听听。”对方会说他们没听过什么好的鬼故事，他说：“没关系，就谈你们听过的好了。”后来苏过告诉朋友，他父亲如果一天没看到客人，就好像有什么不对劲似的。  </p><p>小屋完成后的两年半期间，苏东坡过着无忧无虑却十分贫穷的生活。他有两位妙友，一个是替他转信的广州道士何德顺，另一位是四处游荡，送他食物、药物、米、泡菜、书本的小学者（吴复古）。  </p><p>他写了不少有关药草的笔记，我要特别谈谈荨麻治风湿的办法，荨麻含有荨麻素和黄体素，像毒藤似的，碰到皮肤就会肿痛。照他的说法，把荨麻敷在风湿起始的肿痛关节上，全身各处的酸痛都会停止。他还热烈信仰苍耳。苍耳到处都有，对人无害，不管吃多久怎么吃都可以（含有脂肪，少量树胶、维他命C1和耳醣）。他写下苍耳白粉碾制的办法，把叶灰用温火烧二十四小时就成了。白粉吃下去据说可以美化皮肤，“满肌如玉’。有些笔记谈到蔓菁、芦菔和苦荠，苏东坡称为‘葛天氏之遗民”的美食，价值高，味道又好吃。  </p><p>他请子由写序，在一封信中说：“然吾于渊明，岂独好其诗也哉。如其为人，实有感焉。”很多崇拜苏东坡的人也会这样说。  </p><p><strong>徽宗继位时，国家的命脉已经腐蚀削弱。有个性、有才华、有正义感的君子是文明社会的珍贵产物，需要长时间培养茁壮。司马光、欧阳修、范纯仁、吕公著的时代过去了，那一代的人已经分别下狱、流放、病死、老死或被杀。独立批评，勇敢思考写作的气氛已经僵化，整个政治生命都污染了。苏东坡师徒因言论而受罪，不愿意再入政坛，何况政风又对他们不利。皇帝一声命令，不可能马上有一群正直、博学、大无畏的学者出现朝中。己尝了八年权力滋味的大批政客更不可能放弃权位。</strong>  </p><p>显然神宗皇后和她婆婆一样，善于感受国家的利益，她们具有单纯的女性本能，会判断好人坏人。<strong>批评家和历史家迷恋优美的文辞和抽象的特征，善于研究某一时期深奥的问题和政策，却忘记我们看人最后总逃不过“好”、“坏”这两个形容词。总括一个人的事迹和个性，“好人”就是最高的赞美。苏东坡服侍的太后们似乎从来不管政府领袖的问题和政策。当然章惇是一个坚强果断的人。惠卿是雄辩家。蔡京生气勃勃、精明能干。但太后一概把他们归类成坏人。</strong>  </p><p>他本来想等福建大船，苦等不来，就随吴复古、苏过和爱犬（乌嘴）渡海，一行人到雷州去找秦观。吴复古又失去踪影。苏东坡和吴复古一生游遍全中国，不同的是苏东坡被别人的命令赶来赶去，吴复古却是自愿飘泊。追忆往事，苏东坡似乎恨不能和他的朋友交换际遇。那样一定快乐得多，也自由得多。  </p><p>他在广州受到热诚的招待。他到海南岛第二年，有人传说他死了。有一位朋友在宴席上开玩笑说： “我以为你死了。” 苏东坡说：“不错，我死了，在地府半路上碰到章惇，我又决定折回来。”  </p><p>大家庭有不少小孩和年轻的妇女，他们就乘船到南雄。<strong>没走多远，吴复古和一群和尚追上他们，陪老苏畅游了几天。然后吴复古突然病倒去世，一切都那么简单。苏东坡问他临终有什么交代。吴复古笑笑合上了眼睛。</strong>  </p><p>苏东坡最初和最终的乐趣都是写作。他把自己在南方所写的诗文拿给钱世雄看，眼睛闪闪发光，似乎忘记了一切。有时候他还能写短简和题跋，其中包括一篇桂酒颂，他知道好友会仔细珍藏，就把这篇文章送给钱世雄。  </p><p><strong>苏东坡缓缓低语：“西天也许存在，不过设法到那儿也没有用。”钱世雄站在旁边，就对他说：“尤其这个时候，你一定要试试看。”苏东坡最后一句话是“试就不对了”。那就是他的道家哲学，解脱在于自然而不自觉的善行。</strong>  </p><p>由尘世的标准来说，苏东坡的一生相当坎坷不幸。有一次孔子的门生问起两个为信念而饿死的古圣人。门生问孔子，伯夷叔齐死前有没有悔意。孔子说：“他们求仁而得仁，为什么要后悔呢？” 苏东坡今生的“浩然之气”己经用光。人生不过是性灵的生活，而性灵是控制人类事迹和个性的力量，与生俱来，只能靠生命和际遇和环境来表现。正如苏东坡的描写：<strong>“浩然之气不依形而立，不恃力而行，不待生而存，不随生而亡矣。故在天为星辰，在地为河岳，幽则为鬼神，而明则复为人。此理之常，无足怪者。”</strong>  </p><p><strong>读到苏东坡的生平，我们等于追察人类心智和性灵暂时显现在地球上的生命。苏东坡死了，他的名字只是一段回忆，但是他却为我们留下了他灵魂的欢欣和心智的乐趣，这些都是不可磨灭的宝藏。</strong></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今年五月份读完的这本书，暑假开始的时候决定把读书笔记和读后感整理放在博客上。关于《苏东坡传》的文章一共有两篇，这是第一篇，内容为读书时的摘录。第二篇如下：  &lt;/p&gt;
&lt;p&gt;人生到处知何似， 应似飞鸿踏雪泥， 泥上偶然留指爪， 鸿飞那复计东西。&lt;/p&gt;
&lt;p&gt;拿西洋作家为例，李白可以媲美雪莱或拜伦，是一个燃烧自己展现出瞬间壮景的文学彗星。杜甫就象米尔顿，是一个热心的哲学家和老好人，以贴切、渊博的古典比喻写出了丰富的作品。苏东坡永远年轻。他性格比较象萨克莱，政治和诗词的盛名则象雨果，同时又具有约翰生博士那份动人的本质。不知怎么约翰生博士的痛风病直到今天还叫我们感动，米尔顿的瞎眼却不尽然。如果约翰生同时又兼有甘斯伯劳的特色，而且象波普用诗词批评政治，又象史维夫特吃过那么多苦而没有史维夫特的尖酸味儿，我们就能找出一个英国的类比了。苏东坡的道精神由于遭受许多困难而更醇美，却没有变酸。今天我们爱他，只因为他吃苦吃得太多了。&lt;/p&gt;
&lt;p&gt;苏东坡在面对痛苦时展现出了超常的淡然，他像是一个已经预知了自己一生的人。我最爱他的地方是他面对所有事情的坦诚。  &lt;/p&gt;
&lt;p&gt;今年五月份读完的这本书，暑假开始的时候决定把读书笔记和读后感整理放在博客上。关于《苏东坡传》的文章一共有两篇，这是第一篇，内容为读书时的摘录。第二篇如下：&lt;/p&gt;</summary>
    
    
    
    <category term="阅读" scheme="https://blog.crocodilezs.top/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="苏东坡" scheme="https://blog.crocodilezs.top/tags/%E8%8B%8F%E4%B8%9C%E5%9D%A1/"/>
    
  </entry>
  
  <entry>
    <title>《祭亡妻程氏文》 苏洵</title>
    <link href="https://blog.crocodilezs.top/202002/%E7%A5%AD%E4%BA%A1%E5%A6%BB%E7%A8%8B%E6%B0%8F%E6%96%87/"/>
    <id>https://blog.crocodilezs.top/202002/%E7%A5%AD%E4%BA%A1%E5%A6%BB%E7%A8%8B%E6%B0%8F%E6%96%87/</id>
    <published>2020-02-13T05:50:03.000Z</published>
    <updated>2022-05-06T13:24:22.435Z</updated>
    
    <content type="html"><![CDATA[<p>与子相好，相期百年。不知中道，弃我而先。</p><p>东坡刚考中准备做官的时候，他的母亲去世了，她临死还没有听到京师的好消息。《苏东坡传》中林语堂先生引了苏洵的《祭亡妻程氏文》的一段话：“我归旧庐，无不改移。魂兮未泯，不日来归。”</p><p>昨晚读到这里的时候有被触动。今天去找来了原文查词典把文章理顺了。</p><p>呜呼！与子相好，相期百年，不知中道，弃我而先。  </p><p>我徂京师，不远当还；嗟子之去，曾不须臾。子去不返，我怀永哀。反复求思，意子复回。  </p><p>人亦有言，死生短长，苟皆不欲，尔避谁当，我独悲子。  </p><p>生逢百殃，有子六人，今谁在堂？唯轼与辙，仅存不亡。咻呴抚摩，既冠既昏，教以学问，畏其无闻。昼夜孜孜，孰知子勤？  </p><p>提携东去，出门迟迟。今往不捷，后何以归？二子告我，母氏劳苦，今不汲汲，奈后将悔。大寒酷热，崎岖在外。亦既荐名，试于南宫。文字炜炜，叹惊群公。二子喜跃，我知母心，非官实好，要以文称。  </p><p>我今西归，有以藉口。故乡千里，期母寿考。归来空堂，哭不见人。伤心故物，感涕殷勤。  </p><p>嗟予老矣，四海一身。自子之逝，内失良朋。孤居终日，有过谁箴？  </p><p>昔予少年，游荡不学，子虽不言，耿耿不乐。我知子心，忧我泯没，感叹折节，以至今日！ 呜呼死矣！不可再得。  </p><p>安镇之乡，里名可龙，隶武阳县，在州北东。有蟠其丘，惟子之坟。凿为二室，期与子同。骨肉归土，魂无不之。我归旧庐，无不改移。魂兮未泯，不日来归。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;与子相好，相期百年。不知中道，弃我而先。&lt;/p&gt;
&lt;p&gt;东坡刚考中准备做官的时候，他的母亲去世了，她临死还没有听到京师的好消息。《苏东坡传》中林语堂先生引了苏洵的《祭亡妻程氏文》的一段话：“我归旧庐，无不改移。魂兮未泯，不日来归。”&lt;/p&gt;
&lt;p&gt;昨晚读到这里的时候有被触动</summary>
      
    
    
    
    <category term="阅读" scheme="https://blog.crocodilezs.top/categories/%E9%98%85%E8%AF%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>《极简欧洲史》读书笔记</title>
    <link href="https://blog.crocodilezs.top/202002/%E6%9E%81%E7%AE%80%E6%AC%A7%E6%B4%B2%E5%8F%B2%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <id>https://blog.crocodilezs.top/202002/%E6%9E%81%E7%AE%80%E6%AC%A7%E6%B4%B2%E5%8F%B2%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</id>
    <published>2020-02-11T12:18:03.000Z</published>
    <updated>2022-05-23T17:35:16.353Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2020/02/11/EG6KiIgvJp5sq4m.png" alt=""></p><p>从2019年年底到2020年年初我一直在读的这本书——《极简欧洲史》，我应该不会向身边的人推荐这本书，因为这是一本比较枯燥的历史课本。唯一能让我对历史提起兴趣的描写性语言少之又少。在我读到大概三分之二的时候就想放弃，然而最后还是因为微职的读书小组坚持读下来了。这本书越往后节奏越快，到后面快要变成了流水账。不过通过这本书我也做了非常多的思考：像我这样对历史毫无兴趣的理科生还需不需要读历史？有没有更舒服的、更有趣地了解历史的方式？</p><span id="more"></span><blockquote><p>本书的内容原本是授课用的讲义，目的是让澳大利亚的大学生对欧洲历史有个初步的认识。但身为老师的我并不是从最前面开始，按部就班讲到最后。我的做法是先为学生很快地做个概论，再回头补充细节。<br>——《极简欧洲史》引言</p></blockquote><p>既然整本书都只是知识性的，那这本书的读书笔记应该是一份复习提纲。然而我并没有按照书的逻辑做摘录，而是仅仅把其中触动我的一些文字摘录了出来。现在回去看，这些摘录并不是关于“历史”的，而是关于“人”的。</p><blockquote><p>在雅典，死刑通常都是立刻执行，这次却往后推迟了，因为宗教庆典的关系。苏格拉底大可趁机潜逃，说不定那些官员还暗自希望他逃之夭夭，但他却拒绝逃跑。他问：“既然我不能永远活着，那又何必苟且偷生？活着不是目的，好好活着才是。我曾在雅典的法治下过着很好的生活，如今我已准备好接受惩罚。”直到最后一刻，他还是充满了哲学思辨。直到他的镣铐被取下，他还在发表高论，说痛苦和享乐只是一线之隔。<br>他被判处服毒芹汁自绝，必须在一日将尽时服下毒药，他的弟子求他晚点再喝，现在太阳还没下山呢！苏格拉底回复道，要是他这样偷生，自己看了都觉得荒谬，他平静地接过毒药一饮而尽，全无半点神伤，很快就药效发作而亡。</p><p>“他该受到什么报应，”鲁克丽丝说，“我交由你们决定。至于我，虽然失节非我之过，但我要接受自己的惩罚。失贞的女人应该得到什么报应，我绝不会首开避脱的先例。”话声甫落，她便从衣袍中掏出一把刀刺入心脏，应声倒下，就此香销玉殒。她的父亲和丈夫哀恸欲绝。两人只能呆立着无助地哭泣，但布鲁图斯拔出鲁克丽丝胸前染血的刀，举着它高喊：“我要对这位烈女的血发誓：在她被暴君蹧蹋之前，没有人比她更为贞洁，我也对上帝发誓，我要借助刀剑、烈火以及所有能让我更强大的东西，追捕骄傲者塔克文、邪恶的王后及其所有的子女，绝不让他们任何人再登上罗马的王座。”</p><p>拿破仑是启蒙运动之子，深信革命所揭橥的诸多原则，却不相信人民有权统治自己。自1789年之后，法国在这个志业上可说是节节落败，拿破仑的政见因此非常吸引人。他是独裁者中最有魅力的一个，他不准任何团体享有特权，所有国民一律得到平等对待，国家提供所有孩童受教育的机会，所有职务都要公开选才。他延揽各方人才进入政府，无论是保王派还是共和派，雅各宾恐怖政权的支持者还是反对者，完全不计较他们过去在革命中扮演的角色。他只交给他们一个使命：创立一个有理性、有秩序的政府体制。</p><p>你一定会喜欢这些平民百姓。他们很脏很臭，看来很不讨喜，因为他们一年到头无分寒暑地日夜操劳，形容憔悴、伤痕累累、营养不良、疾病缠身。那为什么你还会喜欢他们？<br>因为他们的命运很容易追踪；百年复百年，他们做的都是同样的事，几乎所有的人都在耕种。</p></blockquote><p>历史是枯燥的，但历史河流中的每个人都是鲜活的。我们可以通过面对死亡时平淡和毅然的苏格拉底去了解雅典的法庭；通过坚贞的鲁克莉丝了解罗马共和政体的开启；通过飒爽的拿破仑了解法国的革命。当我们从人的角度切入历史的时候，我们可以收获更多的共鸣。《中国的历史精神》里说“历史便即是人生”，因为历史确实是由无数的人生构成的。历史本身就是我们人生整个过往的经验。</p><p>当我发现人物对于历史的重要性之后，也明确了自己想要了解历史的一个更好的途径应该是人物传记。所以也从自己最喜欢的苏轼开始读《苏东坡传》，上一周的读书时间也比较开心，不仅仅是因为苏轼本身就是一个很有意思的人，林语堂的文风也比较轻松诙谐。苏轼身上的纯真相当感染人，也对我自己面对选择的态度有挺大的触动。希望这周能把《苏东坡传》读完吧。</p><p>以古为鉴，可知兴替；以人为鉴，可正德身。</p><div style="text-align:right">——唐太宗</div><!-- more -->]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/02/11/EG6KiIgvJp5sq4m.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;从2019年年底到2020年年初我一直在读的这本书——《极简欧洲史》，我应该不会向身边的人推荐这本书，因为这是一本比较枯燥的历史课本。唯一能让我对历史提起兴趣的描写性语言少之又少。在我读到大概三分之二的时候就想放弃，然而最后还是因为微职的读书小组坚持读下来了。这本书越往后节奏越快，到后面快要变成了流水账。不过通过这本书我也做了非常多的思考：像我这样对历史毫无兴趣的理科生还需不需要读历史？有没有更舒服的、更有趣地了解历史的方式？&lt;/p&gt;</summary>
    
    
    
    <category term="阅读" scheme="https://blog.crocodilezs.top/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="极简欧洲史" scheme="https://blog.crocodilezs.top/tags/%E6%9E%81%E7%AE%80%E6%AC%A7%E6%B4%B2%E5%8F%B2/"/>
    
    <category term="历史" scheme="https://blog.crocodilezs.top/tags/%E5%8E%86%E5%8F%B2/"/>
    
  </entry>
  
  <entry>
    <title>Price Suggestion Chanllenge</title>
    <link href="https://blog.crocodilezs.top/201911/Price_Suggestion_Chanllenge/"/>
    <id>https://blog.crocodilezs.top/201911/Price_Suggestion_Chanllenge/</id>
    <published>2019-11-12T08:22:10.000Z</published>
    <updated>2022-05-23T17:15:27.378Z</updated>
    
    <content type="html"><![CDATA[<h2 id="实验题目"><a href="#实验题目" class="headerlink" title="实验题目"></a>实验题目</h2><h3 id="题目背景"><a href="#题目背景" class="headerlink" title="题目背景"></a>题目背景</h3><p>考虑到网上销售的产品数量，产品定价在规模上变得更加困难。服装有很强的季节性定价趋势，受品牌影响很大，而电子产品价格根据产品规格波动。如何根据以往信息进行合理定价，有效地帮助商家进行商品的销售是一个有意义的问题。</p><span id="more"></span><h3 id="分析目标"><a href="#分析目标" class="headerlink" title="分析目标"></a>分析目标</h3><p>通过给出的商品描述、商品类别和品牌信息，并结合训练数据中的商品价格来给新商品定价格。Eg ：<br><img src="image/1-1.png" alt=""></p><p>显然 <code>Versace</code> 的衣服价格上应该远高于美特斯邦威的衣服，并且在商品描述中，可以发现两者描述有细微差别。 </p><blockquote><p>本 project 旨在对文本信息进行分析，提取文本信息中重要信息，推导出和价格之间的潜在关系 </p></blockquote><h3 id="数据字段分析"><a href="#数据字段分析" class="headerlink" title="数据字段分析"></a>数据字段分析</h3><p><img src="image/1-2.png" alt=""></p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul><li><code>train.csv</code> 训练集 （含price）</li><li><code>test.csv</code> 测试集 (不含price) ; <code>label_test.csv</code> 测试集 中对应的 <code>price</code></li><li><code>f_test.csv</code> 最终的评价数据集 （不含 <code>price</code> ）</li></ul><h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><p>评价的使用的是 <code>Mean Squared Logarithmic Error</code>: 计算的方式如下</p><script type="math/tex; mode=display">MSLE = \cfrac{1}{n}\sum_{i=1}^n(log(p_i+1)-log(\alpha_i+1))^2</script><p>其中$n$代表测试集的样本数；$p_i$代表的是预测的商品价格值；$\alpha_i$代表实际的销售价格。</p><h3 id="作业要求"><a href="#作业要求" class="headerlink" title="作业要求"></a>作业要求</h3><p>提交的最后文件内容为：  </p><ul><li>最终代码文件（请写清楚使用了那些库，以及相应库的版本，可使用 <code>pip list</code> 命令查看版本，确保能顺利运行）</li><li>在 <code>f_test.csv</code>数据集上的结果</li><li>分析文档<br>请不要是简单的代码粘贴，加入分析过程<br>将你对于数据的理解记录下来，简单来说，缺失值处理这种基本操作<br>写出你的尝试的各种方法，为了解决rank太低的情况下分数太低<br>写一份同学负责哪一部分代码，每一部分没有区别，主要是为了给代码风格打分。  </li></ul><h3 id="提交结果文件格式"><a href="#提交结果文件格式" class="headerlink" title="提交结果文件格式"></a>提交结果文件格式</h3><ul><li>结果文件名为</li><li>提交格式<ul><li>第一行为 test_id \t price 的表头</li><li>接下来的每行为id \t predict_price</li></ul></li></ul><h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h2><p><em>这次实验难度很大，我们所有参考资料均在实验报告的末尾注明</em></p><h3 id="一、样例代码的学习"><a href="#一、样例代码的学习" class="headerlink" title="一、样例代码的学习"></a>一、样例代码的学习</h3><p>首先尝试了给出的样例代码，了解了解决这个问题的大致思路。解决这个价格预测问题的主要过程是：导入数据和数据探索、数据预处理、模型构建、价格预测和测评。</p><h4 id="导入数据和数据探索"><a href="#导入数据和数据探索" class="headerlink" title="导入数据和数据探索"></a>导入数据和数据探索</h4><p>导入数据和初步了解数据<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data = pd.read_csv(<span class="string">&#x27;../data/4/train.csv&#x27;</span>, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">&#x27;../data/4/test.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">train_data.info()</span><br></pre></td></tr></table></figure><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;</span><br><span class="line">RangeIndex: 300000 entries, 0 to 299999</span><br><span class="line">Data columns (total 8 columns):</span><br><span class="line">train_id             300000 non-null int64</span><br><span class="line">name                 300000 non-null object</span><br><span class="line">item_condition_id    300000 non-null int64</span><br><span class="line">category_name        298719 non-null object</span><br><span class="line">brand_name           171929 non-null object</span><br><span class="line">price                300000 non-null float64</span><br><span class="line">shipping             300000 non-null int64</span><br><span class="line">item_description     300000 non-null object</span><br><span class="line">dtypes: float64(1), int64(3), object(4)</span><br><span class="line">memory usage: 18.3+ MB</span><br></pre></td></tr></table></figure></p><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p>首先处理属性，训练数据首先要删去<code>price</code>，再去掉没有用处的 <code>train_id</code> 或者 <code>test_id</code>。通过观察上面的数据属性可知 <code>category_name</code> 和 <code>brand_name</code>有数据缺失，样例代码直接用 <code>missing</code>填充。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">featureProcessing</span>(<span class="params">df</span>):</span><br><span class="line">    <span class="comment"># delete the data that will not be used</span></span><br><span class="line">    df = df.drop([<span class="string">&#x27;price&#x27;</span>, <span class="string">&#x27;test_id&#x27;</span>, <span class="string">&#x27;train_id&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># deal with the missing value with a default value</span></span><br><span class="line">    df[<span class="string">&#x27;category_name&#x27;</span>] = df[<span class="string">&#x27;category_name&#x27;</span>].fillna(<span class="string">&#x27;missing&#x27;</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">    df[<span class="string">&#x27;brand_name&#x27;</span>] = df[<span class="string">&#x27;brand_name&#x27;</span>].fillna(<span class="string">&#x27;missing&#x27;</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">    df[<span class="string">&#x27;item_description&#x27;</span>] = df[<span class="string">&#x27;item_description&#x27;</span>].fillna(<span class="string">&#x27;No&#x27;</span>)</span><br><span class="line">    <span class="comment"># convert the data : int -&gt; str</span></span><br><span class="line">    df[<span class="string">&#x27;shipping&#x27;</span>] = df[<span class="string">&#x27;shipping&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">    df[<span class="string">&#x27;item_condition_id&#x27;</span>] = df[<span class="string">&#x27;item_condition_id&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure></p><h4 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h4><p>首先做模型的输入，通过<code>CountVectorizer</code> 和 <code>TfidfVectorizer</code>生成词频的矩阵， <code>Tfidf</code> 的效果更优，因为考虑了各词在所有字段钟出现的次数，生成的词频矩阵是带有权重的。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vectorizer = FeatureUnion([</span><br><span class="line">    (<span class="string">&#x27;name&#x27;</span>, CountVectorizer(ngram_range=(<span class="number">1</span>, <span class="number">2</span>), max_features=<span class="number">50000</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;name&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;category_name&#x27;</span>, CountVectorizer(token_pattern=<span class="string">&#x27;.+&#x27;</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;category_name&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;brand_name&#x27;</span>, CountVectorizer(token_pattern=<span class="string">&#x27;.+&#x27;</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;brand_name&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;shipping&#x27;</span>, CountVectorizer(token_pattern=<span class="string">&#x27;\d+&#x27;</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;shipping&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;item_condition_id&#x27;</span>, CountVectorizer(token_pattern=<span class="string">&#x27;\d+&#x27;</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;item_condition_id&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;item_description&#x27;</span>, TfidfVectorizer(ngram_range=(<span class="number">1</span>, <span class="number">3</span>),max_features=<span class="number">100000</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;item_description&#x27;</span>))),</span><br><span class="line">])</span><br></pre></td></tr></table></figure><p>利用岭回归，实现价格预测。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ridgeClassify</span>(<span class="params">train_data, train_label</span>):</span><br><span class="line">    ridgeClf = Ridge(</span><br><span class="line">        solver=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">        fit_intercept=<span class="literal">True</span>,</span><br><span class="line">        alpha=<span class="number">0.5</span>,</span><br><span class="line">        max_iter=<span class="number">500</span>,</span><br><span class="line">        normalize=<span class="literal">False</span>,</span><br><span class="line">        tol=<span class="number">0.05</span>)</span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    ridgeClf.fit(train_data, train_label)</span><br><span class="line">    <span class="keyword">return</span> ridgeClf</span><br></pre></td></tr></table></figure><br>通过对数据集的了解和对样例代码的学习，我们了解到优化这个问题的答案有三个角度可以入手：</p><ol><li>数据预处理：怎样处理缺失值？数据该怎样结合？</li><li>形成词频矩阵时进行优化：调整 <code>CountVectorizer</code> 和 <code>TfidfVectorizer</code> 的参数</li><li>模型的选择和优化：尝试岭回归之外的模型、调整模型参数。</li></ol><h3 id="二、尝试更多的模型"><a href="#二、尝试更多的模型" class="headerlink" title="二、尝试更多的模型"></a>二、尝试更多的模型</h3><p>在上面的样例代码中，利用岭回归模型得到的结果是3.01左右。经过之前课上的提示和网上的资料查找，我们准备再去尝试一下 <code>MLP</code>模型和 <code>Lgmb</code>模型。在粗略的尝试了两个模型之后我们决定进一步利用 <code>MLP</code> 进行下一步的优化。</p><h4 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a><code>MLP</code></h4><p><code>MLP</code> 模型的结果如下：<br><img src="image/2-2.png" alt=""></p><h4 id="LGBM"><a href="#LGBM" class="headerlink" title="LGBM"></a><code>LGBM</code></h4><p><code>Lgbm</code> 模型的结果如下：<br><img src="image/2-1.png" alt=""></p><h4 id="MLP-和-LGBM-结合"><a href="#MLP-和-LGBM-结合" class="headerlink" title="MLP 和 LGBM 结合"></a><code>MLP</code> 和 <code>LGBM</code> 结合</h4><ol><li>特征处理</li></ol><ul><li>导入数据集</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读文件</span></span><br><span class="line">   train = pd.read_csv(<span class="string">&#x27;data/train.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">   test = pd.read_csv(<span class="string">&#x27;data/test.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">   <span class="comment"># 训练数据和测试数据一起处理</span></span><br><span class="line">   df = pd.concat([train, test], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><ul><li>缺失值处理</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#对缺失值进行处理</span></span><br><span class="line">   df[<span class="string">&#x27;category_name&#x27;</span>] = df[<span class="string">&#x27;category_name&#x27;</span>].fillna(<span class="string">&#x27;MISS&#x27;</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">   df[<span class="string">&#x27;brand_name&#x27;</span>] = df[<span class="string">&#x27;brand_name&#x27;</span>].fillna(<span class="string">&#x27;missing&#x27;</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">   df[<span class="string">&#x27;item_description&#x27;</span>] = df[<span class="string">&#x27;item_description&#x27;</span>].fillna(<span class="string">&#x27;No&#x27;</span>)</span><br><span class="line">   <span class="comment">#数据类型处理</span></span><br><span class="line">   df[<span class="string">&#x27;shipping&#x27;</span>] = df[<span class="string">&#x27;shipping&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">   df[<span class="string">&#x27;item_condition_id&#x27;</span>] = df[<span class="string">&#x27;item_condition_id&#x27;</span>].astype(<span class="built_in">str</span>)</span><br></pre></td></tr></table></figure><ul><li>特征向量化</li></ul><p>使用 <code>sklearn</code> 库中的 <code>CountVectorizer</code> 类将文本特征进行向量化处理，并使用 <code>FeatureUnion</code> 进行特征联合</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vectorizer = FeatureUnion([</span><br><span class="line">        (<span class="string">&#x27;name&#x27;</span>, CountVectorizer(</span><br><span class="line">            ngram_range=(<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            max_features=<span class="number">100000</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;name&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;category_name&#x27;</span>, CountVectorizer(</span><br><span class="line">            token_pattern=<span class="string">&#x27;.+&#x27;</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;category_name&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;brand_name&#x27;</span>, CountVectorizer(</span><br><span class="line">            token_pattern=<span class="string">&#x27;.+&#x27;</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;brand_name&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;shipping&#x27;</span>, CountVectorizer(</span><br><span class="line">            token_pattern=<span class="string">&#x27;\d+&#x27;</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;shipping&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;item_condition_id&#x27;</span>, CountVectorizer(</span><br><span class="line">            token_pattern=<span class="string">&#x27;\d+&#x27;</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;item_condition_id&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;item_description&#x27;</span>, TfidfVectorizer(</span><br><span class="line">            ngram_range=(<span class="number">1</span>, <span class="number">3</span>),</span><br><span class="line">            max_features=<span class="number">200000</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;item_description&#x27;</span>),</span><br><span class="line">            stop_words=<span class="string">&#x27;english&#x27;</span>)),</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure><ol><li>模型构建</li></ol><p>对特征分别使用岭回归模型，<code>Lgbm</code> 模型和 <code>mlp</code> 模型进行训练，在本地测试得到的解分别为3.01，3.00，0.26</p><ul><li>岭回归模型</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ridge_classify</span>(<span class="params">train_data,train_label</span>):</span><br><span class="line">    <span class="comment">#模型</span></span><br><span class="line">    model = Ridge(</span><br><span class="line">            solver=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">            fit_intercept=<span class="literal">True</span>,</span><br><span class="line">            alpha=<span class="number">0.4</span>,</span><br><span class="line">            max_iter=<span class="number">100</span>,</span><br><span class="line">            normalize=<span class="literal">False</span>,</span><br><span class="line">            tol=<span class="number">0.05</span>)</span><br><span class="line">    <span class="comment">#训练</span></span><br><span class="line">    model.fit(train_data, train_label)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><ul><li><code>lgbm</code>模型 </li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lgbm_classify</span>(<span class="params">train_data,train_label</span>):</span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.75</span>,</span><br><span class="line">        <span class="string">&#x27;application&#x27;</span>: <span class="string">&#x27;regression&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">3</span>,</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">100</span>,</span><br><span class="line">        <span class="string">&#x27;verbosity&#x27;</span>: -<span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;RMSE&#x27;</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    train_X, valid_X, train_y, valid_y = train_test_split(train_data, train_label, test_size=<span class="number">0.1</span>, random_state=<span class="number">144</span>)</span><br><span class="line">    d_train = lgb.Dataset(train_X, label=train_y)</span><br><span class="line">    d_valid = lgb.Dataset(valid_X, label=valid_y)</span><br><span class="line">    watchlist = [d_train, d_valid]</span><br><span class="line"></span><br><span class="line">    model = lgb.train(params, train_set=d_train, num_boost_round=<span class="number">2200</span>, valid_sets=watchlist, \</span><br><span class="line">                      early_stopping_rounds=<span class="number">50</span>, verbose_eval=<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><ul><li><code>mlp</code> 模型</li></ul><p><code>MLP</code> 模型由两个全连接层和一个dropout层组成，本质上就是一个多隐藏层的网络</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mlp_model</span>(<span class="params">train_data,train_label,row_train</span>):</span><br><span class="line">    model = Sequential()</span><br><span class="line">    <span class="comment"># 全连接层</span></span><br><span class="line">    model.add(Dense(<span class="number">64</span>, input_shape=(row_train,), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    <span class="comment"># DropOut层</span></span><br><span class="line">    model.add(Dropout(<span class="number">0.4</span>))</span><br><span class="line">    <span class="comment"># 全连接层+分类器</span></span><br><span class="line">    model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mean_squared_logarithmic_error&#x27;</span>,</span><br><span class="line">                  optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">                  metrics=[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">                  )</span><br><span class="line"></span><br><span class="line">    model.fit(train_data, train_label,</span><br><span class="line">              batch_size=<span class="number">300</span>,</span><br><span class="line">              epochs=<span class="number">1</span>,</span><br><span class="line">              )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model.predict(X_test)</span><br></pre></td></tr></table></figure><h3 id="三、形成词频矩阵时进行优化"><a href="#三、形成词频矩阵时进行优化" class="headerlink" title="三、形成词频矩阵时进行优化"></a>三、形成词频矩阵时进行优化</h3><p>在样例代码中我们尝试了将所有 <code>CountVectorizer</code> 替换为 <code>TdidfVectorizer</code>，然后利用岭模型进行预测，但是结果并没有优化很多，仅仅到2.9而已。<br>在后面利用 <code>MLP</code>时完全舍弃了 <code>CountVectorizer</code> 只利用  <code>TdidfVectorizer</code>。</p><h3 id="四、优化数据预处理过程"><a href="#四、优化数据预处理过程" class="headerlink" title="四、优化数据预处理过程"></a>四、优化数据预处理过程</h3><p>我们对上面基本已经完善的 <code>MLP</code> 进行优化的方式是<strong>尝试不同特征的组合</strong>。</p><h4 id="数据属性分析（详见Price-Suggestion-Challenge1-ipynb）"><a href="#数据属性分析（详见Price-Suggestion-Challenge1-ipynb）" class="headerlink" title="数据属性分析（详见Price Suggestion Challenge1.ipynb）"></a>数据属性分析（详见<code>Price Suggestion Challenge1.ipynb</code>）</h4><p>首先对属性进行分析：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">item_condition_id    300000 non-null int64</span><br><span class="line">shipping             300000 non-null int64</span><br><span class="line"></span><br><span class="line">name                 300000 non-null object</span><br><span class="line">category_name        298719 non-null object</span><br><span class="line">brand_name           171929 non-null object</span><br><span class="line">item_description     300000 non-null object</span><br><span class="line"></span><br></pre></td></tr></table></figure><br><code>item_condition_id</code> 和 <code>shipping</code> 直接作为输入考虑，而 <code>name</code>, <code>category_name</code>, <code>brand_name</code>, <code>item_description</code> 考虑不同的组合进行尝试。</p><p>在此之前，我们找到了一个数据可视化的实例教程，对数据的属性进行分析。<br>通过详细观察数据得到最优的输入组合：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train.head()</span><br></pre></td></tr></table></figure><br><img src="image/1-3.png" alt=""></p><ol><li><p><code>price</code><br>通过数据可视化后的观察我们得知为什么要对 <code>price</code> 做 <code>log1p</code> 处理，这样使 <code>price</code> 分布更优。<br><img src="image/1-4.png" alt=""></p></li><li><p><code>category_name</code><br>尝试对该属性进行拆分，分成各种子类并查看相应数据。<br><img src="image/1-5.png" alt=""></p></li><li><p><code>item_description</code><br><img src="image/1-6.png" alt=""></p></li></ol><h4 id="不同的输入组合"><a href="#不同的输入组合" class="headerlink" title="不同的输入组合"></a>不同的输入组合</h4><ol><li>在样例代码中只是简单地将各个属性结合在一起进行文本分析，即<code>name</code> + <code>item_condition_id</code> + <code>category_name</code> + <code>brand_name</code> + <code>shipping</code> + <code>item_description</code>（6个输入）</li><li>尝试<code>name</code>, <code>item_condition_id</code>, <code>shipping</code>,<code>category_name</code> + <code>item_description</code>, <code>brand_name</code>（5个输入）</li><li>尝试<code>name</code>, <code>item_condition_id</code>, <code>shipping</code>, <code>category_name</code> + <code>brand_name</code> + <code>item_description</code>（4个输入）</li><li>尝试<code>name</code>, <code>item_condition_id</code>, <code>shipping</code>, <code>name</code> + <code>category_name</code> + <code>brand_name</code> + <code>item_description</code>  （4个输入）</li></ol><p>四种组合作为输入的结果非常相近，除了组合1<code>MSLE</code>在0.4左右，组合2和3 在0.21 左右，组合4最终能跑到0.17左右。组合4实际上加大了<code>name</code>的权重，让最终结果更好。</p><h2 id="最终源码及实验结果"><a href="#最终源码及实验结果" class="headerlink" title="最终源码及实验结果"></a>最终源码及实验结果</h2><ol><li><p>数据预处理</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line"><span class="comment"># 属性共有8个，删去price，train_id对结果没有影响。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_preprocess</span>(<span class="params">df</span>):</span><br><span class="line">    df[<span class="string">&#x27;name&#x27;</span>] = df[<span class="string">&#x27;name&#x27;</span>].fillna(<span class="string">&#x27;&#x27;</span>) + <span class="string">&#x27; &#x27;</span> + df[<span class="string">&#x27;brand_name&#x27;</span>].fillna(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    df[<span class="string">&#x27;text&#x27;</span>] = (df[<span class="string">&#x27;item_description&#x27;</span>].fillna(<span class="string">&#x27;&#x27;</span>) + <span class="string">&#x27; &#x27;</span> + df[<span class="string">&#x27;name&#x27;</span>] + <span class="string">&#x27; &#x27;</span> + df[<span class="string">&#x27;category_name&#x27;</span>].fillna(<span class="string">&#x27;&#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> df[[<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;shipping&#x27;</span>, <span class="string">&#x27;item_condition_id&#x27;</span>]]</span><br></pre></td></tr></table></figure></li><li><p>构建模型</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit_predict</span>(<span class="params">xs, y_train</span>):</span><br><span class="line">    X_train, X_test = xs</span><br><span class="line">    <span class="comment"># 配置tf.Session的运算方式，比如gpu运算或者cpu运算</span></span><br><span class="line">    config = tf.ConfigProto(</span><br><span class="line">        <span class="comment"># 设置多个操作并行运算的线程数</span></span><br><span class="line">        intra_op_parallelism_threads=<span class="number">1</span>, use_per_session_threads=<span class="number">1</span>, inter_op_parallelism_threads=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># Session提供了Operation执行和Tensor求值的环境。</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session(graph=tf.Graph(), config=config) <span class="keyword">as</span> sess, timer(<span class="string">&#x27;fit_predict&#x27;</span>):</span><br><span class="line">        ks.backend.set_session(sess)</span><br><span class="line">        model_in = ks.Input(shape=(X_train.shape[<span class="number">1</span>],), dtype=<span class="string">&#x27;float32&#x27;</span>, sparse=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># ks.layers.Dense 表示输出空间的维度</span></span><br><span class="line">        <span class="comment"># Dense全连接层，相当于直接添加一层</span></span><br><span class="line">        <span class="comment"># activation 是按逐个元素计算的激活函数</span></span><br><span class="line">        out = ks.layers.Dense(<span class="number">192</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(model_in)</span><br><span class="line">        out = ks.layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(out)</span><br><span class="line">        out = ks.layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(out)</span><br><span class="line">        out = ks.layers.Dense(<span class="number">1</span>)(out)</span><br><span class="line">        model = ks.Model(model_in, out)</span><br><span class="line">        model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mean_squared_error&#x27;</span>, optimizer=ks.optimizers.Adam(lr=<span class="number">3e-3</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            <span class="keyword">with</span> timer(<span class="string">f&#x27;epoch <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>&#x27;</span>):</span><br><span class="line">                model.fit(x=X_train, y=y_train, batch_size=<span class="number">2</span> ** (<span class="number">11</span> + i), epochs=<span class="number">1</span>, verbose=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> model.predict(X_test)[:, <span class="number">0</span>]</span><br></pre></td></tr></table></figure></li><li><p>训练模型并预测结果</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    vectorizer = make_union(<span class="comment"># 把所有的transformers组装成一个FeatureUnion. n_jobs表示可以同时进行</span></span><br><span class="line">        <span class="comment"># FunctionTransformer 实现自定义转换，validate=False 时没有输入验证</span></span><br><span class="line">        <span class="comment"># TfidfVectorizer函数，仅考虑按照词频排列前max_feature位的词，token_pattern=&#x27;\w+&#x27;至少匹配一位的词</span></span><br><span class="line">        make_pipeline(FunctionTransformer(itemgetter(<span class="string">&#x27;name&#x27;</span>), validate=<span class="literal">False</span>), TfidfVectorizer(max_features=<span class="number">100000</span>, token_pattern=<span class="string">&#x27;\w+&#x27;</span>)),</span><br><span class="line">        make_pipeline(FunctionTransformer(itemgetter(<span class="string">&#x27;text&#x27;</span>), validate=<span class="literal">False</span>), TfidfVectorizer(max_features=<span class="number">100000</span>, token_pattern=<span class="string">&#x27;\w+&#x27;</span>)),</span><br><span class="line">        make_pipeline(FunctionTransformer(itemgetter([<span class="string">&#x27;shipping&#x27;</span>, <span class="string">&#x27;item_condition_id&#x27;</span>]), validate=<span class="literal">False</span>),</span><br><span class="line">                      FunctionTransformer(to_records, validate=<span class="literal">False</span>), DictVectorizer()),</span><br><span class="line">        n_jobs=<span class="number">4</span>)</span><br><span class="line">    <span class="comment"># StandardScaler()进行数据标准化。保存训练集中的参数（均值、方差）直接使用其对象转换测试集数据。</span></span><br><span class="line">    y_scaler = StandardScaler()</span><br><span class="line">    <span class="comment"># with 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行必要的“清理”操作，释放资源，比如文件使用后自动关闭、线程中锁的自动获取和释放等。</span></span><br><span class="line">    <span class="keyword">with</span> timer(<span class="string">&#x27;process train&#x27;</span>):</span><br><span class="line">        train = pd.read_csv(<span class="string">&#x27;train.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        test = pd.read_csv(<span class="string">&#x27;test.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="comment"># 删去&#x27;price&#x27;属性</span></span><br><span class="line">        train = train[train[<span class="string">&#x27;price&#x27;</span>] &gt; <span class="number">0</span>].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 将price数据进行标准化</span></span><br><span class="line">        y_train = y_scaler.fit_transform(np.log1p(train[<span class="string">&#x27;price&#x27;</span>].values.reshape(-<span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line">        X_train = vectorizer.fit_transform(data_preprocess(train)).astype(np.float32)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;X_train: <span class="subst">&#123;X_train.shape&#125;</span> of <span class="subst">&#123;X_train.dtype&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> timer(<span class="string">&#x27;process valid&#x27;</span>):</span><br><span class="line">        X_test = vectorizer.transform(data_preprocess(test)).astype(np.float32)</span><br><span class="line">    <span class="keyword">with</span> ThreadPool(processes=<span class="number">4</span>) <span class="keyword">as</span> pool:</span><br><span class="line">        Xb_train, Xb_test = [x.astype(np.<span class="built_in">bool</span>).astype(np.float32) <span class="keyword">for</span> x <span class="keyword">in</span> [X_train, X_test]]</span><br><span class="line">        xs = [[Xb_train, Xb_test], [X_train, X_test]] * <span class="number">2</span></span><br><span class="line">        <span class="comment"># 预测模型</span></span><br><span class="line">        y_pred = np.mean(pool.<span class="built_in">map</span>(partial(fit_predict, y_train=y_train), xs), axis=<span class="number">0</span>)</span><br><span class="line">    y_pred = np.expm1(y_scaler.inverse_transform(y_pred.reshape(-<span class="number">1</span>, <span class="number">1</span>))[:, <span class="number">0</span>])</span><br><span class="line">    <span class="comment"># print(type(y_pred))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出预测结果到csv</span></span><br><span class="line">    test_id = np.array(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(y_pred)))</span><br><span class="line">    dataframe = pd.DataFrame(&#123;<span class="string">&#x27;test_id&#x27;</span>: test_id, <span class="string">&#x27;price&#x27;</span>: y_pred&#125;)</span><br><span class="line">    dataframe.to_csv(<span class="string">&quot;res.csv&quot;</span>, index=<span class="literal">False</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(&#x27;Valid MSLE: &#123;:.4f&#125;&#x27;.format(mean_squared_log_error(valid[&#x27;price&#x27;], y_pred)))</span></span><br></pre></td></tr></table></figure></li></ol><p>最终实验结果达到了0.179。</p><h2 id="在MLP模型下的其他优化方向"><a href="#在MLP模型下的其他优化方向" class="headerlink" title="在MLP模型下的其他优化方向"></a>在<code>MLP</code>模型下的其他优化方向</h2><ol><li>可以观察到在<code>item_desciption</code> 的词云中，有诸如<code>shipping</code> 和<code>free</code>等词，这些词可能代表着免运费等含义，与<code>shipping</code>属性有一定的重复，将它作为特征词训练模型会造成干扰。</li><li>单个关键词可能包含的信息不全面，关键词之间可能有很大的关联。</li><li>在最终的模型中<code>MLP</code>采用了四层感知机，感知机的层数和每层的输入规模还可以做进一步调参。</li></ol><h2 id="实验心得"><a href="#实验心得" class="headerlink" title="实验心得"></a>实验心得</h2><p>这次实验的难度非常大，不知道从何入手。</p><p>在仔细研究了课程中给的样例代码和数据可视化分析的内容之后，对数据集和预测的方法都有了初步的了解。  </p><p>因为对<code>MLP</code>，<code>Lightgbm</code>等模型非常不熟悉，所以从输入的角度入手，在不同属性的组合之处进行尝试，得到了最终的较为优秀的结果。  </p><p>在之后的学习中应该更加深入地学习和了解模型，尽量能够自己独立完成创建模型，而不是修改其他已经写好的模型。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1].<a href="https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html">https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html</a></p><p>[2].  <a href="https://github.com/pjankiewicz/mercari-solution">https://github.com/pjankiewicz/mercari-solution</a></p><p>[3].<a href="https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling">https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling</a></p><p>[4].<a href="https://wklchris.github.io/Py3-pandas.html#统计信息dfdescribe-svalue_counts--unique">https://wklchris.github.io/Py3-pandas.html#%E7%BB%9F%E8%AE%A1%E4%BF%A1%E6%81%AFdfdescribe-svalue_counts—unique</a></p><p>[5].<a href="https://zh.wikipedia.org/wiki/多层感知器">https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8</a></p><p>[6].<a href="https://blog.csdn.net/weixin_39807102/article/details/81912566">https://blog.csdn.net/weixin_39807102/article/details/81912566</a></p><p>[7].<a href="https://github.com/maiwen/NLP">https://github.com/maiwen/NLP</a></p><p>[8]. <a href="https://zh.wikipedia.org/wiki/正则表达式">https://zh.wikipedia.org/wiki/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F</a></p><p>[9].<a href="https://blog.csdn.net/u012609509/article/details/72911564">https://blog.csdn.net/u012609509/article/details/72911564</a></p><p>[10]. <a href="https://www.kaggle.com/tunguz/more-effective-ridge-lgbm-script-lb-0-44823">https://www.kaggle.com/tunguz/more-effective-ridge-lgbm-script-lb-0-44823</a></p><p>[11]. <a href="https://qiita.com/kazuhirokomoda/items/1e9b7ebcacf264b2d814">https://qiita.com/kazuhirokomoda/items/1e9b7ebcacf264b2d814</a></p><p>[12]. <a href="https://www.jianshu.com/p/c532424541ad">https://www.jianshu.com/p/c532424541ad</a></p><p>[13]. <a href="https://www.jiqizhixin.com/articles/2017-11-13-7">https://www.jiqizhixin.com/articles/2017-11-13-7</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;实验题目&quot;&gt;&lt;a href=&quot;#实验题目&quot; class=&quot;headerlink&quot; title=&quot;实验题目&quot;&gt;&lt;/a&gt;实验题目&lt;/h2&gt;&lt;h3 id=&quot;题目背景&quot;&gt;&lt;a href=&quot;#题目背景&quot; class=&quot;headerlink&quot; title=&quot;题目背景&quot;&gt;&lt;/a&gt;题目背景&lt;/h3&gt;&lt;p&gt;考虑到网上销售的产品数量，产品定价在规模上变得更加困难。服装有很强的季节性定价趋势，受品牌影响很大，而电子产品价格根据产品规格波动。如何根据以往信息进行合理定价，有效地帮助商家进行商品的销售是一个有意义的问题。&lt;/p&gt;</summary>
    
    
    
    <category term="数据科学导论" scheme="https://blog.crocodilezs.top/categories/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AF%BC%E8%AE%BA/"/>
    
    
    <category term="NLP" scheme="https://blog.crocodilezs.top/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Linux开发环境及应用作业1</title>
    <link href="https://blog.crocodilezs.top/201911/Linux%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%8F%8A%E5%BA%94%E7%94%A8%E4%BD%9C%E4%B8%9A%2020191031/"/>
    <id>https://blog.crocodilezs.top/201911/Linux%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%8F%8A%E5%BA%94%E7%94%A8%E4%BD%9C%E4%B8%9A%2020191031/</id>
    <published>2019-10-31T14:10:14.000Z</published>
    <updated>2022-05-23T17:15:29.807Z</updated>
    
    <content type="html"><![CDATA[<h2 id="作业要求"><a href="#作业要求" class="headerlink" title="作业要求"></a>作业要求</h2><p>从因特网上搜索相关Web网页，处理网页<code>html</code>数据，从中提取出当前时间点北京各监测站的 PM2.5浓度，输出格式如下。要求：写出各个处 理步骤，并给出解释。<br>2018-03-15 13:00:00,海淀区万柳,73<br>2018-03-15 13:00:00,昌平镇,67<br>2018-03-15 13:00:00,奥体中心,66<br>2018-03-15 14:00:00,海淀区万柳,73<br>2018-03-15 14:00:00,昌平镇,73<br>2018-03-15 14:00:00,奥体中心,75</p><span id="more"></span><h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h2><h3 id="数据搜集"><a href="#数据搜集" class="headerlink" title="数据搜集"></a>数据搜集</h3><p>北京各监测站的<code>PM2.5</code>指数的数据来源网站：<a href="http://www.86pm25.com/city/beijing.html">http://www.86pm25.com/city/beijing.html</a><br><img src="https://s2.ax1x.com/2019/10/31/K5rmkV.jpg" alt=""></p><h3 id="数据整理及汇总"><a href="#数据整理及汇总" class="headerlink" title="数据整理及汇总"></a>数据整理及汇总</h3><p>先展示实现该操作的指令和最后的结果：<br><img src="https://s2.ax1x.com/2019/10/31/K5rl6J.jpg" alt=""><br><img src="https://s2.ax1x.com/2019/10/31/K5rQl4.jpg" alt=""><br><img src="https://s2.ax1x.com/2019/10/31/K5rJTx.jpg" alt=""></p><p>下面详细解释指令：</p><ol><li>首先利用<tr>标签把数据分成单独的行，<code>sed -e &#39;s/&lt;tr/\n&lt;tr/g&#39;</code></li><li>其次删掉html文件中的所有标签<code>-e &#39;s/&lt;[^&lt;&gt;]*&gt;/ /g</code>，把所有标签都换成了空格。</li><li>我先在html文件中寻找日期和时间，发现时间的那一行有“更新”的字样，于是建立awk文件，此时发现“更新”后面中文的冒号紧跟着日期，没发把日期分离开，于是先在中文冒号后面添加空格。顺便把日期和时间的格式改成标准的输出的格式。<code>-e &#39;s/：/： /g&#39; -e &#39;s/[年月]/-/g&#39; -e &#39;s/日//g -e &#39;s/时/:00:00/g&#39;</code><br><img src="https://s2.ax1x.com/2019/10/31/K5rrnA.jpg" alt=""></li><li>此时可以把时间和日期抽离出来了。在建立的awk文件中输入<code>/更新/ &#123;data = $2; time = $3&#125;</code></li><li>得到日期和时间 之后，我们去找监测站和pm2.5指数，发现在这些数据最后都有$m^3$单位在，于是在awk文件中添加<code>/m3/&#123;printf(&quot;%s %s,%s,%s\n&quot;,date, time, $1, $3);&#125;</code><br><img src="https://s2.ax1x.com/2019/10/31/K5rs0I.jpg" alt=""></li><li>最后把单位删掉，并输出到csv文件中即可。<code>awk -f flow.awk | sed -e &#39;s/[ug/m3]//g&#39; &gt; flow.csv</code></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;作业要求&quot;&gt;&lt;a href=&quot;#作业要求&quot; class=&quot;headerlink&quot; title=&quot;作业要求&quot;&gt;&lt;/a&gt;作业要求&lt;/h2&gt;&lt;p&gt;从因特网上搜索相关Web网页，处理网页&lt;code&gt;html&lt;/code&gt;数据，从中提取出当前时间点北京各监测站的 PM2.5浓度，输出格式如下。要求：写出各个处 理步骤，并给出解释。&lt;br&gt;2018-03-15 13:00:00,海淀区万柳,73&lt;br&gt;2018-03-15 13:00:00,昌平镇,67&lt;br&gt;2018-03-15 13:00:00,奥体中心,66&lt;br&gt;2018-03-15 14:00:00,海淀区万柳,73&lt;br&gt;2018-03-15 14:00:00,昌平镇,73&lt;br&gt;2018-03-15 14:00:00,奥体中心,75&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://blog.crocodilezs.top/categories/Linux/"/>
    
    
    <category term="文本处理" scheme="https://blog.crocodilezs.top/tags/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>神经网络前向传播和反向传播算法推导</title>
    <link href="https://blog.crocodilezs.top/201911/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/"/>
    <id>https://blog.crocodilezs.top/201911/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/</id>
    <published>2019-10-28T17:32:10.000Z</published>
    <updated>2022-05-23T16:54:41.206Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、目标"><a href="#一、目标" class="headerlink" title="一、目标"></a>一、目标</h2><ol><li>推导具有单隐层的神经网络的前向传播和反向传播算法，并进行编程（可以使用<code>sklearn</code>中的神经网络）。<ul><li>探讨10，30，100，300，1000，不同隐藏节点数对网络性能的影响。</li><li>探讨不同学习率和迭代次数对网络性能的影响。</li><li>改变数据的标准化方法，探讨对训练的影响。</li></ul></li><li>查阅资料说明什么是<code>Hebb</code>学习规则</li></ol><span id="more"></span><h2 id="二、推导单隐层神经网络的前向传播和反向传播算法"><a href="#二、推导单隐层神经网络的前向传播和反向传播算法" class="headerlink" title="二、推导单隐层神经网络的前向传播和反向传播算法"></a>二、推导单隐层神经网络的前向传播和反向传播算法</h2><p>参考资料：<a href="https://blog.csdn.net/Lucky_Go/article/details/89738286">https://blog.csdn.net/Lucky_Go/article/details/89738286</a><br><img src="image4\reduction1.jpg" alt=""><br><img src="image4\reduction2.jpg" alt=""><br><img src="image4\reduction3.jpg" alt=""><br><img src="image4\reduction4.jpg" alt=""></p><h2 id="三、算法实现"><a href="#三、算法实现" class="headerlink" title="三、算法实现"></a>三、算法实现</h2><p>参考资料：<a href="https://blog.csdn.net/zsx17/article/details/89342506">https://blog.csdn.net/zsx17/article/details/89342506</a></p><p>因为网上神经网络的代码基本都是用<code>tensorflow</code>实现的，这里是直接调库。在完成了作业的基本要求之后我也尝试了自己实现单隐层神经网络的代码（在实验报告的后部分）。</p><h3 id="1-载入数据"><a href="#1-载入数据" class="headerlink" title="1. 载入数据"></a>1. 载入数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1、载入数据</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.examples.tutorials.mnist.input_data <span class="keyword">as</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取mnist数据</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&#x27;MNIST_data/&#x27;</span>, one_hot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="2-建立模型"><a href="#2-建立模型" class="headerlink" title="2. 建立模型"></a>2. 建立模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.建立模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.1 构建输入层</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>], name=<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>], name=<span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.2 构建隐藏层</span></span><br><span class="line"><span class="comment"># 隐藏层神经元数量(随意设置）</span></span><br><span class="line">H1_NN = <span class="number">256</span></span><br><span class="line"><span class="comment"># 权重</span></span><br><span class="line">W1 = tf.Variable(tf.random_normal([<span class="number">784</span>, H1_NN]))</span><br><span class="line"><span class="comment"># 偏置项</span></span><br><span class="line">b1 = tf.Variable(tf.zeros([H1_NN]))</span><br><span class="line"></span><br><span class="line">Y1 = tf.nn.relu(tf.matmul(x, W1) + b1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.3 构建输出层</span></span><br><span class="line">W2 = tf.Variable(tf.random_normal([H1_NN, <span class="number">10</span>]))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">forward = tf.matmul(Y1, W2) + b2</span><br><span class="line">pred = tf.nn.softmax(forward)</span><br></pre></td></tr></table></figure><h3 id="3-训练模型"><a href="#3-训练模型" class="headerlink" title="3. 训练模型"></a>3. 训练模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.1 定义损失函数</span></span><br><span class="line"><span class="comment"># tensorflow提供了下面的函数，用于避免log(0)值为Nan造成数据不稳定</span></span><br><span class="line">loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=forward, labels=y))</span><br><span class="line"><span class="comment"># # 交叉熵损失函数</span></span><br><span class="line"><span class="comment"># loss_function = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.2 设置训练参数</span></span><br><span class="line">train_epochs = <span class="number">40</span>  <span class="comment"># 训练轮数</span></span><br><span class="line">batch_size = <span class="number">50</span>  <span class="comment"># 单次训练样本数(批次大小)</span></span><br><span class="line"><span class="comment"># 一轮训练的批次数</span></span><br><span class="line">total_batch = <span class="built_in">int</span>(mnist.train.num_examples / batch_size)</span><br><span class="line">display_step = <span class="number">1</span>  <span class="comment"># 显示粒数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span>  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.2 选择优化器</span></span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss_function)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.3定义准确率</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(pred, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.4 模型的训练</span></span><br><span class="line"><span class="comment"># 记录训练开始的时间</span></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">startTime = time()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(train_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(total_batch):</span><br><span class="line">        <span class="comment"># 读取批次训练数据</span></span><br><span class="line">        xs, ys = mnist.train.next_batch(batch_size)</span><br><span class="line">        <span class="comment"># 执行批次训练</span></span><br><span class="line">        sess.run(optimizer, feed_dict=&#123;x: xs, y: ys&#125;)</span><br><span class="line">    <span class="comment"># 在total_batch批次数据训练完成后，使用验证数据计算误差和准确率，验证集不分批</span></span><br><span class="line">    loss, acc = sess.run([loss_function, accuracy], feed_dict=&#123;x: mnist.validation.images, y: mnist.validation.labels&#125;)</span><br><span class="line">    <span class="comment"># 打印训练过程中的详细信息</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;训练轮次：&#x27;</span>, <span class="string">&#x27;%02d&#x27;</span> % (epoch + <span class="number">1</span>),</span><br><span class="line">              <span class="string">&#x27;损失：&#x27;</span>, <span class="string">&#x27;&#123;:.9f&#125;&#x27;</span>.<span class="built_in">format</span>(loss),</span><br><span class="line">              <span class="string">&#x27;准确率：&#x27;</span>, <span class="string">&#x27;&#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(acc))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练结束&#x27;</span>)</span><br><span class="line"><span class="comment"># 显示总运行时间</span></span><br><span class="line">duration = time() - startTime</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;总运行时间为：&quot;</span>, <span class="string">&quot;&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(duration))</span><br></pre></td></tr></table></figure><h3 id="4-模型评估"><a href="#4-模型评估" class="headerlink" title="4. 模型评估"></a>4. 模型评估</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4.评估模型</span></span><br><span class="line">accu_test = sess.run(accuracy,</span><br><span class="line">                     feed_dict=&#123;x: mnist.test.images, y: mnist.test.labels&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集准确率：&#x27;</span>, accu_test)</span><br></pre></td></tr></table></figure><h3 id="5-应用模型"><a href="#5-应用模型" class="headerlink" title="5. 应用模型"></a>5. 应用模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 5.应用模型</span></span><br><span class="line">prediction_result = sess.run(tf.argmax(pred, <span class="number">1</span>), feed_dict=&#123;x: mnist.test.images&#125;)</span><br><span class="line"><span class="comment"># 查看预测结果的前10项</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;前10项的结果：&quot;</span>, prediction_result[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.1找出预测错误的样本</span></span><br><span class="line">compare_lists = prediction_result == np.argmax(mnist.test.labels, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(compare_lists)</span><br><span class="line">err_lists = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(compare_lists)) <span class="keyword">if</span> compare_lists[i] == <span class="literal">False</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测错误的图片：&#x27;</span>, err_lists)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测错误图片的总数：&#x27;</span>, <span class="built_in">len</span>(err_lists))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个输出错误分类的函数</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_predict_errs</span>(<span class="params">labels,  <span class="comment"># 标签列表</span></span></span><br><span class="line"><span class="params">                       prediction</span>):  <span class="comment"># 预测值列表</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    compare_lists = (prediction == np.argmax(labels, <span class="number">1</span>))</span><br><span class="line">    err_lists = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(compare_lists)) <span class="keyword">if</span> compare_lists[i] == <span class="literal">False</span>]</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> err_lists:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;index=&#x27;</span> + <span class="built_in">str</span>(x) + <span class="string">&#x27;标签值=&#x27;</span>, np.argmax(labels[x]), <span class="string">&#x27;预测值=&#x27;</span>, prediction[x])</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;总计：&quot;</span> + <span class="built_in">str</span>(count))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print_predict_errs(labels=mnist.test.labels, prediction=prediction_result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_images_labels_prediction</span>(<span class="params">images,  <span class="comment"># 图像列表</span></span></span><br><span class="line"><span class="params">                                  labels,  <span class="comment"># 标签列表</span></span></span><br><span class="line"><span class="params">                                  predication,  <span class="comment"># 预测值列表</span></span></span><br><span class="line"><span class="params">                                  index,  <span class="comment"># 从第index个开始显示</span></span></span><br><span class="line"><span class="params">                                  num=<span class="number">10</span></span>):  <span class="comment"># 缺省一次显示10幅</span></span><br><span class="line">    fig = plt.gcf()  <span class="comment"># 获取当前图表，get current figure</span></span><br><span class="line">    fig.set_size_inches(<span class="number">10</span>, <span class="number">12</span>)  <span class="comment"># 设为英寸，1英寸=2.53厘米</span></span><br><span class="line">    <span class="keyword">if</span> num &gt; <span class="number">25</span>:</span><br><span class="line">        num = <span class="number">25</span>  <span class="comment"># 最多显示25个子图</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num):</span><br><span class="line">        ax = plt.subplot(<span class="number">5</span>, <span class="number">5</span>, i + <span class="number">1</span>)  <span class="comment"># 获取当前要处理的子图</span></span><br><span class="line">        <span class="comment"># 显示第index图像</span></span><br><span class="line">        ax.imshow(np.reshape(images[index], (<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建该图上显示的title</span></span><br><span class="line">        title = <span class="string">&#x27;label=&#x27;</span> + <span class="built_in">str</span>(np.argmax(labels[index]))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(predication) &gt; <span class="number">0</span>:</span><br><span class="line">            title += <span class="string">&quot;,predict=&quot;</span> + <span class="built_in">str</span>(predication[index])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 显示图上的title信息</span></span><br><span class="line">        ax.set_title(title, fontsize=<span class="number">10</span>)</span><br><span class="line">        ax.set_xticks([])  <span class="comment"># 不显示坐标轴</span></span><br><span class="line">        ax.set_yticks([])</span><br><span class="line">        index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plot_images_labels_prediction(mnist.test.images,</span><br><span class="line">                              mnist.test.labels,</span><br><span class="line">                              prediction_result, <span class="number">10</span>, <span class="number">25</span>)</span><br><span class="line">plot_images_labels_prediction(mnist.test.images,</span><br><span class="line">                              mnist.test.labels,</span><br><span class="line">                              prediction_result, <span class="number">610</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure><h3 id="6-结果展示"><a href="#6-结果展示" class="headerlink" title="6. 结果展示"></a>6. 结果展示</h3><p>上面的代码中隐层节点个数为256个，学习率为0.01，迭代次数为40次。训练结果如下：</p><p><img src="image4\256-0.01-40.jpg" alt=""></p><p>部分分类图像如下所示：</p><p><img src="image4\256-0.01-40-1.jpg" alt=""></p><p><img src="image4\256-0.01-40-2.jpg" alt=""></p><h2 id="四、算法调优"><a href="#四、算法调优" class="headerlink" title="四、算法调优"></a>四、算法调优</h2><p>在上面的模型中隐层结点数为256，学习率为0.01，迭代次数为40次。</p><p>下面分别从隐层节点数、学习率和迭代次数三个角度进行调优。</p><h3 id="1-隐层节点数"><a href="#1-隐层节点数" class="headerlink" title="1. 隐层节点数"></a>1. 隐层节点数</h3><p>将隐层节点数设为10，得到的结果如下图所示：</p><p><img src="image4\10-0.01-40.jpg" alt=""></p><p>将隐层节点设为30，100，300，1000的效果不再具体展示，效果如下所示：</p><div class="table-container"><table><thead><tr><th style="text-align:center">隐层节点个数</th><th style="text-align:center">总运行时间/s</th><th style="text-align:center">预测错误的图片数</th><th style="text-align:center">准确率</th></tr></thead><tbody><tr><td style="text-align:center">10</td><td style="text-align:center">46.29</td><td style="text-align:center">736</td><td style="text-align:center">0.9264</td></tr><tr><td style="text-align:center">30</td><td style="text-align:center">43.46</td><td style="text-align:center">528</td><td style="text-align:center">0.9472</td></tr><tr><td style="text-align:center">100</td><td style="text-align:center">59.06</td><td style="text-align:center">343</td><td style="text-align:center">0.9657</td></tr><tr><td style="text-align:center">256</td><td style="text-align:center">84.48</td><td style="text-align:center">249</td><td style="text-align:center">0.9751</td></tr><tr><td style="text-align:center">300</td><td style="text-align:center">76.64</td><td style="text-align:center">269</td><td style="text-align:center">0.9731</td></tr><tr><td style="text-align:center">1000</td><td style="text-align:center">302.27</td><td style="text-align:center">240</td><td style="text-align:center">0.976</td></tr></tbody></table></div><p>由表可知，准确率随着隐层节点个数的增加而增加，增加速率逐步减少。</p><h3 id="2-学习率"><a href="#2-学习率" class="headerlink" title="2. 学习率"></a>2. 学习率</h3><p>学习率分别为0.005，0.01， 0.02， 0.1，隐层节点数选择256，迭代次数选择40。分类结果如下：</p><div class="table-container"><table><thead><tr><th style="text-align:center">学习率</th><th style="text-align:center">总运行时间/s</th><th style="text-align:center">预测错误的图片数</th><th style="text-align:center">准确率</th></tr></thead><tbody><tr><td style="text-align:center">0.005</td><td style="text-align:center">78.81</td><td style="text-align:center">231</td><td style="text-align:center">0.9769</td></tr><tr><td style="text-align:center">0.01</td><td style="text-align:center">84.48</td><td style="text-align:center">249</td><td style="text-align:center">0.9751</td></tr><tr><td style="text-align:center">0.02</td><td style="text-align:center">69.72</td><td style="text-align:center">446</td><td style="text-align:center">0.9554</td></tr><tr><td style="text-align:center">0.1</td><td style="text-align:center">73.87</td><td style="text-align:center">2561</td><td style="text-align:center">0.7439</td></tr></tbody></table></div><p>由表可知，准确率随着学习率的增加而降低。在学习率低于0.01时，图片分类的准确率提升的速率较小。</p><h3 id="3-迭代次数"><a href="#3-迭代次数" class="headerlink" title="3. 迭代次数"></a>3. 迭代次数</h3><p>迭代次数分别为20，40，100，隐层节点数选择256，学习率选择0.01。分类结果如下：</p><div class="table-container"><table><thead><tr><th style="text-align:center">迭代次数</th><th style="text-align:center">总运行时间/s</th><th style="text-align:center">预测错误的图片数</th><th style="text-align:center">准确率</th></tr></thead><tbody><tr><td style="text-align:center">20</td><td style="text-align:center">37.12</td><td style="text-align:center">307</td><td style="text-align:center">0.9693</td></tr><tr><td style="text-align:center">40</td><td style="text-align:center">84.48</td><td style="text-align:center">249</td><td style="text-align:center">0.9751</td></tr><tr><td style="text-align:center">100</td><td style="text-align:center">184.39</td><td style="text-align:center">239</td><td style="text-align:center">0.9761</td></tr></tbody></table></div><p>由表可知，迭代次数对总运行时间的影响率很大，准确率随着迭代次数的增加而增加，但对准确率起决定因素的还是隐层的节点个数以及学习率。</p><h3 id="4-改变数据标准化方法"><a href="#4-改变数据标准化方法" class="headerlink" title="4. 改变数据标准化方法"></a>4. 改变数据标准化方法</h3><h4 id="最大-最小规范化"><a href="#最大-最小规范化" class="headerlink" title="最大-最小规范化"></a>最大-最小规范化</h4><h4 id="Z-score规范化"><a href="#Z-score规范化" class="headerlink" title="Z-score规范化"></a><code>Z-score</code>规范化</h4><h2 id="五、Hebb学习规则"><a href="#五、Hebb学习规则" class="headerlink" title="五、Hebb学习规则"></a>五、<code>Hebb</code>学习规则</h2><p>参考资料：<a href="https://baike.baidu.com/item/Hebb学习规则/3061563?fr=aladdin">https://baike.baidu.com/item/Hebb%E5%AD%A6%E4%B9%A0%E8%A7%84%E5%88%99/3061563?fr=aladdin</a></p><p><code>Hebb</code>学习规则是一个无监督学习规则，这种学习的结果是使网络能够提取训练集的统计特性，从而把输入信息按照它们的相似性程度划分为若干类。这一点与人类观察和认识世界的过程非常吻合，人类观察和认识世界在相当程度上就是在根据事物的统计特征进行分类。<code>Hebb</code>学习规则只根据神经元连接间的激活水平改变权值，因此这种方法又称为相关学习或并联学习。</p><p>无监督学习规则<br> 唐纳德·赫布（1904-1985）是加拿大著名生理心理学家。<code>Hebb</code>学习规则与“条件反射”机理一致，并且已经得到了神经细胞学说的证实。<br> 巴甫洛夫的条件反射实验：每次给狗喂食前都先响铃，时间一长，狗就会将铃声和食物联系起来。以后如果响铃但是不给食物，狗也会流口水。<br> 受该实验的启发，Hebb的理论认为在同一时间被激发的神经元间的联系会被强化。比如，铃声响时一个神经元被激发，在同一时间食物的出现会激发附近的另一个神经元，那么这两个神经元间的联系就会强化，从而记住这两个事物之间存在着联系。相反，如果两个神经元总是不能同步激发，那么它们间的联系将会越来越弱。<br> <code>Hebb</code>学习律可表示为：<br>$W<em>{ij}(t+1)=W</em>{ij}(t)+a⋅y<em>i⋅y_j$<br>$W</em>{ij}(t+1)=W_{ij}(t)+a⋅y_i⋅y_j$</p><p> 其中$W<em>{ij}$表示神经元$j$到神经元$i$的连接权，$y_i$与$y_j$表示两个神经元的输出，$a$是表示学习速率的常数，如果$y_i$与$y_j$同时被激活，即$y_i$与$y_j$同时为正，那么$W</em>{ij}$将增大。如果$y<em>i$被激活，而$y_j$处于抑制状态，即$y_i$为正$y_j$为负，那么$W</em>{ij}$将变小。</p><p><img src="https://images2015.cnblogs.com/blog/520787/201510/520787-20151021081107630-1544768706.png" alt=""></p><h2 id="六、-自己实现单隐层神经网络"><a href="#六、-自己实现单隐层神经网络" class="headerlink" title="六、 自己实现单隐层神经网络"></a>六、 自己实现单隐层神经网络</h2><p>参考资料：<a href="https://blog.csdn.net/hellozhxy/article/details/81055391">https://blog.csdn.net/hellozhxy/article/details/81055391</a></p><p>网络结构的函数定义：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">layer_sizes</span>(<span class="params">X, Y</span>):</span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment"># size of input layer</span></span><br><span class="line">    n_h = <span class="number">4</span> <span class="comment"># size of hidden layer</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment"># size of output layer</span></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><p>参数初始化函数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_parameters</span>(<span class="params">n_x, n_h, n_y</span>):</span><br><span class="line">    W1 = np.random.randn(n_h, n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>)) </span><br><span class="line">   </span><br><span class="line">    <span class="keyword">assert</span> (W1.shape == (n_h, n_x))    </span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (n_h, <span class="number">1</span>))    </span><br><span class="line">    <span class="keyword">assert</span> (W2.shape == (n_y, n_h))    </span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1, </span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,                 </span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,                  </span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;   </span><br><span class="line">                   </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>前向传播计算函数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_propagation</span>(<span class="params">X, parameters</span>):</span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary &quot;parameters&quot;</span></span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&#x27;b2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Implement Forward Propagation to calculate A2 (probabilities)</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, Z1) + b2</span><br><span class="line">    A2 = sigmoid(Z2)    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = &#123;<span class="string">&quot;Z1&quot;</span>: Z1,                   </span><br><span class="line">             <span class="string">&quot;A1&quot;</span>: A1,                   </span><br><span class="line">             <span class="string">&quot;Z2&quot;</span>: Z2,                  </span><br><span class="line">             <span class="string">&quot;A2&quot;</span>: A2&#125;    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure><p>计算损失函数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">A2, Y, parameters</span>):</span><br><span class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># number of example</span></span><br><span class="line">    <span class="comment"># Compute the cross-entropy cost</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(<span class="number">1</span>-A2), <span class="number">1</span>-Y)</span><br><span class="line">    cost = -<span class="number">1</span>/m * np.<span class="built_in">sum</span>(logprobs)</span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># makes sure cost is the dimension we expect.</span></span><br><span class="line">    <span class="keyword">assert</span>(<span class="built_in">isinstance</span>(cost, <span class="built_in">float</span>))    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p>反向传播函数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward_propagation</span>(<span class="params">parameters, cache, X, Y</span>):</span><br><span class="line">    m = X.shape[<span class="number">1</span>]    </span><br><span class="line">    <span class="comment"># First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;.</span></span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Retrieve also A1 and A2 from dictionary &quot;cache&quot;.</span></span><br><span class="line">    A1 = cache[<span class="string">&#x27;A1&#x27;</span>]</span><br><span class="line">    A2 = cache[<span class="string">&#x27;A2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2. </span></span><br><span class="line">    dZ2 = A2-Y</span><br><span class="line">    dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span>/m * np.<span class="built_in">sum</span>(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dZ1 = np.dot(W2.T, dZ2)*(<span class="number">1</span>-np.power(A1, <span class="number">2</span>))</span><br><span class="line">    dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span>/m * np.<span class="built_in">sum</span>(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    grads = &#123;<span class="string">&quot;dW1&quot;</span>: dW1,</span><br><span class="line">             <span class="string">&quot;db1&quot;</span>: db1,                      </span><br><span class="line">             <span class="string">&quot;dW2&quot;</span>: dW2,             </span><br><span class="line">             <span class="string">&quot;db2&quot;</span>: db2&#125;   </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><p>权值更新函数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters</span>(<span class="params">parameters, grads, learning_rate = <span class="number">1.2</span></span>):</span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary &quot;parameters&quot;</span></span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&#x27;b2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Retrieve each gradient from the dictionary &quot;grads&quot;</span></span><br><span class="line">    dW1 = grads[<span class="string">&#x27;dW1&#x27;</span>]</span><br><span class="line">    db1 = grads[<span class="string">&#x27;db1&#x27;</span>]</span><br><span class="line">    dW2 = grads[<span class="string">&#x27;dW2&#x27;</span>]</span><br><span class="line">    db2 = grads[<span class="string">&#x27;db2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    W1 -= dW1 * learning_rate</span><br><span class="line">    b1 -= db1 * learning_rate</span><br><span class="line">    W2 -= dW2 * learning_rate</span><br><span class="line">    b2 -= db2 * learning_rate</span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1, </span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,            </span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,   </span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>最终的神经网络模型：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">nn_model</span>(<span class="params">X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">False</span></span>):</span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]    </span><br><span class="line">    <span class="comment"># Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: &quot;n_x, n_h, n_y&quot;. Outputs = &quot;W1, b1, W2, b2, parameters&quot;.</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&#x27;b2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_iterations):        </span><br><span class="line">    <span class="comment"># Forward propagation. Inputs: &quot;X, parameters&quot;. Outputs: &quot;A2, cache&quot;.</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)        </span><br><span class="line">        <span class="comment"># Cost function. Inputs: &quot;A2, Y, parameters&quot;. Outputs: &quot;cost&quot;.</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)        </span><br><span class="line">        <span class="comment"># Backpropagation. Inputs: &quot;parameters, cache, X, Y&quot;. Outputs: &quot;grads&quot;.</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)        </span><br><span class="line">        <span class="comment"># Gradient descent parameter update. Inputs: &quot;parameters, grads&quot;. Outputs: &quot;parameters&quot;.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate=<span class="number">1.2</span>)        </span><br><span class="line">        <span class="comment"># Print the cost every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:            </span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&quot;Cost after iteration %i: %f&quot;</span> %(i, cost))    </span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;一、目标&quot;&gt;&lt;a href=&quot;#一、目标&quot; class=&quot;headerlink&quot; title=&quot;一、目标&quot;&gt;&lt;/a&gt;一、目标&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;推导具有单隐层的神经网络的前向传播和反向传播算法，并进行编程（可以使用&lt;code&gt;sklearn&lt;/code&gt;中的神经网络）。&lt;ul&gt;
&lt;li&gt;探讨10，30，100，300，1000，不同隐藏节点数对网络性能的影响。&lt;/li&gt;
&lt;li&gt;探讨不同学习率和迭代次数对网络性能的影响。&lt;/li&gt;
&lt;li&gt;改变数据的标准化方法，探讨对训练的影响。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;查阅资料说明什么是&lt;code&gt;Hebb&lt;/code&gt;学习规则&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://blog.crocodilezs.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="神经网络" scheme="https://blog.crocodilezs.top/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Finds算法和ID3算法</title>
    <link href="https://blog.crocodilezs.top/201910/FINDS%E7%AE%97%E6%B3%95%E5%92%8CID3%E7%AE%97%E6%B3%95/"/>
    <id>https://blog.crocodilezs.top/201910/FINDS%E7%AE%97%E6%B3%95%E5%92%8CID3%E7%AE%97%E6%B3%95/</id>
    <published>2019-10-28T04:21:10.000Z</published>
    <updated>2022-05-23T17:10:07.174Z</updated>
    
    <content type="html"><![CDATA[<h2 id="作业要求"><a href="#作业要求" class="headerlink" title="作业要求"></a>作业要求</h2><ol><li>实现<code>FINDS</code>算法</li><li>实现<code>ID3</code>算法</li></ol><ul><li>不要调库自己写。如果有能力可以继续用课件里的数据集测试两个算法（用天气的4条记录测试<code>FINDS</code>，用贷款的15条记录测试<code>ID3</code>）给出训练误差测试误差等；  </li><li>再有能力可以使用更大的数据集测试算法。</li></ul><span id="more"></span><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><h3 id="FINDS算法"><a href="#FINDS算法" class="headerlink" title="FINDS算法"></a><code>FINDS</code>算法</h3><ol><li>目标：寻找极大特殊假设。</li><li>从假设集合H中最特殊的假设开始。在该假设不能正确地划分一个正例的时候将其进行一般化。算法如下：<br><img src="https://s2.ax1x.com/2019/10/22/K3ymOH.jpg" alt="算法流程"></li><li><code>FINDS</code>算法是一种利用<code>more-general-than</code>的偏序结构来搜索假设空间的方法，这一搜索沿着偏序链，从较特殊的假设逐渐演变为较一般的假设。</li><li>算法<code>Python</code>实现：</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string"> Created on 2019/10/21 21:02</span></span><br><span class="line"><span class="string"> FINDS</span></span><br><span class="line"><span class="string"> @Author  : Zhouy</span></span><br><span class="line"><span class="string"> @Blog    : www.crocodilezs.top</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create dataset</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">CreateDataset</span>():</span><br><span class="line">    dataset = [[<span class="string">&#x27;Sunny&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;Normal&#x27;</span>, <span class="string">&#x27;Strong&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;Same&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;Sunny&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;High&#x27;</span>, <span class="string">&#x27;Strong&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;Same&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;Rainy&#x27;</span>, <span class="string">&#x27;Cold&#x27;</span>, <span class="string">&#x27;High&#x27;</span>, <span class="string">&#x27;Strong&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;Change&#x27;</span>, <span class="string">&#x27;No&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;Sunny&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;High&#x27;</span>, <span class="string">&#x27;Strong&#x27;</span>, <span class="string">&#x27;Cold&#x27;</span>, <span class="string">&#x27;Change&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>]]</span><br><span class="line">    labels = [<span class="string">&#x27;Sky&#x27;</span>, <span class="string">&#x27;Temp&#x27;</span>, <span class="string">&#x27;Humidity&#x27;</span>, <span class="string">&#x27;Wind&#x27;</span>, <span class="string">&#x27;Water&#x27;</span>, <span class="string">&#x27;Forest&#x27;</span>, <span class="string">&#x27;OutdoorSport&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> dataset, labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find one version space by using FINDS</span></span><br><span class="line"><span class="comment"># &#x27;/&#x27; means null, and &#x27;*&#x27; means generalization</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">FINDS</span>(<span class="params">dataset</span>):</span><br><span class="line">    constraint = [<span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> dataset:</span><br><span class="line">        <span class="keyword">if</span> item[-<span class="number">1</span>] == <span class="string">&#x27;Yes&#x27;</span>:</span><br><span class="line">            <span class="comment"># only go through positive instances</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(item)-<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span>(item[i] != constraint[i] <span class="keyword">and</span> constraint[i] != <span class="string">&#x27;*&#x27;</span>):</span><br><span class="line">                    <span class="keyword">if</span>(constraint[i] == <span class="string">&#x27;/&#x27;</span>):</span><br><span class="line">                        constraint[i] = item[i]</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        constraint[i] = <span class="string">&#x27;*&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> constraint</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    dataset, labels = CreateDataset()</span><br><span class="line">    constraint = FINDS(dataset)</span><br><span class="line">    <span class="built_in">print</span>(constraint)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h3 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a><code>ID3</code>算法</h3><ol><li>决策树：决策树是一种常用的分类与回归方法。决策树的模型为树形结构，在针对分类问题时，实际上就是针对输入数据的各个特征对实例进行分类的过程，即通过树形结构的模型，在每一层级上对特征值进行判断，进而到达决策树叶子节点，即完成分类过程。<br><strong>决策树的本质是概念学习。</strong></li><li><p>信息熵（香浓熵）、条件熵和信息增益的概念</p><ul><li>信息量：一件事发生的概率越小，我们说它所蕴含的信息量越大。<br><img src="https://s2.ax1x.com/2019/10/22/K36VNq.jpg" alt="信息量"></li><li>信息熵：信息熵就是所有可能发生的事件的信息量的期望<br><img src="https://s2.ax1x.com/2019/10/22/K36EEn.jpg" alt="信息熵"></li><li>条件熵：表示在X给定条件下，Y的条件概率分布的熵对X的数学期望。<br>![条件熵(<a href="https://s2.ax1x.com/2019/10/22/K36FBj.jpg">https://s2.ax1x.com/2019/10/22/K36FBj.jpg</a>)</li><li>信息增益：当我们用另一个变量X对原变量Y分类后，原变量Y的不确定性就会减小了(即熵值减小)。而熵就是不确定性，不确定程度减少了多少其实就是信息增益。这就是信息增益的由来，所以信息增益定义如下：<br><img src="https://s2.ax1x.com/2019/10/22/K36kHs.jpg" alt="信息增益"></li></ul></li><li><p>算法’python’实现:<br>(用课件上的贷款数据集一直没法成功分类，于是参考了csdn博客的另一个数据集合代码)</p></li></ol><p><code>myTrees.py</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string"> Created on 2019/10/22 11:59</span></span><br><span class="line"><span class="string"> myTrees</span></span><br><span class="line"><span class="string"> @Author  : Zhouy</span></span><br><span class="line"><span class="string"> @Blog    : www.crocodilezs.top</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 原始数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createDataSet</span>():</span><br><span class="line">    dataSet = [[<span class="number">1</span>, <span class="number">1</span>, <span class="string">&#x27;yes&#x27;</span>],</span><br><span class="line">               [<span class="number">1</span>, <span class="number">1</span>, <span class="string">&#x27;yes&#x27;</span>],</span><br><span class="line">               [<span class="number">1</span>, <span class="number">0</span>, <span class="string">&#x27;no&#x27;</span>],</span><br><span class="line">               [<span class="number">0</span>, <span class="number">1</span>, <span class="string">&#x27;no&#x27;</span>],</span><br><span class="line">               [<span class="number">0</span>, <span class="number">1</span>, <span class="string">&#x27;no&#x27;</span>]]</span><br><span class="line">    labels = [<span class="string">&#x27;no surfacing&#x27;</span>,<span class="string">&#x27;flippers&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多数表决器</span></span><br><span class="line"><span class="comment"># 列中相同值数量最多为结果</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">majorityCnt</span>(<span class="params">classList</span>):</span><br><span class="line">    classCounts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> (value <span class="keyword">not</span> <span class="keyword">in</span> classCounts.keys()):</span><br><span class="line">            classCounts[value] = <span class="number">0</span></span><br><span class="line">        classCounts[value] += <span class="number">1</span></span><br><span class="line">    sortedClassCount = <span class="built_in">sorted</span>(classCounts.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分数据集</span></span><br><span class="line"><span class="comment"># dataSet:原始数据集</span></span><br><span class="line"><span class="comment"># axis:进行分割的指定列索引</span></span><br><span class="line"><span class="comment"># value:指定列中的值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">splitDataSet</span>(<span class="params">dataSet, axis, value</span>):</span><br><span class="line">    retDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featDataVal <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featDataVal[axis] == value:</span><br><span class="line">            <span class="comment"># 下面两行去除某一项指定列的值，很巧妙有没有</span></span><br><span class="line">            reducedFeatVal = featDataVal[:axis]</span><br><span class="line">            reducedFeatVal.extend(featDataVal[axis + <span class="number">1</span>:])</span><br><span class="line">            retDataSet.append(reducedFeatVal)</span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算香农熵</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcShannonEnt</span>(<span class="params">dataSet</span>):</span><br><span class="line">    <span class="comment"># 数据集总项数</span></span><br><span class="line">    numEntries = <span class="built_in">len</span>(dataSet)</span><br><span class="line">    <span class="comment"># 标签计数对象初始化</span></span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> featDataVal <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="comment"># 获取数据集每一项的最后一列的标签值</span></span><br><span class="line">        currentLabel = featDataVal[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 如果当前标签不在标签存储对象里，则初始化，然后计数</span></span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 熵初始化</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 遍历标签对象，求概率，计算熵</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">        prop = labelCounts[key] / <span class="built_in">float</span>(numEntries)</span><br><span class="line">        shannonEnt -= prop * log(prop, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选出最优特征列索引</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseBestFeatureToSplit</span>(<span class="params">dataSet</span>):</span><br><span class="line">    <span class="comment"># 计算特征个数，dataSet最后一列是标签属性，不是特征量</span></span><br><span class="line">    numFeatures = <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    <span class="comment"># 计算初始数据香农熵</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    <span class="comment"># 初始化信息增益，最优划分特征列索引</span></span><br><span class="line">    bestInfoGain = <span class="number">0.0</span></span><br><span class="line">    bestFeatureIndex = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeatures):</span><br><span class="line">        <span class="comment"># 获取每一列数据</span></span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">        <span class="comment"># 将每一列数据去重</span></span><br><span class="line">        uniqueVals = <span class="built_in">set</span>(featList)</span><br><span class="line">        newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)</span><br><span class="line">            <span class="comment"># 计算条件概率</span></span><br><span class="line">            prob = <span class="built_in">len</span>(subDataSet) / <span class="built_in">float</span>(<span class="built_in">len</span>(dataSet))</span><br><span class="line">            <span class="comment"># 计算条件熵</span></span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</span><br><span class="line">        <span class="comment"># 计算信息增益</span></span><br><span class="line">        infoGain = baseEntropy - newEntropy</span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeatureIndex = i</span><br><span class="line">    <span class="keyword">return</span> bestFeatureIndex</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 决策树创建</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createTree</span>(<span class="params">dataSet, labels</span>):</span><br><span class="line">    <span class="comment"># 获取标签属性，dataSet最后一列，区别于labels标签名称</span></span><br><span class="line">    classList = [example[-<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    <span class="comment"># 树极端终止条件判断</span></span><br><span class="line">    <span class="comment"># 标签属性值全部相同，返回标签属性第一项值</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == <span class="built_in">len</span>(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 只有一个特征（1列）</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    <span class="comment"># 获取最优特征列索引</span></span><br><span class="line">    bestFeatureIndex = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    <span class="comment"># 获取最优索引对应的标签名称</span></span><br><span class="line">    bestFeatureLabel = labels[bestFeatureIndex]</span><br><span class="line">    <span class="comment"># 创建根节点</span></span><br><span class="line">    myTree = &#123;bestFeatureLabel: &#123;&#125;&#125;</span><br><span class="line">    <span class="comment"># 去除最优索引对应的标签名，使labels标签能正确遍历</span></span><br><span class="line">    <span class="keyword">del</span> (labels[bestFeatureIndex])</span><br><span class="line">    <span class="comment"># 获取最优列</span></span><br><span class="line">    bestFeature = [example[bestFeatureIndex] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    uniquesVals = <span class="built_in">set</span>(bestFeature)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniquesVals:</span><br><span class="line">        <span class="comment"># 子标签名称集合</span></span><br><span class="line">        subLabels = labels[:]</span><br><span class="line">        <span class="comment"># 递归</span></span><br><span class="line">        myTree[bestFeatureLabel][value] = createTree(splitDataSet(dataSet, bestFeatureIndex, value), subLabels)</span><br><span class="line">    <span class="keyword">return</span> myTree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取分类结果</span></span><br><span class="line"><span class="comment"># inputTree:决策树字典</span></span><br><span class="line"><span class="comment"># featLabels:标签列表</span></span><br><span class="line"><span class="comment"># testVec:测试向量  例如：简单实例下某一路径 [1,1]  =&gt; yes（树干值组合，从根结点到叶子节点）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classify</span>(<span class="params">inputTree, featLabels, testVec</span>):</span><br><span class="line">    <span class="comment"># 获取根结点名称，将dict转化为list</span></span><br><span class="line">    firstSide = <span class="built_in">list</span>(inputTree.keys())</span><br><span class="line">    <span class="comment"># 根结点名称String类型</span></span><br><span class="line">    firstStr = firstSide[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 获取根结点对应的子节点</span></span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    <span class="comment"># 获取根结点名称在标签列表中对应的索引</span></span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    <span class="comment"># 由索引获取向量表中的对应值</span></span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    <span class="comment"># 获取树干向量后的对象</span></span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    <span class="comment"># 判断是子结点还是叶子节点：子结点就回调分类函数，叶子结点就是分类结果</span></span><br><span class="line">    <span class="comment"># if type(valueOfFeat).__name__==&#x27;dict&#x27;: 等价 if isinstance(valueOfFeat, dict):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(valueOfFeat, <span class="built_in">dict</span>):</span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        classLabel = valueOfFeat</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将决策树分类器存储在磁盘中，filename一般保存为txt格式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">storeTree</span>(<span class="params">inputTree, filename</span>):</span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fw = <span class="built_in">open</span>(filename, <span class="string">&#x27;wb+&#x27;</span>)</span><br><span class="line">    pickle.dump(inputTree, fw)</span><br><span class="line">    fw.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将瓷盘中的对象加载出来，这里的filename就是上面函数中的txt文件</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">grabTree</span>(<span class="params">filename</span>):</span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fr = <span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure><p><code>treePlotter.py</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string"> Created on 2019/10/22 12:00</span></span><br><span class="line"><span class="string"> treePlotter</span></span><br><span class="line"><span class="string"> @Author  : Zhouy</span></span><br><span class="line"><span class="string"> @Blog    : www.crocodilezs.top</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">decisionNode = <span class="built_in">dict</span>(boxstyle=<span class="string">&quot;sawtooth&quot;</span>, fc=<span class="string">&quot;0.8&quot;</span>)</span><br><span class="line">leafNode = <span class="built_in">dict</span>(boxstyle=<span class="string">&quot;round4&quot;</span>, fc=<span class="string">&quot;0.8&quot;</span>)</span><br><span class="line">arrow_args = <span class="built_in">dict</span>(arrowstyle=<span class="string">&quot;&lt;-&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取树的叶子节点</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getNumLeafs</span>(<span class="params">myTree</span>):</span><br><span class="line">    numLeafs = <span class="number">0</span></span><br><span class="line">    <span class="comment"># dict转化为list</span></span><br><span class="line">    firstSides = <span class="built_in">list</span>(myTree.keys())</span><br><span class="line">    firstStr = firstSides[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="comment"># 判断是否是叶子节点（通过类型判断，子类不存在，则类型为str；子类存在，则为dict）</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[</span><br><span class="line">                    key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:  <span class="comment"># test to see if the nodes are dictonaires, if not they are leaf nodes</span></span><br><span class="line">            numLeafs += getNumLeafs(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            numLeafs += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> numLeafs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取树的层数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTreeDepth</span>(<span class="params">myTree</span>):</span><br><span class="line">    maxDepth = <span class="number">0</span></span><br><span class="line">    <span class="comment"># dict转化为list</span></span><br><span class="line">    firstSides = <span class="built_in">list</span>(myTree.keys())</span><br><span class="line">    firstStr = firstSides[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[</span><br><span class="line">                    key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:  <span class="comment"># test to see if the nodes are dictonaires, if not they are leaf nodes</span></span><br><span class="line">            thisDepth = <span class="number">1</span> + getTreeDepth(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            thisDepth = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> thisDepth &gt; maxDepth: maxDepth = thisDepth</span><br><span class="line">    <span class="keyword">return</span> maxDepth</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotNode</span>(<span class="params">nodeTxt, centerPt, parentPt, nodeType</span>):</span><br><span class="line">    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords=<span class="string">&#x27;axes fraction&#x27;</span>,</span><br><span class="line">                            xytext=centerPt, textcoords=<span class="string">&#x27;axes fraction&#x27;</span>,</span><br><span class="line">                            va=<span class="string">&quot;center&quot;</span>, ha=<span class="string">&quot;center&quot;</span>, bbox=nodeType, arrowprops=arrow_args)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotMidText</span>(<span class="params">cntrPt, parentPt, txtString</span>):</span><br><span class="line">    xMid = (parentPt[<span class="number">0</span>] - cntrPt[<span class="number">0</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">0</span>]</span><br><span class="line">    yMid = (parentPt[<span class="number">1</span>] - cntrPt[<span class="number">1</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">1</span>]</span><br><span class="line">    createPlot.ax1.text(xMid, yMid, txtString, va=<span class="string">&quot;center&quot;</span>, ha=<span class="string">&quot;center&quot;</span>, rotation=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotTree</span>(<span class="params">myTree, parentPt, nodeTxt</span>):  <span class="comment"># if the first key tells you what feat was split on</span></span><br><span class="line">    numLeafs = getNumLeafs(myTree)  <span class="comment"># this determines the x width of this tree</span></span><br><span class="line">    depth = getTreeDepth(myTree)</span><br><span class="line">    firstSides = <span class="built_in">list</span>(myTree.keys())</span><br><span class="line">    firstStr = firstSides[<span class="number">0</span>]  <span class="comment"># the text label for this node should be this</span></span><br><span class="line">    cntrPt = (plotTree.xOff + (<span class="number">1.0</span> + <span class="built_in">float</span>(numLeafs)) / <span class="number">2.0</span> / plotTree.totalW, plotTree.yOff)</span><br><span class="line">    plotMidText(cntrPt, parentPt, nodeTxt)</span><br><span class="line">    plotNode(firstStr, cntrPt, parentPt, decisionNode)</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    plotTree.yOff = plotTree.yOff - <span class="number">1.0</span> / plotTree.totalD</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[</span><br><span class="line">                    key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:  <span class="comment"># test to see if the nodes are dictonaires, if not they are leaf nodes</span></span><br><span class="line">            plotTree(secondDict[key], cntrPt, <span class="built_in">str</span>(key))  <span class="comment"># recursion</span></span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># it&#x27;s a leaf node print the leaf node</span></span><br><span class="line">            plotTree.xOff = plotTree.xOff + <span class="number">1.0</span> / plotTree.totalW</span><br><span class="line">            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)</span><br><span class="line">            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, <span class="built_in">str</span>(key))</span><br><span class="line">    plotTree.yOff = plotTree.yOff + <span class="number">1.0</span> / plotTree.totalD</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># if you do get a dictonary you know it&#x27;s a tree, and the first element will be another dict</span></span><br><span class="line"><span class="comment"># 绘制决策树</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createPlot</span>(<span class="params">inTree</span>):</span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">    fig.clf()</span><br><span class="line">    axprops = <span class="built_in">dict</span>(xticks=[], yticks=[])</span><br><span class="line">    createPlot.ax1 = plt.subplot(<span class="number">111</span>, frameon=<span class="literal">False</span>, **axprops)  <span class="comment"># no ticks</span></span><br><span class="line">    <span class="comment"># createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses</span></span><br><span class="line">    plotTree.totalW = <span class="built_in">float</span>(getNumLeafs(inTree))</span><br><span class="line">    plotTree.totalD = <span class="built_in">float</span>(getTreeDepth(inTree))</span><br><span class="line">    plotTree.xOff = -<span class="number">0.5</span> / plotTree.totalW</span><br><span class="line">    plotTree.yOff = <span class="number">1.0</span></span><br><span class="line">    plotTree(inTree, (<span class="number">0.5</span>, <span class="number">1.0</span>), <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制树的根节点和叶子节点（根节点形状：长方形，叶子节点：椭圆形）</span></span><br><span class="line"><span class="comment"># def createPlot():</span></span><br><span class="line"><span class="comment">#    fig = plt.figure(1, facecolor=&#x27;white&#x27;)</span></span><br><span class="line"><span class="comment">#    fig.clf()</span></span><br><span class="line"><span class="comment">#    createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses</span></span><br><span class="line"><span class="comment">#    plotNode(&#x27;a decision node&#x27;, (0.5, 0.1), (0.1, 0.5), decisionNode)</span></span><br><span class="line"><span class="comment">#    plotNode(&#x27;a leaf node&#x27;, (0.8, 0.1), (0.3, 0.8), leafNode)</span></span><br><span class="line"><span class="comment">#    plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">retrieveTree</span>(<span class="params">i</span>):</span><br><span class="line">    listOfTrees = [&#123;<span class="string">&#x27;no surfacing&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;no&#x27;</span>, <span class="number">1</span>: &#123;<span class="string">&#x27;flippers&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;no&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;yes&#x27;</span>&#125;&#125;&#125;&#125;,</span><br><span class="line">                   &#123;<span class="string">&#x27;no surfacing&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;no&#x27;</span>, <span class="number">1</span>: &#123;<span class="string">&#x27;flippers&#x27;</span>: &#123;<span class="number">0</span>: &#123;<span class="string">&#x27;head&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;no&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;yes&#x27;</span>&#125;&#125;, <span class="number">1</span>: <span class="string">&#x27;no&#x27;</span>&#125;&#125;&#125;&#125;</span><br><span class="line">                   ]</span><br><span class="line">    <span class="keyword">return</span> listOfTrees[i]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># thisTree = retrieveTree(0)</span></span><br><span class="line"><span class="comment"># createPlot(thisTree)</span></span><br><span class="line"><span class="comment"># createPlot()</span></span><br><span class="line"><span class="comment"># myTree = retrieveTree(0)</span></span><br><span class="line"><span class="comment"># numLeafs =getNumLeafs(myTree)</span></span><br><span class="line"><span class="comment"># treeDepth =getTreeDepth(myTree)</span></span><br><span class="line"><span class="comment"># print(u&quot;叶子节点数目：%d&quot;% numLeafs)</span></span><br><span class="line"><span class="comment"># print(u&quot;树深度：%d&quot;%treeDepth)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>testTrees_3.py</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string"> Created on 2019/10/22 12:00</span></span><br><span class="line"><span class="string"> testTrees_3</span></span><br><span class="line"><span class="string"> @Author  : Zhouy</span></span><br><span class="line"><span class="string"> @Blog    : www.crocodilezs.top</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> myTrees <span class="keyword">as</span> mt</span><br><span class="line"><span class="keyword">import</span> treePlotter <span class="keyword">as</span> tp</span><br><span class="line"><span class="comment">#测试</span></span><br><span class="line">dataSet, labels = mt.createDataSet()</span><br><span class="line"><span class="comment">#copy函数：新开辟一块内存，然后将list的所有值复制到新开辟的内存中</span></span><br><span class="line">labels1 = labels.copy()</span><br><span class="line"><span class="comment">#createTree函数中将labels1的值改变了，所以在分类测试时不能用labels1</span></span><br><span class="line">myTree = mt.createTree(dataSet,labels1)</span><br><span class="line"><span class="comment">#保存树到本地</span></span><br><span class="line">mt.storeTree(myTree,<span class="string">&#x27;myTree.txt&#x27;</span>)</span><br><span class="line"><span class="comment">#在本地磁盘获取树</span></span><br><span class="line">myTree = mt.grabTree(<span class="string">&#x27;myTree.txt&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">u&quot;决策树结构：%s&quot;</span>%myTree)</span><br><span class="line"><span class="comment">#绘制决策树</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;绘制决策树：&quot;</span>)</span><br><span class="line">tp.createPlot(myTree)</span><br><span class="line">numLeafs =tp.getNumLeafs(myTree)</span><br><span class="line">treeDepth =tp.getTreeDepth(myTree)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;叶子节点数目：%d&quot;</span>% numLeafs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;树深度：%d&quot;</span>%treeDepth)</span><br><span class="line"><span class="comment">#测试分类 简单样本数据3列</span></span><br><span class="line">labelResult =mt.classify(myTree,labels,[<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;[1,1] 测试结果为：%s&quot;</span>%labelResult)</span><br><span class="line">labelResult =mt.classify(myTree,labels,[<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;[1,0] 测试结果为：%s&quot;</span>%labelResult)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;作业要求&quot;&gt;&lt;a href=&quot;#作业要求&quot; class=&quot;headerlink&quot; title=&quot;作业要求&quot;&gt;&lt;/a&gt;作业要求&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;实现&lt;code&gt;FINDS&lt;/code&gt;算法&lt;/li&gt;
&lt;li&gt;实现&lt;code&gt;ID3&lt;/code&gt;算法&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;不要调库自己写。如果有能力可以继续用课件里的数据集测试两个算法（用天气的4条记录测试&lt;code&gt;FINDS&lt;/code&gt;，用贷款的15条记录测试&lt;code&gt;ID3&lt;/code&gt;）给出训练误差测试误差等；  &lt;/li&gt;
&lt;li&gt;再有能力可以使用更大的数据集测试算法。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://blog.crocodilezs.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="FindS" scheme="https://blog.crocodilezs.top/tags/FindS/"/>
    
    <category term="ID3" scheme="https://blog.crocodilezs.top/tags/ID3/"/>
    
  </entry>
  
</feed>
