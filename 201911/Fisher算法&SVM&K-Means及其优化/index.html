<!DOCTYPE html>
<html lang='zh-CN'>

<head>
  <meta name="generator" content="Hexo 5.4.2">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.18.5">
  <meta charset="utf-8">
  

  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel='dns-prefetch' href='https://gcore.jsdelivr.net'>
  <link rel="preconnect" href="https://gcore.jsdelivr.net" crossorigin>
  <link rel='dns-prefetch' href='//unpkg.com'>

  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  
  <title>Fisher算法&SVM&K-Means及其优化 - 鳄鱼的博客</title>

  
    <meta name="description" content="fisher算法及其实现 请实现fisher算法，并采用自己随机生成2类数据（每类100个）的方式，验证自己的算法。参考资料:https:&#x2F;&#x2F;blog.csdn.net&#x2F;pengjian444&#x2F;article&#x2F;details&#x2F;71138003">
<meta property="og:type" content="article">
<meta property="og:title" content="Fisher算法&amp;SVM&amp;K-Means及其优化">
<meta property="og:url" content="https://blog.crocodilezs.top/201911/Fisher%E7%AE%97%E6%B3%95&SVM&K-Means%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/index.html">
<meta property="og:site_name" content="鳄鱼的博客">
<meta property="og:description" content="fisher算法及其实现 请实现fisher算法，并采用自己随机生成2类数据（每类100个）的方式，验证自己的算法。参考资料:https:&#x2F;&#x2F;blog.csdn.net&#x2F;pengjian444&#x2F;article&#x2F;details&#x2F;71138003">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.ax1x.com/2019/10/08/uWyoX4.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/10/08/uWy4pT.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/10/08/uWyIcF.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/10/08/uWy51U.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/10/08/uWyfhV.png">
<meta property="og:image" content="https://blog.crocodilezs.top/201911/Fisher%E7%AE%97%E6%B3%95&SVM&K-Means%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/output_12_1.png">
<meta property="og:image" content="https://blog.crocodilezs.top/201911/Fisher%E7%AE%97%E6%B3%95&SVM&K-Means%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/output_14_1.png">
<meta property="og:image" content="https://blog.crocodilezs.top/201911/Fisher%E7%AE%97%E6%B3%95&SVM&K-Means%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/output_16_0.png">
<meta property="og:image" content="https://blog.crocodilezs.top/201911/Fisher%E7%AE%97%E6%B3%95&SVM&K-Means%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/output_20_1.png">
<meta property="article:published_time" content="2019-10-22T04:21:10.000Z">
<meta property="article:modified_time" content="2022-05-23T17:20:51.000Z">
<meta property="article:author" content="CrocodileZS">
<meta property="article:tag" content="Fisher">
<meta property="article:tag" content="SVM">
<meta property="article:tag" content="K-Means">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.ax1x.com/2019/10/08/uWyoX4.png">
  
  

  <!-- feed -->
  
    <link rel="alternate" href="/atom.xml" title="鳄鱼的博客" type="application/atom+xml">
  

  
    
<link rel="stylesheet" href="/css/main.css">

  

  
    <link rel="shortcut icon" href="https://bu.dusays.com/2022/05/24/628bb5874996a.jpg">
  

  

  


  
    
      <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC&display=swap" rel="stylesheet">
    
  
</head>

<body>
  




  <div class='l_body' id='start'>
    <aside class='l_left' layout='post'>
    

  

<header class="header"><div class="logo-wrap"><a class="avatar" href="/about/"><div class="bg" style="opacity:0;background-image:url(https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.4/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="/media/avatar.png" onerror="javascript:this.classList.add('error');this.src='https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.4/image/2659360.svg';"></a><a class="title" href="/"><div class="main" ff="title">鳄鱼的博客</div><div class="sub cap">一只野生鳄鱼的互联网自留地</div></a></div>

<nav class="menu dis-select"><a class="nav-item active" href="/">博客</a><a class="nav-item" href="/chat/">动态</a><a class="nav-item" href="/about/">更多</a></nav>
</header>


<div class="widgets">
<widget class="widget-wrapper search"><div class="widget-body"><div class="search-wrapper" id="search"><form class="search-form"><input type="text" class="search-input" id="search-input" data-filter="/blog/" placeholder="文章搜索"><svg t="1670596976048" class="icon search-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2676" width="200" height="200"><path d="M938.2 832.6L723.8 618.1c-2.5-2.5-5.3-4.4-7.9-6.4 36.2-55.6 57.3-121.8 57.3-193.1C773.3 222.8 614.6 64 418.7 64S64 222.8 64 418.6c0 195.9 158.8 354.6 354.6 354.6 71.3 0 137.5-21.2 193.2-57.4 2 2.7 3.9 5.4 6.3 7.8L832.5 938c14.6 14.6 33.7 21.9 52.8 21.9 19.1 0 38.2-7.3 52.8-21.8 29.2-29.1 29.2-76.4 0.1-105.5M418.7 661.3C284.9 661.3 176 552.4 176 418.6 176 284.9 284.9 176 418.7 176c133.8 0 242.6 108.9 242.6 242.7 0 133.7-108.9 242.6-242.6 242.6" p-id="2677"></path></svg></form><div id="search-result"></div><div class="search-no-result">没有找到内容！</div></div></div></widget>


<widget class="widget-wrapper toc single" id="data-toc"><div class="widget-header cap dis-select"><span class="name">Fisher算法&SVM&K-Means及其优化</span></div><div class="widget-body fs14"><div class="doc-tree active"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#fisher%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0"><span class="toc-text">fisher算法及其实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90"><span class="toc-text">数据生成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fisher%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="toc-text">fisher算法实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A4%E5%AE%9A%E7%B1%BB%E5%88%AB"><span class="toc-text">判定类别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%98%E5%9B%BE"><span class="toc-text">绘图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM%E4%BC%98%E5%8C%96%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%E7%9A%84%E8%AF%A6%E7%BB%86%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B"><span class="toc-text">SVM优化对偶问题的详细推导过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM%E7%AE%97%E6%B3%95%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-text">SVM算法的实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#k-means%E7%AE%97%E6%B3%95%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-text">k-means算法的实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#k-means%E7%AE%97%E6%B3%95%E7%9A%84%E6%94%B9%E8%BF%9B"><span class="toc-text">k-means算法的改进</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#k-means-%EF%BC%88%E6%94%B9%E5%8F%98%E4%B8%AD%E5%BF%83%E7%82%B9%E7%9A%84%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95%EF%BC%89"><span class="toc-text">k-means++（改变中心点的选择方法）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#elkan-k-means%E7%AE%97%E6%B3%95"><span class="toc-text">elkan k-means算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ISODATA%E7%AE%97%E6%B3%95"><span class="toc-text">ISODATA算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-Batch-k-means%EF%BC%88%E7%94%A8%E4%B8%80%E9%83%A8%E5%88%86%E6%A0%B7%E6%9C%AC%E5%81%9A%E4%BC%A0%E7%BB%9F%E7%9A%84k-means%EF%BC%8C%E8%88%8D%E5%BC%83%E4%B8%80%E9%83%A8%E5%88%86%E7%B2%BE%E7%A1%AE%E5%BA%A6%E5%A4%A7%E5%A4%A7%E6%8F%90%E9%AB%98%E6%94%B6%E6%95%9B%E9%80%9F%E5%BA%A6%EF%BC%89"><span class="toc-text">Mini Batch k-means（用一部分样本做传统的k-means，舍弃一部分精确度大大提高收敛速度）</span></a></li></ol></li></ol></div></div></widget>

<widget class="widget-wrapper recent"><div class="widget-header cap theme dis-select"><span class="name">最近更新</span><a class="cap-action" id="rss" title="Subscribe" href="atom.xml"><svg class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="8938"><path d="M800.966 947.251c0-404.522-320.872-732.448-716.69-732.448V62.785c477.972 0 865.44 395.987 865.44 884.466h-148.75z m-162.273 0h-148.74c0-228.98-181.628-414.598-405.678-414.598v-152.01c306.205 0 554.418 253.68 554.418 566.608z m-446.24-221.12c59.748 0 108.189 49.503 108.189 110.557 0 61.063-48.44 110.563-108.188 110.563-59.747 0-108.18-49.5-108.18-110.563 0-61.054 48.433-110.556 108.18-110.556z" p-id="8939"></path></svg></a></div><div class="widget-body related-posts fs14"><a class="item title" href="/202212/%E5%9B%9E%E9%A1%BE2021%E5%B9%B4%E7%9A%84%E7%9B%AE%E6%A0%87%E6%B8%85%E5%8D%95/"><span class="title">回顾2021年的目标清单</span></a><a class="item title" href="/202212/2022%E5%B9%B4%E4%B9%A6%E5%BD%B1%E6%B8%B8%E9%9F%B3%E9%80%9F%E8%AF%84/"><span class="title">2022年的书籍/影视/游戏/音乐总结</span></a><a class="item title" href="/202212/2022%E5%B9%B4%E4%B8%AA%E4%BA%BAOKR%E6%A0%B8%E9%AA%8C/"><span class="title">2022年个人OKR核验</span></a><a class="item title" href="/202111/%E5%8D%9A%E5%AE%A2%E5%86%85%E5%AE%B9%E5%BD%92%E6%A1%A3%E5%92%8C%E8%8B%B1%E6%96%87%E5%8D%9A%E5%AE%A2/"><span class="title">博客内容归档和英文博客</span></a><a class="item title" href="/202202/pm%E9%9D%A2%E7%BB%8F/"><span class="title">2022年2月产品实习面经</span></a></div></widget>
</div>


    </aside>
    <div class='l_main'>
      

      


<div class="bread-nav fs12"><div id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a><span class="sep"></span><a class="cap breadcrumb" href="/">文章</a><span class="sep"></span><a class="cap breadcrumb-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div id="post-meta">发布于&nbsp;<time datetime="2019-10-22T04:21:10.000Z">2019-10-22</time></div></div>

<article class='md-text content post'>
<h1 class="article-title"><span>Fisher算法&SVM&K-Means及其优化</span></h1>
<h2 id="fisher算法及其实现"><a href="#fisher算法及其实现" class="headerlink" title="fisher算法及其实现"></a><code>fisher</code>算法及其实现</h2><ol>
<li>请实现<code>fisher</code>算法，并采用自己随机生成2类数据（每类100个）的方式，验证自己的算法。<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/pengjian444/article/details/71138003">参考资料:https://blog.csdn.net/pengjian444/article/details/71138003</a></li>
</ol>
<span id="more"></span>
<h3 id="数据生成"><a href="#数据生成" class="headerlink" title="数据生成"></a>数据生成</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_multilabel_classification</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x, y = make_multilabel_classification(n_samples=<span class="number">200</span>, n_features=<span class="number">2</span>,</span><br><span class="line">                                      n_labels=<span class="number">1</span>, n_classes=<span class="number">1</span>,</span><br><span class="line">                                      random_state=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据类别分类</span></span><br><span class="line">index1 = np.array([index <span class="keyword">for</span> (index, value) <span class="keyword">in</span> <span class="built_in">enumerate</span>(y) <span class="keyword">if</span> value == <span class="number">0</span>])  <span class="comment"># 获取类别1的indexs</span></span><br><span class="line">index2 = np.array([index <span class="keyword">for</span> (index, value) <span class="keyword">in</span> <span class="built_in">enumerate</span>(y) <span class="keyword">if</span> value == <span class="number">1</span>])  <span class="comment"># 获取类别2的indexs</span></span><br><span class="line"></span><br><span class="line">c_1 = x[index1]   <span class="comment"># 类别1的所有数据(x1, x2) in X_1</span></span><br><span class="line">c_2 = x[index2]  <span class="comment"># 类别2的所有数据(x1, x2) in X_2</span></span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.datasets.make_multilabel_classification.html#sklearn.datasets.make_multilabel_classification">make_multilabel_classification方法参数说明</a><br><code>n_samples</code>:样本的数量。<br><code>n_features</code>：样本的特征，这里是在二维平面中的点，所以为2.<br><code>n_labels</code>：每个实例的平均标签数。<br><code>n_classes</code>：分类问题的分类数。<br><code>random_state</code>：设置随机数种子，保证每次产生相同的数据。</p>
<p><a target="_blank" rel="noopener" href="https://www.runoob.com/python/python-func-enumerate.html">enumerate()函数说明</a><br><code>enumerate()</code>函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。</p>
<h3 id="fisher算法实现"><a href="#fisher算法实现" class="headerlink" title="fisher算法实现"></a>fisher算法实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cal_cov_and_avg</span>(<span class="params">samples</span>):</span><br><span class="line">    u1 = np.mean(samples, axis=<span class="number">0</span>)</span><br><span class="line">    cov_m = np.zeros((samples.shape[<span class="number">1</span>], samples.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> samples:</span><br><span class="line">        t = s - u1</span><br><span class="line">        cov_m += t * t.reshape(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> cov_m, u1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fisher</span>(<span class="params">c_1, c_2</span>):</span><br><span class="line">    cov_1, u1 = cal_cov_and_avg(c_1)</span><br><span class="line">    cov_2, u2 = cal_cov_and_avg(c_2)</span><br><span class="line">    s_w = cov_1 + cov_2</span><br><span class="line">    u, s, v = np.linalg.svd(s_w)  <span class="comment"># 奇异值分解</span></span><br><span class="line">    s_w_inv = np.dot(np.dot(v.T, np.linalg.inv(np.diag(s))), u.T)</span><br><span class="line">    <span class="keyword">return</span> np.dot(s_w_inv, u1 - u2)</span><br></pre></td></tr></table></figure>
<p><code>np.mean</code>：计算制定轴上的平均值。<br><code>np.zeros</code>：给定形状和类型确定的数组，并用0填充。</p>
<h3 id="判定类别"><a href="#判定类别" class="headerlink" title="判定类别"></a>判定类别</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">judge</span>(<span class="params">sample, w, c_1, c_2</span>):</span><br><span class="line">    u1 = np.mean(c_1, axis=<span class="number">0</span>)</span><br><span class="line">    u2 = np.mean(c_2, axis=<span class="number">0</span>)</span><br><span class="line">    center_1 = np.dot(w.T, u1)</span><br><span class="line">    center_2 = np.dot(w.T, u2)</span><br><span class="line">    pos = np.dot(w.T, sample)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">abs</span>(pos - center_1) &lt; <span class="built_in">abs</span>(pos - center_2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">w = fisher(c_1, c_2)  <span class="comment"># 调用函数，得到参数w</span></span><br><span class="line">out = judge(c_1[<span class="number">1</span>], w, c_1, c_2)   <span class="comment"># 判断所属的类别</span></span><br><span class="line"><span class="comment"># print(out)</span></span><br></pre></td></tr></table></figure>
<h3 id="绘图"><a href="#绘图" class="headerlink" title="绘图"></a>绘图</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.scatter(c_1[:, <span class="number">0</span>], c_1[:, <span class="number">1</span>], c=<span class="string">&#x27;#99CC99&#x27;</span>)</span><br><span class="line">plt.scatter(c_2[:, <span class="number">0</span>], c_2[:, <span class="number">1</span>], c=<span class="string">&#x27;#FFCC00&#x27;</span>)</span><br><span class="line">line_x = np.arange(<span class="built_in">min</span>(np.<span class="built_in">min</span>(c_1[:, <span class="number">0</span>]), np.<span class="built_in">min</span>(c_2[:, <span class="number">0</span>])),</span><br><span class="line">                   <span class="built_in">max</span>(np.<span class="built_in">max</span>(c_1[:, <span class="number">0</span>]), np.<span class="built_in">max</span>(c_2[:, <span class="number">0</span>])),</span><br><span class="line">                   step=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">line_y = - (w[<span class="number">0</span>] * line_x) / w[<span class="number">1</span>]</span><br><span class="line">plt.plot(line_x, line_y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;Figure size 640x480 with 1 Axes&gt;
</code></pre><h2 id="SVM优化对偶问题的详细推导过程"><a href="#SVM优化对偶问题的详细推导过程" class="headerlink" title="SVM优化对偶问题的详细推导过程"></a><code>SVM</code>优化对偶问题的详细推导过程</h2><ol>
<li>请给出<code>SVM</code>优化对偶问题的详细推导过程；并给出只有2维特征情况下的，对偶问题的优化求解过程（可以采用<code>lagrange</code>方法，也可以采用其他方法。）<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/49331510">参考资料：https://zhuanlan.zhihu.com/p/49331510</a>  </li>
</ol>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.ax1x.com/2019/10/08/uWyoX4.png" alt=""><br><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.ax1x.com/2019/10/08/uWy4pT.png" alt=""><br><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.ax1x.com/2019/10/08/uWyIcF.png" alt=""><br><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.ax1x.com/2019/10/08/uWy51U.png" alt=""><br><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.ax1x.com/2019/10/08/uWyfhV.png" alt=""></p>
<h2 id="SVM算法的实现"><a href="#SVM算法的实现" class="headerlink" title="SVM算法的实现"></a><code>SVM</code>算法的实现</h2><ol>
<li>请实现<code>SVM</code>算法；并采用自己随机生成2类线性可分数据（每类100个）的方式验证自己的算法。<br><a target="_blank" rel="noopener" href="https://www.jb51.net/article/131580.htm">参考资料：https://www.jb51.net/article/131580.htm</a></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line">  </span><br><span class="line">np.random.seed(<span class="number">0</span>) </span><br><span class="line">x = np.r_[np.random.randn(<span class="number">100</span>,<span class="number">2</span>)-[<span class="number">2</span>,<span class="number">2</span>],np.random.randn(<span class="number">100</span>,<span class="number">2</span>)+[<span class="number">2</span>,<span class="number">2</span>]] <span class="comment">#正态分布来产生数字,20行2列*2 </span></span><br><span class="line">y = [<span class="number">0</span>]*<span class="number">100</span>+[<span class="number">1</span>]*<span class="number">100</span> <span class="comment">#100个class0，100个class1 </span></span><br><span class="line">  </span><br><span class="line">clf = svm.SVC(kernel=<span class="string">&#x27;linear&#x27;</span>) </span><br><span class="line">clf.fit(x,y) </span><br><span class="line">  </span><br><span class="line">w = clf.coef_[<span class="number">0</span>] <span class="comment">#获取w </span></span><br><span class="line">a = -w[<span class="number">0</span>]/w[<span class="number">1</span>] <span class="comment">#斜率 </span></span><br><span class="line"><span class="comment">#画图划线 </span></span><br><span class="line">xx = np.linspace(-<span class="number">5</span>,<span class="number">5</span>) <span class="comment">#(-5,5)之间x的值 </span></span><br><span class="line">yy = a*xx-(clf.intercept_[<span class="number">0</span>])/w[<span class="number">1</span>] <span class="comment">#xx带入y，截距 </span></span><br><span class="line">  </span><br><span class="line"><span class="comment">#画出与点相切的线 </span></span><br><span class="line">b = clf.support_vectors_[<span class="number">0</span>] </span><br><span class="line">yy_down = a*xx+(b[<span class="number">1</span>]-a*b[<span class="number">0</span>]) </span><br><span class="line">b = clf.support_vectors_[-<span class="number">1</span>] </span><br><span class="line">yy_up = a*xx+(b[<span class="number">1</span>]-a*b[<span class="number">0</span>]) </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W:&quot;</span>,w) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;a:&quot;</span>,a) </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;support_vectors_:&quot;</span>,clf.support_vectors_) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;clf.coef_:&quot;</span>,clf.coef_) </span><br><span class="line">  </span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">4</span>)) </span><br><span class="line">plt.plot(xx,yy) </span><br><span class="line">plt.plot(xx,yy_down) </span><br><span class="line">plt.plot(xx,yy_up) </span><br><span class="line">plt.scatter(clf.support_vectors_[:,<span class="number">0</span>],clf.support_vectors_[:,<span class="number">1</span>],s=<span class="number">80</span>) </span><br><span class="line">plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>],c=y,cmap=plt.cm.Paired) <span class="comment">#[:，0]列切片，第0列 </span></span><br><span class="line">  </span><br><span class="line">plt.axis(<span class="string">&#x27;tight&#x27;</span>) </span><br><span class="line">  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>W: [0.95070185 1.15607502]
a: -0.8223530762163854
support_vectors_: [[-0.51174781 -0.10411082]
 [ 0.16323595 -0.66347205]
 [ 2.39904635 -0.77259276]
 [ 0.66574153  0.65328249]
 [-0.25556423  0.97749316]]
clf.coef_: [[0.95070185 1.15607502]]
</code></pre><p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="output_12_1.png" alt="png"></p>
<h2 id="k-means算法的实现"><a href="#k-means算法的实现" class="headerlink" title="k-means算法的实现"></a><code>k-means</code>算法的实现</h2><ol>
<li>请实现<code>k-means</code>的算法；并采用自己随机生成3类数据（每类100个）的方式，验证自己的算法。<br><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1465020">参考资料1:https://cloud.tencent.com/developer/article/1465020</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42029738/article/details/81978038">参考资料2:https://blog.csdn.net/weixin_42029738/article/details/81978038</a>  </li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    K-Means clustering algorithms</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(__doc__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MiniBatchKMeans, KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> pairwise_distances_argmin</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Generate sample data</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">45</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">1</span>, -<span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>]] <span class="comment"># 初始化3个中心</span></span><br><span class="line">n_clusters = <span class="built_in">len</span>(centers) <span class="comment"># 聚类的数目为3</span></span><br><span class="line"><span class="comment"># 产生10000组二维数据，以上面三个点为中心，以(-10,10)为边界，数据集的标准差是0.7</span></span><br><span class="line">X, labels_true = make_blobs(n_samples=<span class="number">10000</span>, centers=centers, cluster_std=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Compute clustering with Means</span></span><br><span class="line"></span><br><span class="line">k_means = KMeans(init=<span class="string">&#x27;k-means++&#x27;</span>, n_clusters=<span class="number">3</span>, n_init=<span class="number">10</span>)</span><br><span class="line">t0 = time.time()</span><br><span class="line">k_means.fit(X)</span><br><span class="line"><span class="comment"># 使用k-means对300组数据集训练算法的时间消耗</span></span><br><span class="line">t_batch = time.time() - t0</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Plot result</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">3</span>))</span><br><span class="line">fig.subplots_adjust(left=<span class="number">0.02</span>, right=<span class="number">0.98</span>, bottom=<span class="number">0.05</span>, top=<span class="number">0.9</span>)</span><br><span class="line">colors = [<span class="string">&#x27;#4EACC5&#x27;</span>, <span class="string">&#x27;#FF9C34&#x27;</span>, <span class="string">&#x27;#4E9A06&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># We want to have the same colors for the same cluster from the</span></span><br><span class="line"><span class="comment"># MiniBatchKMeans and the KMeans algorithm. Let&#x27;s pair the cluster centers per</span></span><br><span class="line"><span class="comment"># closest one.</span></span><br><span class="line">k_means_cluster_centers = np.sort(k_means.cluster_centers_, axis=<span class="number">0</span>)</span><br><span class="line">k_means_labels = pairwise_distances_argmin(X, k_means_cluster_centers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># KMeans</span></span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(n_clusters), colors):</span><br><span class="line">    my_members = k_means_labels == k</span><br><span class="line">    cluster_center = k_means_cluster_centers[k]</span><br><span class="line">    ax.plot(X[my_members, <span class="number">0</span>], X[my_members, <span class="number">1</span>], <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">            markerfacecolor=col, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">    ax.plot(cluster_center[<span class="number">0</span>], cluster_center[<span class="number">1</span>], <span class="string">&#x27;o&#x27;</span>, markerfacecolor=col,</span><br><span class="line">            markeredgecolor=<span class="string">&#x27;k&#x27;</span>, markersize=<span class="number">6</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;KMeans&#x27;</span>)</span><br><span class="line">ax.set_xticks(())</span><br><span class="line">ax.set_yticks(())</span><br><span class="line">plt.text(-<span class="number">3.5</span>, <span class="number">1.8</span>,  <span class="string">&#x27;train time: %.2fs\ninertia: %f&#x27;</span> % (</span><br><span class="line">    t_batch, k_means.inertia_))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>    K-Means clustering algorithms
</code></pre><p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="output_14_1.png" alt="png"></p>
<h2 id="k-means算法的改进"><a href="#k-means算法的改进" class="headerlink" title="k-means算法的改进"></a><code>k-means</code>算法的改进</h2><ol>
<li>请给出三种<code>k-means</code>算法在大数据量时的改进方法，并分析改进的结果。<br>改进方法包括：<ol>
<li><code>k-means++</code>（改变中心点的选取方法）</li>
<li><code>elkan K-Means</code>（减少不必要的距离计算）</li>
<li><code>ISODATA</code>算法（在运行过程中根据实际情况调整聚类中心数k）</li>
<li><code>Mini Batch k-means</code>算法（采用部分样本，舍弃一些精确度大大加快收敛速度）</li>
</ol>
</li>
</ol>
<p>其中1和4改进方法给出了源码和对比。</p>
<h3 id="k-means-（改变中心点的选择方法）"><a href="#k-means-（改变中心点的选择方法）" class="headerlink" title="k-means++（改变中心点的选择方法）"></a><code>k-means++</code>（改变中心点的选择方法）</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/github_39261590/article/details/76910689">参考资料1:https://blog.csdn.net/github_39261590/article/details/76910689</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/yszd/p/9672885.html">参考资料2:https://www.cnblogs.com/yszd/p/9672885.html</a><br><code>k-means++</code>算法选择初始seeds的基本思想就是：<strong>初始的聚类中心之间的相互距离要尽可能的远。</strong>  </p>
<p>算法步骤：</p>
<ol>
<li>从输入的数据点集合中随机选择一个点作为第一个聚类中心</li>
<li>对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)</li>
<li>选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大</li>
<li>重复2和3直到k个聚类中心被选出来</li>
<li>利用这k个初始的聚类中心来运行标准的k-means算法</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets <span class="keyword">as</span> ds</span><br><span class="line"><span class="keyword">import</span> matplotlib.colors</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MiniBatchKMeans</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">expand</span>(<span class="params">a, b</span>):</span><br><span class="line">    d = (b - a) * <span class="number">0.1</span></span><br><span class="line">    <span class="keyword">return</span> a-b, b+d</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    N = <span class="number">400</span></span><br><span class="line">    centers = <span class="number">4</span></span><br><span class="line">    data, y = ds.make_blobs(N, n_features=<span class="number">2</span>, centers=centers, random_state=<span class="number">2</span>)</span><br><span class="line">    data2, y2 = ds.make_blobs(N, n_features=<span class="number">2</span>, centers=centers, cluster_std=(<span class="number">1</span>, <span class="number">2.5</span>, <span class="number">0.5</span>, <span class="number">2</span>), random_state=<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 按行拼接numpy数组</span></span><br><span class="line">    data3 = np.vstack((data[y == <span class="number">0</span>][:], data[y == <span class="number">1</span>][:<span class="number">50</span>], data[y == <span class="number">2</span>][:<span class="number">20</span>], data[y == <span class="number">3</span>][:<span class="number">5</span>]))</span><br><span class="line">    y3 = np.array([<span class="number">0</span>] * <span class="number">100</span> + [<span class="number">1</span>] * <span class="number">50</span> + [<span class="number">2</span>] * <span class="number">20</span> + [<span class="number">3</span>] * <span class="number">5</span>)</span><br><span class="line">    cls = KMeans(n_clusters=<span class="number">4</span>, init=<span class="string">&#x27;k-means++&#x27;</span>)</span><br><span class="line">    y_hat = cls.fit_predict(data)</span><br><span class="line">    y2_hat = cls.fit_predict(data2)</span><br><span class="line">    y3_hat = cls.fit_predict(data3)</span><br><span class="line">    </span><br><span class="line">    m = np.array(((<span class="number">1</span>, <span class="number">1</span>),(<span class="number">1</span>, <span class="number">3</span>)))</span><br><span class="line">    data_r = data.dot(m)</span><br><span class="line">    y_r_hat = cls.fit_predict(data_r)</span><br><span class="line">    </span><br><span class="line">    matplotlib.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">u&#x27;SimHei&#x27;</span>]</span><br><span class="line">    matplotlib.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span></span><br><span class="line">    cm = matplotlib.colors.ListedColormap(<span class="built_in">list</span>(<span class="string">&#x27;rgbm&#x27;</span>))</span><br><span class="line">    plt.figure(figsize=(<span class="number">9</span>, <span class="number">10</span>), facecolor=<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    plt.subplot(<span class="number">421</span>)</span><br><span class="line">    plt.title(<span class="string">u&#x27;原始数据&#x27;</span>)</span><br><span class="line">    plt.scatter(data[:, <span class="number">0</span>], data[:, <span class="number">1</span>], c=y, s=<span class="number">30</span>, cmap=cm, edgecolors=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    x1_min, x2_min = np.<span class="built_in">min</span>(data, axis=<span class="number">0</span>)</span><br><span class="line">    x1_max, x2_max = np.<span class="built_in">max</span>(data, axis=<span class="number">0</span>)</span><br><span class="line">    x1_min, x1_max = expand(x1_min, x1_max)</span><br><span class="line">    x2_min, x2_max = expand(x2_min, x2_max)</span><br><span class="line">    plt.xlim((x1_min, x1_max))</span><br><span class="line">    plt.ylim((x2_min, x2_max))</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">422</span>)</span><br><span class="line">    plt.title(<span class="string">u&#x27;KMeans++聚类&#x27;</span>)</span><br><span class="line">    plt.scatter(data[:, <span class="number">0</span>], data[:, <span class="number">1</span>], c=y_hat, s=<span class="number">30</span>, cmap=cm, edgecolors=<span class="string">&#x27;none&#x27;</span>)    </span><br><span class="line">    plt.xlim((x1_min, x1_max))</span><br><span class="line">    plt.ylim((x2_min, x2_max))</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="output_16_0.png" alt="png"></p>
<h3 id="elkan-k-means算法"><a href="#elkan-k-means算法" class="headerlink" title="elkan k-means算法"></a><code>elkan k-means</code>算法</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u014465639/article/details/71342072">参考资料:https://blog.csdn.net/u014465639/article/details/71342072</a><br><code>elkan K-Means</code>利用了两边之和大于等于第三边,以及两边之差小于第三边的三角形性质，来减少距离的计算。<br>第一种规律是对于一个样本点和两个质心。如果我们预先计算出了这两个质心之间的距离，则如果计算发现,我们立即就可以知道。此时我们不需要再计算,也就是说省了一步距离计算。<br>第二种规律是对于一个样本点和两个质心。我们可以得到。这个从三角形的性质也很容易得到。<br>利用上边的两个规律，elkan K-Means比起传统的K-Means迭代速度有很大的提高。但是如果我们的样本的特征是稀疏的，有缺失值的话，这个方法就不使用了，此时某些距离无法计算，则不能使用该算法。  </p>
<h3 id="ISODATA算法"><a href="#ISODATA算法" class="headerlink" title="ISODATA算法"></a><code>ISODATA</code>算法</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/houston11235/article/details/8511379">参考资料1:https://blog.csdn.net/houston11235/article/details/8511379</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/huadongw/p/4101422.html">参考资料2:https://www.cnblogs.com/huadongw/p/4101422.html</a><br>k-means 的一个缺点就是必须指定聚类的个数，这个有些时候并不太行得通。于是就要求最好这个类别的个数也可以改变，这就形成了 isodata 方法，通过设定一些类别分裂和合并的条件，在聚类的过程中自动增减类别的数目。当然这也带来了一个问题，就是这个条件有时候并不那么好给出。当然 isodata 在很多情况下还是可以得到比较靠谱的结果。  </p>
<h3 id="Mini-Batch-k-means（用一部分样本做传统的k-means，舍弃一部分精确度大大提高收敛速度）"><a href="#Mini-Batch-k-means（用一部分样本做传统的k-means，舍弃一部分精确度大大提高收敛速度）" class="headerlink" title="Mini Batch k-means（用一部分样本做传统的k-means，舍弃一部分精确度大大提高收敛速度）"></a><code>Mini Batch k-means</code>（用一部分样本做传统的k-means，舍弃一部分精确度大大提高收敛速度）</h3><p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1465020">参考资料1:https://cloud.tencent.com/developer/article/1465020</a><br>Mini Batch KMeans算法是一种能<strong>尽量保持聚类准确性下但能大幅度降低计算时间的聚类模型</strong>，采用小批量的数据子集减少计算时间，同时仍试图优化目标函数，这里所谓的Mini Batch是指每次训练算法时随机抽取的数据子集，采用这些随机选取的数据进行训练，大大的减少了计算的时间，减少的KMeans算法的收敛时间，但要比标准算法略差一点，建议当样本量大于一万做聚类时，就需要考虑选用Mini Batch KMeans算法。  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Comparison of the K-Means and MiniBatchKMeans clustering algorithms</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(__doc__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MiniBatchKMeans, KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> pairwise_distances_argmin</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Generate sample data</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">45</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">1</span>, -<span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>]] <span class="comment"># 初始化3个中心</span></span><br><span class="line">n_clusters = <span class="built_in">len</span>(centers) <span class="comment"># 聚类的数目为3</span></span><br><span class="line"><span class="comment"># 产生10000组二维数据，以上面三个点为中心，以(-10,10)为边界，数据集的标准差是0.7</span></span><br><span class="line">X, labels_true = make_blobs(n_samples=<span class="number">10000</span>, centers=centers, cluster_std=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Compute clustering with Means</span></span><br><span class="line"></span><br><span class="line">k_means = KMeans(init=<span class="string">&#x27;k-means++&#x27;</span>, n_clusters=<span class="number">3</span>, n_init=<span class="number">10</span>)</span><br><span class="line">t0 = time.time()</span><br><span class="line">k_means.fit(X)</span><br><span class="line"><span class="comment"># 使用k-means对300组数据集训练算法的时间消耗</span></span><br><span class="line">t_batch = time.time() - t0</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Compute clustering with MiniBatchKMeans</span></span><br><span class="line"></span><br><span class="line">mbk = MiniBatchKMeans(init=<span class="string">&#x27;k-means++&#x27;</span>, n_clusters=<span class="number">3</span>, batch_size=batch_size,</span><br><span class="line">                      n_init=<span class="number">10</span>, max_no_improvement=<span class="number">10</span>, verbose=<span class="number">0</span>)</span><br><span class="line">t0 = time.time()</span><br><span class="line">mbk.fit(X)</span><br><span class="line"><span class="comment"># 使用MiniBatchKMeans对300组数据集训练算法的时间消耗</span></span><br><span class="line">t_mini_batch = time.time() - t0</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Plot result</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">3</span>))</span><br><span class="line">fig.subplots_adjust(left=<span class="number">0.02</span>, right=<span class="number">0.98</span>, bottom=<span class="number">0.05</span>, top=<span class="number">0.9</span>)</span><br><span class="line">colors = [<span class="string">&#x27;#4EACC5&#x27;</span>, <span class="string">&#x27;#FF9C34&#x27;</span>, <span class="string">&#x27;#4E9A06&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># We want to have the same colors for the same cluster from the</span></span><br><span class="line"><span class="comment"># MiniBatchKMeans and the KMeans algorithm. Let&#x27;s pair the cluster centers per</span></span><br><span class="line"><span class="comment"># closest one.</span></span><br><span class="line">k_means_cluster_centers = np.sort(k_means.cluster_centers_, axis=<span class="number">0</span>)</span><br><span class="line">mbk_means_cluster_centers = np.sort(mbk.cluster_centers_, axis=<span class="number">0</span>)</span><br><span class="line">k_means_labels = pairwise_distances_argmin(X, k_means_cluster_centers)</span><br><span class="line">mbk_means_labels = pairwise_distances_argmin(X, mbk_means_cluster_centers)</span><br><span class="line">order = pairwise_distances_argmin(k_means_cluster_centers,</span><br><span class="line">                                  mbk_means_cluster_centers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># KMeans</span></span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(n_clusters), colors):</span><br><span class="line">    my_members = k_means_labels == k</span><br><span class="line">    cluster_center = k_means_cluster_centers[k]</span><br><span class="line">    ax.plot(X[my_members, <span class="number">0</span>], X[my_members, <span class="number">1</span>], <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">            markerfacecolor=col, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">    ax.plot(cluster_center[<span class="number">0</span>], cluster_center[<span class="number">1</span>], <span class="string">&#x27;o&#x27;</span>, markerfacecolor=col,</span><br><span class="line">            markeredgecolor=<span class="string">&#x27;k&#x27;</span>, markersize=<span class="number">6</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;KMeans&#x27;</span>)</span><br><span class="line">ax.set_xticks(())</span><br><span class="line">ax.set_yticks(())</span><br><span class="line">plt.text(-<span class="number">3.5</span>, <span class="number">1.8</span>,  <span class="string">&#x27;train time: %.2fs\ninertia: %f&#x27;</span> % (</span><br><span class="line">    t_batch, k_means.inertia_))</span><br><span class="line"></span><br><span class="line"><span class="comment"># MiniBatchKMeans</span></span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(n_clusters), colors):</span><br><span class="line">    my_members = mbk_means_labels == order[k]</span><br><span class="line">    cluster_center = mbk_means_cluster_centers[order[k]]</span><br><span class="line">    ax.plot(X[my_members, <span class="number">0</span>], X[my_members, <span class="number">1</span>], <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">            markerfacecolor=col, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">    ax.plot(cluster_center[<span class="number">0</span>], cluster_center[<span class="number">1</span>], <span class="string">&#x27;o&#x27;</span>, markerfacecolor=col,</span><br><span class="line">            markeredgecolor=<span class="string">&#x27;k&#x27;</span>, markersize=<span class="number">6</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;MiniBatchKMeans&#x27;</span>)</span><br><span class="line">ax.set_xticks(())</span><br><span class="line">ax.set_yticks(())</span><br><span class="line">plt.text(-<span class="number">3.5</span>, <span class="number">1.8</span>, <span class="string">&#x27;train time: %.2fs\ninertia: %f&#x27;</span> %</span><br><span class="line">         (t_mini_batch, mbk.inertia_))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialise the different array to all False</span></span><br><span class="line">different = (mbk_means_labels == <span class="number">4</span>)</span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n_clusters):</span><br><span class="line">    different += ((k_means_labels == k) != (mbk_means_labels == order[k]))</span><br><span class="line"></span><br><span class="line">identic = np.logical_not(different)</span><br><span class="line">ax.plot(X[identic, <span class="number">0</span>], X[identic, <span class="number">1</span>], <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">        markerfacecolor=<span class="string">&#x27;#bbbbbb&#x27;</span>, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">ax.plot(X[different, <span class="number">0</span>], X[different, <span class="number">1</span>], <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">        markerfacecolor=<span class="string">&#x27;m&#x27;</span>, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Difference&#x27;</span>)</span><br><span class="line">ax.set_xticks(())</span><br><span class="line">ax.set_yticks(())</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>    Comparison of the K-Means and MiniBatchKMeans clustering algorithms
</code></pre><p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="output_20_1.png" alt="png"></p>


<div class="article-footer reveal fs14"><section id="license"><div class="header"><span>许可协议</span></div><div class="body"><p>本文采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享 4.0 国际</a> 许可协议，转载请注明出处。</p>
</div></section></div>

</article>

<div class="related-wrap reveal" id="read-next"><section class="body"><div class="item" id="prev"><div class="note">较新文章</div><a href="/201911/KNN%E4%B8%8ENaive_Bayes%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/">KNN与Naive_Bayes代码实现</a></div><div class="item" id="next"><div class="note">较早文章</div><a href="/201909/%E5%86%99%E4%BA%8E%E5%8D%9A%E5%AE%A2%E6%AD%A3%E5%BC%8F%E5%AF%B9%E5%A4%96%E5%BC%80%E6%94%BE%E4%B9%8B%E9%99%85/">写于10月15日——博客正式对外开放之际</a></div></section></div>






  <div class='related-wrap md-text reveal' id="comments">
    <section class='header cmt-title cap theme'>
      快来参与讨论吧
    </section>
    <section class='body cmt-body waline'>
      

<div id="waline_container" class="waline_thread"><svg class="loading" style="vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2709"><path d="M832 512c0-176-144-320-320-320V128c211.2 0 384 172.8 384 384h-64zM192 512c0 176 144 320 320 320v64C300.8 896 128 723.2 128 512h64z" p-id="2710"></path></svg></div>

    </section>
  </div>



      
<footer class="page-footer reveal fs12"><hr><div class="text"><p>本站由 <a href="/">@CrocodileZS</a> 使用 <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar">Stellar</a> 主题创建。<br>本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议，转载请注明出处。</p>
</div></footer>

      <div class='float-panel mobile-only blur' style='display:none'>
  <button type='button' class='sidebar-toggle mobile' onclick='sidebar.toggle()'>
    <svg class="icon" style="width: 1em; height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15301"><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 2.3 26.8 24.6 47.5 51.6 47.6h416.5v4z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15302"></path><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 1.9 27.7 23.9 49.7 51.6 51.6h416.5z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15303"></path></svg>
  </button>
</div>

    </div>
  </div>
  <div class='scripts'>
    <script type="text/javascript">
  const stellar = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    loadCSS: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    // 从 butterfly 和 volantis 获得灵感
    loadScript: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    // https://github.com/jerryc127/hexo-theme-butterfly
    jQuery: (fn) => {
      if (typeof jQuery === 'undefined') {
        stellar.loadScript(stellar.plugins.jQuery).then(fn)
      } else {
        fn()
      }
    }
  };
  stellar.version = '1.18.5';
  stellar.github = 'https://github.com/xaoxuu/hexo-theme-stellar/tree/1.18.5';
  stellar.config = {
    date_suffix: {
      just: '刚刚',
      min: '分钟前',
      hour: '小时前',
      day: '天前',
      month: '个月前',
    },
  };

  // required plugins (only load if needs)
  stellar.plugins = {
    jQuery: 'https://gcore.jsdelivr.net/npm/jquery@3.6.2/dist/jquery.min.js'
  };

  if ('local_search') {
    stellar.search = {};
    stellar.search.service = 'local_search';
    if (stellar.search.service == 'local_search') {
      let service_obj = Object.assign({}, {"field":"all","path":"/search.json","content":true,"sort":"-date"});
      stellar.search[stellar.search.service] = service_obj;
    }
  }

  // stellar js
  stellar.plugins.stellar = Object.assign({"sites":"/js/plugins/sites.js","friends":"/js/plugins/friends.js","ghinfo":"/js/plugins/ghinfo.js","timeline":"/js/plugins/timeline.js","linkcard":"/js/plugins/linkcard.js","fcircle":"/js/plugins/fcircle.js","weibo":"/js/plugins/weibo.js"});

  stellar.plugins.marked = Object.assign("https://cdn.bootcdn.net/ajax/libs/marked/4.0.18/marked.min.js");
  // optional plugins
  if ('true' == 'true') {
    stellar.plugins.lazyload = Object.assign({"enable":true,"js":"https://gcore.jsdelivr.net/npm/vanilla-lazyload@17.8.3/dist/lazyload.min.js","transition":"blur"});
  }
  if ('true' == 'true') {
    stellar.plugins.swiper = Object.assign({"enable":true,"css":"https://unpkg.com/swiper@8.4.5/swiper-bundle.min.css","js":"https://unpkg.com/swiper@8.4.5/swiper-bundle.min.js"});
  }
  if ('' == 'true') {
    stellar.plugins.scrollreveal = Object.assign({"enable":null,"js":"https://gcore.jsdelivr.net/npm/scrollreveal@4.0.9/dist/scrollreveal.min.js","distance":"8px","duration":500,"interval":100,"scale":1});
  }
  if ('true' == 'true') {
    stellar.plugins.preload = Object.assign({"enable":true,"service":"flying_pages","instant_page":"https://gcore.jsdelivr.net/gh/volantis-x/cdn-volantis@4.1.2/js/instant_page.js","flying_pages":"https://gcore.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"});
  }
  if ('true' == 'true') {
    stellar.plugins.fancybox = Object.assign({"enable":true,"js":"https://gcore.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.umd.js","css":"https://gcore.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.css","selector":".swiper-slide img"});
  }
  if ('false' == 'true') {
    stellar.plugins.heti = Object.assign({"enable":false,"css":"https://unpkg.com/heti@0.9.2/umd/heti.min.css","js":"https://unpkg.com/heti@0.9.2/umd/heti-addon.min.js"});
  }
</script>

<!-- required -->

  
<script src="/js/main.js" async></script>



<!-- optional -->

  <script>
  function load_comment(){
    if(!document.getElementById("waline_container"))return;
    stellar.loadCSS('https://unpkg.com/@waline/client@2.14.1/dist/waline.css');
    stellar.loadScript('https://unpkg.com/@waline/client@2.14.1/dist/waline.js', {defer:true}).then(function () {
      const el = document.getElementById("waline_container");
      var path = el.getAttribute('comment_id');
      if (!path) {
        path = decodeURI(window.location.pathname);
      }
      Waline.init(Object.assign({"js":"https://unpkg.com/@waline/client@2.14.1/dist/waline.js","css":"https://unpkg.com/@waline/client@2.14.1/dist/waline.css","serverURL":"https://comments-waline-ak74rjyxn-crocodilezs.vercel.app","commentCount":true,"pageview":true,"emoji":{"default":"https://cdn.jsdelivr.net/gh/volantis-x/cdn-emoji/qq/%s.gif","twemoji":"https://cdn.jsdelivr.net/gh/twitter/twemoji/assets/svg/%s.svg","qq":"https://cdn.jsdelivr.net/gh/volantis-x/cdn-emoji/qq/%s.gif","aru":"https://cdn.jsdelivr.net/gh/volantis-x/cdn-emoji/aru-l/%s.gif","tieba":"https://cdn.jsdelivr.net/gh/volantis-x/cdn-emoji/tieba/%s.png"}}, {
        el: '#waline_container',
        path: path,
      }));
    });
  }
  window.addEventListener('DOMContentLoaded', (event) => {
    console.log('DOM fully loaded and parsed');
    load_comment();
  });

</script>




<!-- inject -->


  </div>
</body>
</html>
