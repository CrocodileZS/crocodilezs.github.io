<!DOCTYPE html>





<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="generator" content="Hexo 3.9.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/rainbow.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/rainbow.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/rainbow.png?v=7.3.0">
  <link rel="mask-icon" href="/images/rainbow.svg?v=7.3.0" color="#222">
  <meta name="baidu-site-verification" content="MBR3SWgP01">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css">
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":true},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    save_scroll: true,
    copycode: {"enable":true,"show_result":true,"style":"flat"},
    fancybox: true,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>

  <meta name="description" content="神经网络作业报告[TOC]本次作业部分源码的参考资料已注明 一、实验要求 推导具有单隐层的神经网络的前向传播和反向传播算法，并进行编程（可以使用sklearn中的神经网络）。 探讨10，30，100，300，1000，不同隐藏节点数对网络性能的影响。 探讨不同学习率和迭代次数对网络性能的影响。 改变数据的标准化方法，探讨对训练的影响。   查阅资料说明什么是Hebb学习规则">
<meta name="keywords" content="神经网络">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络作业报告">
<meta property="og:url" content="https://blog.crocodilezs.top/201911/神经网络作业报告/index.html">
<meta property="og:site_name" content="鰐魚先生的水族館">
<meta property="og:description" content="神经网络作业报告[TOC]本次作业部分源码的参考资料已注明 一、实验要求 推导具有单隐层的神经网络的前向传播和反向传播算法，并进行编程（可以使用sklearn中的神经网络）。 探讨10，30，100，300，1000，不同隐藏节点数对网络性能的影响。 探讨不同学习率和迭代次数对网络性能的影响。 改变数据的标准化方法，探讨对训练的影响。   查阅资料说明什么是Hebb学习规则">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://blog.crocodilezs.top/201911/神经网络作业报告/image4/reduction1.jpg">
<meta property="og:image" content="https://blog.crocodilezs.top/201911/神经网络作业报告/image4/reduction2.jpg">
<meta property="og:image" content="https://blog.crocodilezs.top/201911/神经网络作业报告/image4/reduction3.jpg">
<meta property="og:image" content="https://blog.crocodilezs.top/201911/神经网络作业报告/image4/reduction4.jpg">
<meta property="og:image" content="https://blog.crocodilezs.top/201911/神经网络作业报告/image4/256-0.01-40.jpg">
<meta property="og:image" content="https://blog.crocodilezs.top/201911/神经网络作业报告/image4/256-0.01-40-1.jpg">
<meta property="og:image" content="https://blog.crocodilezs.top/201911/神经网络作业报告/image4/256-0.01-40-2.jpg">
<meta property="og:image" content="https://blog.crocodilezs.top/201911/神经网络作业报告/image4/10-0.01-40.jpg">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/520787/201510/520787-20151021081107630-1544768706.png">
<meta property="og:updated_time" content="2019-11-12T14:33:52.646Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络作业报告">
<meta name="twitter:description" content="神经网络作业报告[TOC]本次作业部分源码的参考资料已注明 一、实验要求 推导具有单隐层的神经网络的前向传播和反向传播算法，并进行编程（可以使用sklearn中的神经网络）。 探讨10，30，100，300，1000，不同隐藏节点数对网络性能的影响。 探讨不同学习率和迭代次数对网络性能的影响。 改变数据的标准化方法，探讨对训练的影响。   查阅资料说明什么是Hebb学习规则">
<meta name="twitter:image" content="https://blog.crocodilezs.top/201911/神经网络作业报告/image4/reduction1.jpg">
  <link rel="alternate" href="/atom.xml" title="鰐魚先生的水族館" type="application/atom+xml">
  <link rel="canonical" href="https://blog.crocodilezs.top/201911/神经网络作业报告/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>神经网络作业报告 | 鰐魚先生的水族館</title>
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  <div class="container sidebar-position-left">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">鰐魚先生的水族館</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">言念君子，溫其如玉。</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br>关于</a>

  </li>
  </ul>

    

</nav>
</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content page-post-detail">
            

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://blog.crocodilezs.top/201911/神经网络作业报告/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CrocodileZS">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/tom.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="鰐魚先生的水族館">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">神经网络作业报告

            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-10-29 01:32:10" itemprop="dateCreated datePublished" datetime="2019-10-29T01:32:10+08:00">2019-10-29</time>
            </span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/201911/神经网络作业报告/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/201911/神经网络作业报告/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
          
          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              
                <span class="post-meta-item-text">本文字数：</span>
              
              <span>11k</span>
            </span>
          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              
              <span>18 分钟</span>
            </span>
          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="神经网络作业报告"><a href="#神经网络作业报告" class="headerlink" title="神经网络作业报告"></a>神经网络作业报告</h1><p>[TOC]<br>本次作业部分源码的参考资料已注明</p>
<h2 id="一、实验要求"><a href="#一、实验要求" class="headerlink" title="一、实验要求"></a>一、实验要求</h2><ol>
<li>推导具有单隐层的神经网络的前向传播和反向传播算法，并进行编程（可以使用<code>sklearn</code>中的神经网络）。<ul>
<li>探讨10，30，100，300，1000，不同隐藏节点数对网络性能的影响。</li>
<li>探讨不同学习率和迭代次数对网络性能的影响。</li>
<li>改变数据的标准化方法，探讨对训练的影响。</li>
</ul>
</li>
<li>查阅资料说明什么是<code>Hebb</code>学习规则</li>
</ol>
<a id="more"></a>
<h2 id="二、推导单隐层神经网络的前向传播和反向传播算法"><a href="#二、推导单隐层神经网络的前向传播和反向传播算法" class="headerlink" title="二、推导单隐层神经网络的前向传播和反向传播算法"></a>二、推导单隐层神经网络的前向传播和反向传播算法</h2><p>参考资料：<a href="https://blog.csdn.net/Lucky_Go/article/details/89738286" target="_blank" rel="noopener">https://blog.csdn.net/Lucky_Go/article/details/89738286</a><br><img src="image4\reduction1.jpg" alt><br><img src="image4\reduction2.jpg" alt><br><img src="image4\reduction3.jpg" alt><br><img src="image4\reduction4.jpg" alt></p>
<h2 id="三、算法实现"><a href="#三、算法实现" class="headerlink" title="三、算法实现"></a>三、算法实现</h2><p>参考资料：<a href="https://blog.csdn.net/zsx17/article/details/89342506" target="_blank" rel="noopener">https://blog.csdn.net/zsx17/article/details/89342506</a></p>
<p>因为网上神经网络的代码基本都是用<code>tensorflow</code>实现的，这里是直接调库。在完成了作业的基本要求之后我也尝试了自己实现单隐层神经网络的代码（在实验报告的后部分）。</p>
<h3 id="1-载入数据"><a href="#1-载入数据" class="headerlink" title="1. 载入数据"></a>1. 载入数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1、载入数据</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.examples.tutorials.mnist.input_data <span class="keyword">as</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取mnist数据</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data/'</span>, one_hot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-建立模型"><a href="#2-建立模型" class="headerlink" title="2. 建立模型"></a>2. 建立模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.建立模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.1 构建输入层</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>], name=<span class="string">'X'</span>)</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>], name=<span class="string">'Y'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.2 构建隐藏层</span></span><br><span class="line"><span class="comment"># 隐藏层神经元数量(随意设置）</span></span><br><span class="line">H1_NN = <span class="number">256</span></span><br><span class="line"><span class="comment"># 权重</span></span><br><span class="line">W1 = tf.Variable(tf.random_normal([<span class="number">784</span>, H1_NN]))</span><br><span class="line"><span class="comment"># 偏置项</span></span><br><span class="line">b1 = tf.Variable(tf.zeros([H1_NN]))</span><br><span class="line"></span><br><span class="line">Y1 = tf.nn.relu(tf.matmul(x, W1) + b1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.3 构建输出层</span></span><br><span class="line">W2 = tf.Variable(tf.random_normal([H1_NN, <span class="number">10</span>]))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">forward = tf.matmul(Y1, W2) + b2</span><br><span class="line">pred = tf.nn.softmax(forward)</span><br></pre></td></tr></table></figure>
<h3 id="3-训练模型"><a href="#3-训练模型" class="headerlink" title="3. 训练模型"></a>3. 训练模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.1 定义损失函数</span></span><br><span class="line"><span class="comment"># tensorflow提供了下面的函数，用于避免log(0)值为Nan造成数据不稳定</span></span><br><span class="line">loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=forward, labels=y))</span><br><span class="line"><span class="comment"># # 交叉熵损失函数</span></span><br><span class="line"><span class="comment"># loss_function = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.2 设置训练参数</span></span><br><span class="line">train_epochs = <span class="number">40</span>  <span class="comment"># 训练轮数</span></span><br><span class="line">batch_size = <span class="number">50</span>  <span class="comment"># 单次训练样本数(批次大小)</span></span><br><span class="line"><span class="comment"># 一轮训练的批次数</span></span><br><span class="line">total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">display_step = <span class="number">1</span>  <span class="comment"># 显示粒数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span>  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.2 选择优化器</span></span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss_function)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.3定义准确率</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(pred, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.4 模型的训练</span></span><br><span class="line"><span class="comment"># 记录训练开始的时间</span></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">startTime = time()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(train_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> range(total_batch):</span><br><span class="line">        <span class="comment"># 读取批次训练数据</span></span><br><span class="line">        xs, ys = mnist.train.next_batch(batch_size)</span><br><span class="line">        <span class="comment"># 执行批次训练</span></span><br><span class="line">        sess.run(optimizer, feed_dict=&#123;x: xs, y: ys&#125;)</span><br><span class="line">    <span class="comment"># 在total_batch批次数据训练完成后，使用验证数据计算误差和准确率，验证集不分批</span></span><br><span class="line">    loss, acc = sess.run([loss_function, accuracy], feed_dict=&#123;x: mnist.validation.images, y: mnist.validation.labels&#125;)</span><br><span class="line">    <span class="comment"># 打印训练过程中的详细信息</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'训练轮次：'</span>, <span class="string">'%02d'</span> % (epoch + <span class="number">1</span>),</span><br><span class="line">              <span class="string">'损失：'</span>, <span class="string">'&#123;:.9f&#125;'</span>.format(loss),</span><br><span class="line">              <span class="string">'准确率：'</span>, <span class="string">'&#123;:.4f&#125;'</span>.format(acc))</span><br><span class="line">print(<span class="string">'训练结束'</span>)</span><br><span class="line"><span class="comment"># 显示总运行时间</span></span><br><span class="line">duration = time() - startTime</span><br><span class="line">print(<span class="string">"总运行时间为："</span>, <span class="string">"&#123;:.2f&#125;"</span>.format(duration))</span><br></pre></td></tr></table></figure>
<h3 id="4-模型评估"><a href="#4-模型评估" class="headerlink" title="4. 模型评估"></a>4. 模型评估</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4.评估模型</span></span><br><span class="line">accu_test = sess.run(accuracy,</span><br><span class="line">                     feed_dict=&#123;x: mnist.test.images, y: mnist.test.labels&#125;)</span><br><span class="line">print(<span class="string">'测试集准确率：'</span>, accu_test)</span><br></pre></td></tr></table></figure>
<h3 id="5-应用模型"><a href="#5-应用模型" class="headerlink" title="5. 应用模型"></a>5. 应用模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 5.应用模型</span></span><br><span class="line">prediction_result = sess.run(tf.argmax(pred, <span class="number">1</span>), feed_dict=&#123;x: mnist.test.images&#125;)</span><br><span class="line"><span class="comment"># 查看预测结果的前10项</span></span><br><span class="line">print(<span class="string">"前10项的结果："</span>, prediction_result[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.1找出预测错误的样本</span></span><br><span class="line">compare_lists = prediction_result == np.argmax(mnist.test.labels, <span class="number">1</span>)</span><br><span class="line">print(compare_lists)</span><br><span class="line">err_lists = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(len(compare_lists)) <span class="keyword">if</span> compare_lists[i] == <span class="literal">False</span>]</span><br><span class="line">print(<span class="string">'预测错误的图片：'</span>, err_lists)</span><br><span class="line">print(<span class="string">'预测错误图片的总数：'</span>, len(err_lists))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个输出错误分类的函数</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_predict_errs</span><span class="params">(labels,  # 标签列表</span></span></span><br><span class="line"><span class="function"><span class="params">                       prediction)</span>:</span>  <span class="comment"># 预测值列表</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    compare_lists = (prediction == np.argmax(labels, <span class="number">1</span>))</span><br><span class="line">    err_lists = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(len(compare_lists)) <span class="keyword">if</span> compare_lists[i] == <span class="literal">False</span>]</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> err_lists:</span><br><span class="line">        print(<span class="string">'index='</span> + str(x) + <span class="string">'标签值='</span>, np.argmax(labels[x]), <span class="string">'预测值='</span>, prediction[x])</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line">    print(<span class="string">"总计："</span> + str(count))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print_predict_errs(labels=mnist.test.labels, prediction=prediction_result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_images_labels_prediction</span><span class="params">(images,  # 图像列表</span></span></span><br><span class="line"><span class="function"><span class="params">                                  labels,  # 标签列表</span></span></span><br><span class="line"><span class="function"><span class="params">                                  predication,  # 预测值列表</span></span></span><br><span class="line"><span class="function"><span class="params">                                  index,  # 从第index个开始显示</span></span></span><br><span class="line"><span class="function"><span class="params">                                  num=<span class="number">10</span>)</span>:</span>  <span class="comment"># 缺省一次显示10幅</span></span><br><span class="line">    fig = plt.gcf()  <span class="comment"># 获取当前图表，get current figure</span></span><br><span class="line">    fig.set_size_inches(<span class="number">10</span>, <span class="number">12</span>)  <span class="comment"># 设为英寸，1英寸=2.53厘米</span></span><br><span class="line">    <span class="keyword">if</span> num &gt; <span class="number">25</span>:</span><br><span class="line">        num = <span class="number">25</span>  <span class="comment"># 最多显示25个子图</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num):</span><br><span class="line">        ax = plt.subplot(<span class="number">5</span>, <span class="number">5</span>, i + <span class="number">1</span>)  <span class="comment"># 获取当前要处理的子图</span></span><br><span class="line">        <span class="comment"># 显示第index图像</span></span><br><span class="line">        ax.imshow(np.reshape(images[index], (<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建该图上显示的title</span></span><br><span class="line">        title = <span class="string">'label='</span> + str(np.argmax(labels[index]))</span><br><span class="line">        <span class="keyword">if</span> len(predication) &gt; <span class="number">0</span>:</span><br><span class="line">            title += <span class="string">",predict="</span> + str(predication[index])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 显示图上的title信息</span></span><br><span class="line">        ax.set_title(title, fontsize=<span class="number">10</span>)</span><br><span class="line">        ax.set_xticks([])  <span class="comment"># 不显示坐标轴</span></span><br><span class="line">        ax.set_yticks([])</span><br><span class="line">        index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plot_images_labels_prediction(mnist.test.images,</span><br><span class="line">                              mnist.test.labels,</span><br><span class="line">                              prediction_result, <span class="number">10</span>, <span class="number">25</span>)</span><br><span class="line">plot_images_labels_prediction(mnist.test.images,</span><br><span class="line">                              mnist.test.labels,</span><br><span class="line">                              prediction_result, <span class="number">610</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<h3 id="6-结果展示"><a href="#6-结果展示" class="headerlink" title="6. 结果展示"></a>6. 结果展示</h3><p>上面的代码中隐层节点个数为256个，学习率为0.01，迭代次数为40次。训练结果如下：</p>
<p><img src="image4\256-0.01-40.jpg" alt></p>
<p>部分分类图像如下所示：</p>
<p><img src="image4\256-0.01-40-1.jpg" alt></p>
<p><img src="image4\256-0.01-40-2.jpg" alt></p>
<h2 id="四、算法调优"><a href="#四、算法调优" class="headerlink" title="四、算法调优"></a>四、算法调优</h2><p>在上面的模型中隐层结点数为256，学习率为0.01，迭代次数为40次。</p>
<p>下面分别从隐层节点数、学习率和迭代次数三个角度进行调优。</p>
<h3 id="1-隐层节点数"><a href="#1-隐层节点数" class="headerlink" title="1. 隐层节点数"></a>1. 隐层节点数</h3><p>将隐层节点数设为10，得到的结果如下图所示：</p>
<p><img src="image4\10-0.01-40.jpg" alt></p>
<p>将隐层节点设为30，100，300，1000的效果不再具体展示，效果如下所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">隐层节点个数</th>
<th style="text-align:center">总运行时间/s</th>
<th style="text-align:center">预测错误的图片数</th>
<th style="text-align:center">准确率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">46.29</td>
<td style="text-align:center">736</td>
<td style="text-align:center">0.9264</td>
</tr>
<tr>
<td style="text-align:center">30</td>
<td style="text-align:center">43.46</td>
<td style="text-align:center">528</td>
<td style="text-align:center">0.9472</td>
</tr>
<tr>
<td style="text-align:center">100</td>
<td style="text-align:center">59.06</td>
<td style="text-align:center">343</td>
<td style="text-align:center">0.9657</td>
</tr>
<tr>
<td style="text-align:center">256</td>
<td style="text-align:center">84.48</td>
<td style="text-align:center">249</td>
<td style="text-align:center">0.9751</td>
</tr>
<tr>
<td style="text-align:center">300</td>
<td style="text-align:center">76.64</td>
<td style="text-align:center">269</td>
<td style="text-align:center">0.9731</td>
</tr>
<tr>
<td style="text-align:center">1000</td>
<td style="text-align:center">302.27</td>
<td style="text-align:center">240</td>
<td style="text-align:center">0.976</td>
</tr>
</tbody>
</table>
</div>
<p>由表可知，准确率随着隐层节点个数的增加而增加，增加速率逐步减少。</p>
<h3 id="2-学习率"><a href="#2-学习率" class="headerlink" title="2. 学习率"></a>2. 学习率</h3><p>学习率分别为0.005，0.01， 0.02， 0.1，隐层节点数选择256，迭代次数选择40。分类结果如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">学习率</th>
<th style="text-align:center">总运行时间/s</th>
<th style="text-align:center">预测错误的图片数</th>
<th style="text-align:center">准确率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0.005</td>
<td style="text-align:center">78.81</td>
<td style="text-align:center">231</td>
<td style="text-align:center">0.9769</td>
</tr>
<tr>
<td style="text-align:center">0.01</td>
<td style="text-align:center">84.48</td>
<td style="text-align:center">249</td>
<td style="text-align:center">0.9751</td>
</tr>
<tr>
<td style="text-align:center">0.02</td>
<td style="text-align:center">69.72</td>
<td style="text-align:center">446</td>
<td style="text-align:center">0.9554</td>
</tr>
<tr>
<td style="text-align:center">0.1</td>
<td style="text-align:center">73.87</td>
<td style="text-align:center">2561</td>
<td style="text-align:center">0.7439</td>
</tr>
</tbody>
</table>
</div>
<p>由表可知，准确率随着学习率的增加而降低。在学习率低于0.01时，图片分类的准确率提升的速率较小。</p>
<h3 id="3-迭代次数"><a href="#3-迭代次数" class="headerlink" title="3. 迭代次数"></a>3. 迭代次数</h3><p>迭代次数分别为20，40，100，隐层节点数选择256，学习率选择0.01。分类结果如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">迭代次数</th>
<th style="text-align:center">总运行时间/s</th>
<th style="text-align:center">预测错误的图片数</th>
<th style="text-align:center">准确率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">20</td>
<td style="text-align:center">37.12</td>
<td style="text-align:center">307</td>
<td style="text-align:center">0.9693</td>
</tr>
<tr>
<td style="text-align:center">40</td>
<td style="text-align:center">84.48</td>
<td style="text-align:center">249</td>
<td style="text-align:center">0.9751</td>
</tr>
<tr>
<td style="text-align:center">100</td>
<td style="text-align:center">184.39</td>
<td style="text-align:center">239</td>
<td style="text-align:center">0.9761</td>
</tr>
</tbody>
</table>
</div>
<p>由表可知，迭代次数对总运行时间的影响率很大，准确率随着迭代次数的增加而增加，但对准确率起决定因素的还是隐层的节点个数以及学习率。</p>
<h3 id="4-改变数据标准化方法"><a href="#4-改变数据标准化方法" class="headerlink" title="4. 改变数据标准化方法"></a>4. 改变数据标准化方法</h3><h4 id="最大-最小规范化"><a href="#最大-最小规范化" class="headerlink" title="最大-最小规范化"></a>最大-最小规范化</h4><h4 id="Z-score规范化"><a href="#Z-score规范化" class="headerlink" title="Z-score规范化"></a><code>Z-score</code>规范化</h4><h2 id="五、Hebb学习规则"><a href="#五、Hebb学习规则" class="headerlink" title="五、Hebb学习规则"></a>五、<code>Hebb</code>学习规则</h2><p>参考资料：<a href="https://baike.baidu.com/item/Hebb学习规则/3061563?fr=aladdin" target="_blank" rel="noopener">https://baike.baidu.com/item/Hebb%E5%AD%A6%E4%B9%A0%E8%A7%84%E5%88%99/3061563?fr=aladdin</a></p>
<p><code>Hebb</code>学习规则是一个无监督学习规则，这种学习的结果是使网络能够提取训练集的统计特性，从而把输入信息按照它们的相似性程度划分为若干类。这一点与人类观察和认识世界的过程非常吻合，人类观察和认识世界在相当程度上就是在根据事物的统计特征进行分类。<code>Hebb</code>学习规则只根据神经元连接间的激活水平改变权值，因此这种方法又称为相关学习或并联学习。</p>
<p>无监督学习规则<br> 唐纳德·赫布（1904-1985）是加拿大著名生理心理学家。<code>Hebb</code>学习规则与“条件反射”机理一致，并且已经得到了神经细胞学说的证实。<br> 巴甫洛夫的条件反射实验：每次给狗喂食前都先响铃，时间一长，狗就会将铃声和食物联系起来。以后如果响铃但是不给食物，狗也会流口水。<br> 受该实验的启发，Hebb的理论认为在同一时间被激发的神经元间的联系会被强化。比如，铃声响时一个神经元被激发，在同一时间食物的出现会激发附近的另一个神经元，那么这两个神经元间的联系就会强化，从而记住这两个事物之间存在着联系。相反，如果两个神经元总是不能同步激发，那么它们间的联系将会越来越弱。<br> <code>Hebb</code>学习律可表示为：<br>$W_{ij}(t+1)=W_{ij}(t)+a⋅y_i⋅y_j$<br>$W_{ij}(t+1)=W_{ij}(t)+a⋅y_i⋅y_j$</p>
<p> 其中$W_{ij}$表示神经元$j$到神经元$i$的连接权，$y_i$与$y_j$表示两个神经元的输出，$a$是表示学习速率的常数，如果$y_i$与$y_j$同时被激活，即$y_i$与$y_j$同时为正，那么$W_{ij}$将增大。如果$y_i$被激活，而$y_j$处于抑制状态，即$y_i$为正$y_j$为负，那么$W_{ij}$将变小。</p>
<p><img src="https://images2015.cnblogs.com/blog/520787/201510/520787-20151021081107630-1544768706.png" alt></p>
<h2 id="六、-自己实现单隐层神经网络"><a href="#六、-自己实现单隐层神经网络" class="headerlink" title="六、 自己实现单隐层神经网络"></a>六、 自己实现单隐层神经网络</h2><p>参考资料：<a href="https://blog.csdn.net/hellozhxy/article/details/81055391" target="_blank" rel="noopener">https://blog.csdn.net/hellozhxy/article/details/81055391</a></p>
<p>网络结构的函数定义：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment"># size of input layer</span></span><br><span class="line">    n_h = <span class="number">4</span> <span class="comment"># size of hidden layer</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment"># size of output layer</span></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure>
<p>参数初始化函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    W1 = np.random.randn(n_h, n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>)) </span><br><span class="line">   </span><br><span class="line">    <span class="keyword">assert</span> (W1.shape == (n_h, n_x))    </span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (n_h, <span class="number">1</span>))    </span><br><span class="line">    <span class="keyword">assert</span> (W2.shape == (n_y, n_h))    </span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1, </span><br><span class="line">                  <span class="string">"b1"</span>: b1,                 </span><br><span class="line">                  <span class="string">"W2"</span>: W2,                  </span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;   </span><br><span class="line">                   </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p>前向传播计算函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    b1 = parameters[<span class="string">'b1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    b2 = parameters[<span class="string">'b2'</span>]    </span><br><span class="line">    <span class="comment"># Implement Forward Propagation to calculate A2 (probabilities)</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, Z1) + b2</span><br><span class="line">    A2 = sigmoid(Z2)    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,                   </span><br><span class="line">             <span class="string">"A1"</span>: A1,                   </span><br><span class="line">             <span class="string">"Z2"</span>: Z2,                  </span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure>
<p>计算损失函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># number of example</span></span><br><span class="line">    <span class="comment"># Compute the cross-entropy cost</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(<span class="number">1</span>-A2), <span class="number">1</span>-Y)</span><br><span class="line">    cost = <span class="number">-1</span>/m * np.sum(logprobs)</span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># makes sure cost is the dimension we expect.</span></span><br><span class="line">    <span class="keyword">assert</span>(isinstance(cost, float))    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>
<p>反向传播函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]    </span><br><span class="line">    <span class="comment"># First, retrieve W1 and W2 from the dictionary "parameters".</span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]    </span><br><span class="line">    <span class="comment"># Retrieve also A1 and A2 from dictionary "cache".</span></span><br><span class="line">    A1 = cache[<span class="string">'A1'</span>]</span><br><span class="line">    A2 = cache[<span class="string">'A2'</span>]    </span><br><span class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2. </span></span><br><span class="line">    dZ2 = A2-Y</span><br><span class="line">    dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dZ1 = np.dot(W2.T, dZ2)*(<span class="number">1</span>-np.power(A1, <span class="number">2</span>))</span><br><span class="line">    dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,                      </span><br><span class="line">             <span class="string">"dW2"</span>: dW2,             </span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;   </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<p>权值更新函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate = <span class="number">1.2</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    b1 = parameters[<span class="string">'b1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    b2 = parameters[<span class="string">'b2'</span>]    </span><br><span class="line">    <span class="comment"># Retrieve each gradient from the dictionary "grads"</span></span><br><span class="line">    dW1 = grads[<span class="string">'dW1'</span>]</span><br><span class="line">    db1 = grads[<span class="string">'db1'</span>]</span><br><span class="line">    dW2 = grads[<span class="string">'dW2'</span>]</span><br><span class="line">    db2 = grads[<span class="string">'db2'</span>]    </span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    W1 -= dW1 * learning_rate</span><br><span class="line">    b1 -= db1 * learning_rate</span><br><span class="line">    W2 -= dW2 * learning_rate</span><br><span class="line">    b2 -= db2 * learning_rate</span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1, </span><br><span class="line">                  <span class="string">"b1"</span>: b1,            </span><br><span class="line">                  <span class="string">"W2"</span>: W2,   </span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p>最终的神经网络模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]    </span><br><span class="line">    <span class="comment"># Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: "n_x, n_h, n_y". Outputs = "W1, b1, W2, b2, parameters".</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    b1 = parameters[<span class="string">'b1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    b2 = parameters[<span class="string">'b2'</span>]    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):        </span><br><span class="line">    <span class="comment"># Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)        </span><br><span class="line">        <span class="comment"># Cost function. Inputs: "A2, Y, parameters". Outputs: "cost".</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)        </span><br><span class="line">        <span class="comment"># Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)        </span><br><span class="line">        <span class="comment"># Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate=<span class="number">1.2</span>)        </span><br><span class="line">        <span class="comment"># Print the cost every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:            </span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))    </span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>

    </div>

    

    
    
    
      

        
      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/神经网络/" rel="tag"># 神经网络</a>
            
          </div>
        

        
  <div class="post-widgets">
    <div class="social-share">
      
      
        <div id="needsharebutton-postbottom">
          <span class="btn">
            <i class="fa fa-share-alt" aria-hidden="true"></i>
          </span>
        </div>
      
    </div>
  
  </div>

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/201910/FINDS算法和ID3算法/" rel="next" title="Finds算法和ID3算法">
                  <i class="fa fa-chevron-left"></i> Finds算法和ID3算法
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/201911/Linux开发环境及应用作业 20191031/" rel="prev" title="Linux开发环境及应用作业1">
                  Linux开发环境及应用作业1 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="comments"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/tom.jpg"
      alt="CrocodileZS">
  <p class="site-author-name" itemprop="name">CrocodileZS</p>
  <div class="site-description motion-element" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">72</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:ammo@bupt.edu.cn" title="E-Mail &rarr; mailto:ammo@bupt.edu.cn" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://instagram.com/crocodile_zs" title="Instagram &rarr; https://instagram.com/crocodile_zs" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i>Instagram</a>
      </span>
    
  </div>


  <div class="links-of-blogroll motion-element links-of-blogroll-block">
    <div class="links-of-blogroll-title">
      <i class="fa  fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.wangxiaofeng.me/" title="http://www.wangxiaofeng.me/" rel="noopener" target="_blank">不许联想</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://idx0.dev/" title="https://idx0.dev/" rel="noopener" target="_blank">思维</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://128bit.top" title="https://128bit.top" rel="noopener" target="_blank">Dixeran</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://icontofig.leanote.com/" title="http://icontofig.leanote.com/" rel="noopener" target="_blank">Icontofig</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://www.nekomio.com" title="https://www.nekomio.com" rel="noopener" target="_blank">NekoMio</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://al0ha0e.github.io" title="https://al0ha0e.github.io" rel="noopener" target="_blank">Al0ha0e</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://gaokeyong.coding.me" title="http://gaokeyong.coding.me" rel="noopener" target="_blank">Keyong</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://poeroz.cn/" title="http://poeroz.cn/" rel="noopener" target="_blank">Poeroz</a>
        </li>
      
    </ul>
  </div>

        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络作业报告"><span class="nav-text">神经网络作业报告</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、实验要求"><span class="nav-text">一、实验要求</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、推导单隐层神经网络的前向传播和反向传播算法"><span class="nav-text">二、推导单隐层神经网络的前向传播和反向传播算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三、算法实现"><span class="nav-text">三、算法实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-载入数据"><span class="nav-text">1. 载入数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-建立模型"><span class="nav-text">2. 建立模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-训练模型"><span class="nav-text">3. 训练模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-模型评估"><span class="nav-text">4. 模型评估</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-应用模型"><span class="nav-text">5. 应用模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-结果展示"><span class="nav-text">6. 结果展示</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四、算法调优"><span class="nav-text">四、算法调优</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-隐层节点数"><span class="nav-text">1. 隐层节点数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-学习率"><span class="nav-text">2. 学习率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-迭代次数"><span class="nav-text">3. 迭代次数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-改变数据标准化方法"><span class="nav-text">4. 改变数据标准化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#最大-最小规范化"><span class="nav-text">最大-最小规范化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Z-score规范化"><span class="nav-text">Z-score规范化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#五、Hebb学习规则"><span class="nav-text">五、Hebb学习规则</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#六、-自己实现单隐层神经网络"><span class="nav-text">六、 自己实现单隐层神经网络</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">鰐魚先生</span>
</div>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>








        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>

    
  
  <div id="needsharebutton-float">
    <span class="btn">
      <i class="fa fa-share-alt" aria-hidden="true"></i>
    </span>
  </div>
<script src="/lib/needsharebutton/needsharebutton.js"></script>
<script>
    pbOptions = {};
      pbOptions.iconStyle = "default";
    
      pbOptions.boxForm = "horizontal";
    
      pbOptions.position = "bottomCenter";
    
      pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
    
    new needShareButton('#needsharebutton-postbottom', pbOptions);
    flOptions = {};
      flOptions.iconStyle = "box";
    
      flOptions.boxForm = "horizontal";
    
      flOptions.position = "middleRight";
    
      flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
    
    new needShareButton('#needsharebutton-float', flOptions);
</script>


  </div>

  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

<script src="/js/utils.js?v=7.3.0"></script>
  <script src="/js/motion.js?v=7.3.0"></script>


  <script src="/js/schemes/muse.js?v=7.3.0"></script>


<script src="/js/next-boot.js?v=7.3.0"></script>




  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




























  

  

  

  


  
  <script src="/js/scrollspy.js?v=7.3.0"></script>
<script src="/js/post-details.js?v=7.3.0"></script>


    

<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', function() {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: '9VEOQepq1oDMG0XHxBOrKuwU-gzGzoHsz',
    appKey: 'XVzSWQhACVUcrVzQDF0AJlb8',
    placeholder: 'ヽ(✿ﾟ▽ﾟ)ノ',
    avatar: 'zhouyuyang17@163.com',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: 'zh-cn' || 'zh-cn',
    path: location.pathname
  });
}, window.Valine);
</script>

  <script type="text/javascript" src="/lib/zclip/clipboard.min.js"></script>	
<script type="text/javascript" src="/js/src/custom.js"></script>
</body>
</html>